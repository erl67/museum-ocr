{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d974f27-a0d1-45e6-84a0-e8bb3be4492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "JPEGS_DIR = Path(r\".\\JPEGS\")\n",
    "OUT_DIR = Path(r\".\\out\")\n",
    "OCR_TEXT_DIR = OUT_DIR / \"ocr_text\"\n",
    "\n",
    "WORKERS = 6\n",
    "PROGRESS_EVERY = 10\n",
    "\n",
    "# If you installed tesseract but it's not on PATH, set it explicitly:\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OCR_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ACC_RE = r\"\\d{1,6}\"\n",
    "CAT_RE = r\"\\d{1,7}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1235edc-1719-4b6b-b409-6aaa840cc9bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Duplicate files detected for 102 pages. Keeping largest file for each.\n",
      "       Examples: 1:2, 2:2, 3:2, 4:2, 5:2, 6:2, 7:2, 8:2, 9:2, 10:2, 11:2, 12:2 ...\n",
      "[Start] OCR on 102 pages from .\\JPEGS\\ using 6 workers\n",
      "  completed 10/102 | 1.71 pages/sec | ETA ~ 0.9 min\n",
      "  completed 20/102 | 1.88 pages/sec | ETA ~ 0.7 min\n",
      "  completed 30/102 | 2.14 pages/sec | ETA ~ 0.6 min\n",
      "  completed 40/102 | 2.21 pages/sec | ETA ~ 0.5 min\n",
      "  completed 50/102 | 2.36 pages/sec | ETA ~ 0.4 min\n",
      "  completed 60/102 | 2.30 pages/sec | ETA ~ 0.3 min\n",
      "  completed 70/102 | 2.41 pages/sec | ETA ~ 0.2 min\n",
      "  completed 80/102 | 2.34 pages/sec | ETA ~ 0.2 min\n",
      "  completed 90/102 | 2.33 pages/sec | ETA ~ 0.1 min\n",
      "  completed 100/102 | 2.40 pages/sec | ETA ~ 0.0 min\n",
      "  completed 102/102 | 2.44 pages/sec | ETA ~ 0.0 min\n",
      "\n",
      "[Done] Outputs:\n",
      "  - out\\output.csv         (pages that passed checks) rows=9371\n",
      "  - out\\output_all_rows.csv     (all extracted rows)        rows=16210\n",
      "  - out\\page_counts.csv\n",
      "  - out\\needs_review.csv   pages_flagged=44\n",
      "  - out\\ocr_text\\page_###.txt\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob, time\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG (all RELATIVE)\n",
    "# -------------------------\n",
    "JPEGS_DIR = \"JPEGS\"     # reads from .\\JPEGS\\\n",
    "OUT_DIR = \"out\"         # writes to .\\out\\\n",
    "OCR_TEXT_DIR = os.path.join(OUT_DIR, \"ocr_text\")\n",
    "\n",
    "OUTPUT_CSV = os.path.join(OUT_DIR, \"output.csv\")\n",
    "OUTPUT_ALL_CSV = os.path.join(OUT_DIR, \"output_all_rows.csv\")\n",
    "PAGE_COUNTS_CSV = os.path.join(OUT_DIR, \"page_counts.csv\")\n",
    "NEEDS_REVIEW_CSV = os.path.join(OUT_DIR, \"needs_review.csv\")\n",
    "\n",
    "# If Tesseract isn't on PATH, set this:\n",
    "# pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# Threaded OCR: good for Jupyter on Windows (Tesseract runs as external processes anyway).\n",
    "N_WORKERS = 6\n",
    "\n",
    "# Avoid CPU oversubscription (important when parallelizing OCR)\n",
    "os.environ[\"OMP_THREAD_LIMIT\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# -------------------------\n",
    "# EXPECTED ROW COUNTS\n",
    "# -------------------------\n",
    "EXPECTED = {1: 141}\n",
    "for p in range(2, 99):\n",
    "    EXPECTED[p] = 162\n",
    "EXPECTED[99] = 160\n",
    "EXPECTED[100] = 160\n",
    "EXPECTED[101] = 162\n",
    "EXPECTED[102] = 124\n",
    "\n",
    "# -------------------------\n",
    "# REGEX: tolerant of \"E.\", \"E-\", etc.\n",
    "# -------------------------\n",
    "TRIPLET_RE = re.compile(r\"(?<!\\d)(\\d{1,6})\\s*([EDG])[\\.\\-–—]?\\s*(\\d{1,7})(?!\\d)\")\n",
    "\n",
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "def page_num_from_path(p: str) -> int:\n",
    "    \"\"\"Extract the first integer from the filename stem (robust to '99 (1).jpg', 'page_99.jpg', etc.).\"\"\"\n",
    "    stem = os.path.splitext(os.path.basename(p))[0]\n",
    "    m = re.search(r\"(\\d+)\", stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse page number from: {p}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def list_pages_from_folder():\n",
    "    \"\"\"Find JPGs in JPEGS_DIR, dedupe by page number (pick largest file), return 102 paths for pages 1..102.\"\"\"\n",
    "    if not os.path.isdir(JPEGS_DIR):\n",
    "        raise FileNotFoundError(f\"Folder not found: .\\\\{JPEGS_DIR}\\\\  (current dir: {os.getcwd()})\")\n",
    "\n",
    "    jpgs = glob.glob(os.path.join(JPEGS_DIR, \"*.jpg\"))\n",
    "    jpgs += glob.glob(os.path.join(JPEGS_DIR, \"*.JPG\"))\n",
    "\n",
    "    if not jpgs:\n",
    "        raise FileNotFoundError(f\"No .jpg files found in .\\\\{JPEGS_DIR}\\\\\")\n",
    "\n",
    "    by_page = defaultdict(list)\n",
    "    for p in jpgs:\n",
    "        try:\n",
    "            pg = page_num_from_path(p)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        by_page[pg].append(p)\n",
    "\n",
    "    missing = [pg for pg in range(1, 103) if pg not in by_page]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing required pages: {missing[:20]}{'...' if len(missing)>20 else ''}\")\n",
    "\n",
    "    # Choose best (largest file) for each page\n",
    "    chosen = {}\n",
    "    dup_pages = []\n",
    "    for pg in range(1, 103):\n",
    "        paths = by_page[pg]\n",
    "        if len(paths) > 1:\n",
    "            dup_pages.append((pg, len(paths)))\n",
    "        chosen[pg] = max(paths, key=lambda x: os.path.getsize(x))\n",
    "\n",
    "    if dup_pages:\n",
    "        print(f\"[Info] Duplicate files detected for {len(dup_pages)} pages. Keeping largest file for each.\")\n",
    "        print(\"       Examples:\", \", \".join([f\"{pg}:{n}\" for pg, n in dup_pages[:12]]) + (\" ...\" if len(dup_pages) > 12 else \"\"))\n",
    "\n",
    "    return [chosen[pg] for pg in range(1, 103)]\n",
    "\n",
    "def ensure_out_dirs():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    os.makedirs(OCR_TEXT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# IMAGE PREPROCESSING\n",
    "# -------------------------\n",
    "def preprocess(img: Image.Image, scale=1.0, pad=20, thresh=205) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Preprocess for OCR speed/accuracy:\n",
    "    - grayscale + autocontrast\n",
    "    - crop to content bbox (removes margins)\n",
    "    - optional upscale\n",
    "    - median filter\n",
    "    - binarize\n",
    "    \"\"\"\n",
    "    img = img.convert(\"L\")\n",
    "    img = ImageOps.autocontrast(img)\n",
    "\n",
    "    inv = ImageOps.invert(img)\n",
    "    bbox = inv.getbbox()\n",
    "    if bbox:\n",
    "        l, t, r, b = bbox\n",
    "        l = max(0, l - pad); t = max(0, t - pad)\n",
    "        r = min(img.size[0], r + pad); b = min(img.size[1], b + pad)\n",
    "        img = img.crop((l, t, r, b))\n",
    "\n",
    "    if scale != 1.0:\n",
    "        img = img.resize((int(img.size[0] * scale), int(img.size[1] * scale)), Image.Resampling.BICUBIC)\n",
    "\n",
    "    img = img.filter(ImageFilter.MedianFilter(size=3))\n",
    "    arr = np.array(img)\n",
    "    bw = (arr > thresh).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "def ocr_text(img: Image.Image, allow_letters: bool) -> str:\n",
    "    \"\"\"\n",
    "    OCR with whitelist:\n",
    "    - letters needed for detecting 'SKELETONS' pivot on page 100 (and harmless elsewhere)\n",
    "    \"\"\"\n",
    "    if allow_letters:\n",
    "        whitelist = \"0123456789EDGSKINSPICKLETONS.-\"\n",
    "    else:\n",
    "        whitelist = \"0123456789EDG.-\"\n",
    "\n",
    "    cfg = (\n",
    "        \"--oem 3 --psm 6 \"\n",
    "        \"-c preserve_interword_spaces=1 \"\n",
    "        f\"-c tessedit_char_whitelist={whitelist}\"\n",
    "    )\n",
    "    return pytesseract.image_to_string(img, config=cfg)\n",
    "\n",
    "def extract_triplets_by_line(text: str):\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    triplets = []\n",
    "    for i, ln in enumerate(lines):\n",
    "        for a, c, b in TRIPLET_RE.findall(ln):\n",
    "            triplets.append((i, int(a), c, int(b)))\n",
    "    return lines, triplets\n",
    "\n",
    "def find_pivot_line(lines, keyword: str):\n",
    "    kw = keyword.upper()\n",
    "    for i, ln in enumerate(lines):\n",
    "        if kw in ln.upper():\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def classify_type(page: int, line_idx: int, catalog: int, pivot_skeletons=None):\n",
    "    # Pages 1–98 are SKINS throughout\n",
    "    if page <= 98:\n",
    "        return \"SKINS\"\n",
    "\n",
    "    # Page 99: PICKLES starts mid-page in the right column.\n",
    "    # Robust shortcut: PICKLES catalogs are small (hundreds/thousands); SKINS are ~113k+ on that page.\n",
    "    if page == 99:\n",
    "        return \"PICKLES\" if catalog < 10000 else \"SKINS\"\n",
    "\n",
    "    # Page 100: SKELETONS starts mid-page; pivot by header line.\n",
    "    if page == 100:\n",
    "        if pivot_skeletons is None:\n",
    "            return None\n",
    "        return \"PICKLES\" if line_idx < pivot_skeletons else \"SKELETONS\"\n",
    "\n",
    "    # Pages 101–102 are SKELETONS throughout\n",
    "    return \"SKELETONS\"\n",
    "\n",
    "# -------------------------\n",
    "# PAGE PROCESSING\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class PageResult:\n",
    "    page: int\n",
    "    rows: list\n",
    "    extracted: int\n",
    "    expected: int\n",
    "    pivot: int | None\n",
    "    error: str\n",
    "    text_file: str\n",
    "    image_file: str\n",
    "\n",
    "def process_one(task):\n",
    "    page, path = task\n",
    "    expected = EXPECTED.get(page, None)\n",
    "\n",
    "    # Allow letters only where we might need to detect section words\n",
    "    allow_letters = page in (99, 100, 1)\n",
    "\n",
    "    img = Image.open(path)\n",
    "    imgp = preprocess(img, scale=1.0, thresh=205)  # if misses occur, try scale=1.2 or thresh=200\n",
    "    text = ocr_text(imgp, allow_letters=allow_letters)\n",
    "    lines, triplets = extract_triplets_by_line(text)\n",
    "\n",
    "    pivot = None\n",
    "    if page == 100:\n",
    "        pivot = find_pivot_line(lines, \"SKELETONS\")\n",
    "\n",
    "    # Dump OCR text (for debugging)\n",
    "    text_file = os.path.join(OCR_TEXT_DIR, f\"page_{page:03d}.txt\")\n",
    "    with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    rows = []\n",
    "    for line_idx, accession, code, catalog in triplets:\n",
    "        typ = classify_type(page, line_idx, catalog, pivot_skeletons=pivot)\n",
    "        rows.append({\n",
    "            \"Accession\": accession,\n",
    "            \"Code\": code,\n",
    "            \"Catalog\": catalog,\n",
    "            \"Type\": typ,\n",
    "            \"Page\": page\n",
    "        })\n",
    "\n",
    "    # Validate\n",
    "    errors = []\n",
    "    if expected is not None and len(rows) != expected:\n",
    "        errors.append(f\"COUNT_MISMATCH {len(rows)} != {expected}\")\n",
    "    if page == 100 and pivot is None:\n",
    "        errors.append(\"NO_SKELETONS_PIVOT_FOUND\")\n",
    "    if any(r[\"Type\"] is None for r in rows):\n",
    "        errors.append(\"UNASSIGNED_TYPE\")\n",
    "\n",
    "    return PageResult(\n",
    "        page=page,\n",
    "        rows=rows,\n",
    "        extracted=len(rows),\n",
    "        expected=expected,\n",
    "        pivot=pivot,\n",
    "        error=\"; \".join(errors),\n",
    "        text_file=text_file,\n",
    "        image_file=path\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# RUN EVERYTHING + PROGRESS\n",
    "# -------------------------\n",
    "def run_all():\n",
    "    ensure_out_dirs()\n",
    "    paths = list_pages_from_folder()\n",
    "    tasks = [(i + 1, paths[i]) for i in range(102)]\n",
    "\n",
    "    print(f\"[Start] OCR on 102 pages from .\\\\{JPEGS_DIR}\\\\ using {N_WORKERS} workers\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    results = []\n",
    "    done = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=N_WORKERS) as ex:\n",
    "        futures = [ex.submit(process_one, t) for t in tasks]\n",
    "        for fut in as_completed(futures):\n",
    "            res = fut.result()\n",
    "            results.append(res)\n",
    "            done += 1\n",
    "            if done % 10 == 0 or done == 102:\n",
    "                elapsed = time.time() - t0\n",
    "                rate = done / elapsed if elapsed > 0 else 0.0\n",
    "                eta = (102 - done) / rate if rate > 0 else float(\"inf\")\n",
    "                print(f\"  completed {done}/102 | {rate:.2f} pages/sec | ETA ~ {eta/60:.1f} min\")\n",
    "\n",
    "    results.sort(key=lambda r: r.page)\n",
    "\n",
    "    # page_counts.csv + needs_review.csv\n",
    "    page_counts = pd.DataFrame([{\n",
    "        \"Page\": r.page,\n",
    "        \"RowsExtracted\": r.extracted,\n",
    "        \"RowsExpected\": r.expected,\n",
    "        \"SkeletonsPivotLine\": r.pivot,\n",
    "        \"Error\": r.error,\n",
    "        \"ImageFile\": r.image_file,\n",
    "        \"OcrTextFile\": r.text_file\n",
    "    } for r in results])\n",
    "\n",
    "    page_counts.to_csv(PAGE_COUNTS_CSV, index=False)\n",
    "\n",
    "    needs_review = page_counts[page_counts[\"Error\"].astype(str).str.len() > 0].copy()\n",
    "    needs_review.to_csv(NEEDS_REVIEW_CSV, index=False)\n",
    "\n",
    "    # rows -> csv\n",
    "    all_rows = pd.DataFrame([row for r in results for row in r.rows])\n",
    "    all_rows.to_csv(OUTPUT_ALL_CSV, index=False)\n",
    "\n",
    "    good_pages = set(page_counts[page_counts[\"Error\"] == \"\"][\"Page\"].tolist())\n",
    "    safe_rows = all_rows[all_rows[\"Page\"].isin(good_pages)].copy()\n",
    "    safe_rows.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "    print(\"\\n[Done] Outputs:\")\n",
    "    print(f\"  - {OUTPUT_CSV}         (pages that passed checks) rows={len(safe_rows)}\")\n",
    "    print(f\"  - {OUTPUT_ALL_CSV}     (all extracted rows)        rows={len(all_rows)}\")\n",
    "    print(f\"  - {PAGE_COUNTS_CSV}\")\n",
    "    print(f\"  - {NEEDS_REVIEW_CSV}   pages_flagged={len(needs_review)}\")\n",
    "    print(f\"  - {OCR_TEXT_DIR}\\\\page_###.txt\")\n",
    "\n",
    "    return page_counts, needs_review, safe_rows, all_rows\n",
    "\n",
    "page_counts_df, needs_review_df, safe_df, all_df = run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92f4eaf-0d98-4616-8fce-b4f7a5bc4dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Duplicate files for 102 pages. Keeping largest for each.\n",
      "       Examples: 1:2, 2:2, 3:2, 4:2, 5:2, 6:2, 7:2, 8:2, 9:2, 10:2, 11:2, 12:2 ...\n",
      "[Start] Processing 102 pages from .\\JPEGS\\ with 6 workers\n",
      "  completed 10/102 | 0.77 pages/sec | ETA ~ 2.0 min\n",
      "  completed 20/102 | 0.44 pages/sec | ETA ~ 3.1 min\n",
      "  completed 30/102 | 0.33 pages/sec | ETA ~ 3.6 min\n",
      "  completed 40/102 | 0.33 pages/sec | ETA ~ 3.1 min\n",
      "  completed 50/102 | 0.37 pages/sec | ETA ~ 2.4 min\n",
      "  completed 60/102 | 0.38 pages/sec | ETA ~ 1.9 min\n",
      "  completed 70/102 | 0.37 pages/sec | ETA ~ 1.4 min\n",
      "  completed 80/102 | 0.38 pages/sec | ETA ~ 1.0 min\n",
      "  completed 90/102 | 0.37 pages/sec | ETA ~ 0.5 min\n",
      "  completed 100/102 | 0.39 pages/sec | ETA ~ 0.1 min\n",
      "  completed 102/102 | 0.37 pages/sec | ETA ~ 0.0 min\n",
      "\n",
      "[Done] Outputs:\n",
      "  - out\\output.csv         rows=13221 (only pages that passed checks)\n",
      "  - out\\output_all_rows.csv     rows=16206 (everything)\n",
      "  - out\\page_counts.csv\n",
      "  - out\\needs_review.csv   pages_flagged=20\n",
      "  - Debug dumps in out\\ocr_text\\page_###.txt\n",
      "  - Pages that needed retries: ~36/102\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# EXPECTED ROW COUNTS\n",
    "# -------------------------\n",
    "EXPECTED = {1: 141}\n",
    "for p in range(2, 99):\n",
    "    EXPECTED[p] = 162\n",
    "EXPECTED[99]  = 160\n",
    "EXPECTED[100] = 160\n",
    "EXPECTED[101] = 162\n",
    "EXPECTED[102] = 124\n",
    "\n",
    "# -------------------------\n",
    "# PARSING\n",
    "# accession + code + catalog\n",
    "# tighten ranges to reduce false positives:\n",
    "# accession: 3-6 digits (yours include 4-5, but keep 6 for safety)\n",
    "# catalog:   2-7 digits (some are small like 795; some large like 119203)\n",
    "TRIPLET_RE = re.compile(r\"(?<!\\d)(\\d{3,6})\\s*([EDG])\\s*(\\d{2,7})(?!\\d)\")\n",
    "\n",
    "def ensure_out_dirs():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    os.makedirs(OCR_TEXT_DIR, exist_ok=True)\n",
    "\n",
    "def page_num_from_path(p: str) -> int:\n",
    "    stem = os.path.splitext(os.path.basename(p))[0]\n",
    "    m = re.search(r\"(\\d+)\", stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse page number from: {p}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def list_pages_from_folder():\n",
    "    if not os.path.isdir(JPEGS_DIR):\n",
    "        raise FileNotFoundError(f\"Folder not found: .\\\\{JPEGS_DIR}\\\\ (cwd: {os.getcwd()})\")\n",
    "\n",
    "    jpgs = glob.glob(os.path.join(JPEGS_DIR, \"*.jpg\")) + glob.glob(os.path.join(JPEGS_DIR, \"*.JPG\"))\n",
    "    if not jpgs:\n",
    "        raise FileNotFoundError(f\"No .jpg files found in .\\\\{JPEGS_DIR}\\\\\")\n",
    "\n",
    "    by_page = defaultdict(list)\n",
    "    for p in jpgs:\n",
    "        try:\n",
    "            pg = page_num_from_path(p)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        by_page[pg].append(p)\n",
    "\n",
    "    missing = [pg for pg in range(1, 103) if pg not in by_page]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing required pages: {missing}\")\n",
    "\n",
    "    # dedupe by choosing largest file\n",
    "    chosen = {}\n",
    "    dup_pages = [(pg, len(paths)) for pg, paths in by_page.items() if len(paths) > 1 and 1 <= pg <= 102]\n",
    "    for pg in range(1, 103):\n",
    "        chosen[pg] = max(by_page[pg], key=lambda x: os.path.getsize(x))\n",
    "\n",
    "    if dup_pages:\n",
    "        dup_pages.sort()\n",
    "        print(f\"[Info] Duplicate files for {len(dup_pages)} pages. Keeping largest for each.\")\n",
    "        print(\"       Examples:\", \", \".join([f\"{pg}:{n}\" for pg, n in dup_pages[:12]]) + (\" ...\" if len(dup_pages) > 12 else \"\"))\n",
    "\n",
    "    return [chosen[pg] for pg in range(1, 103)]\n",
    "\n",
    "# -------------------------\n",
    "# IMAGE PREPROCESSING\n",
    "# -------------------------\n",
    "def otsu_threshold(arr: np.ndarray) -> int:\n",
    "    # arr is grayscale uint8\n",
    "    hist = np.bincount(arr.ravel(), minlength=256).astype(np.float64)\n",
    "    total = arr.size\n",
    "    if total == 0:\n",
    "        return 200\n",
    "    sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    max_var = -1.0\n",
    "    thresh = 200\n",
    "\n",
    "    for t in range(256):\n",
    "        w_b += hist[t]\n",
    "        if w_b == 0:\n",
    "            continue\n",
    "        w_f = total - w_b\n",
    "        if w_f == 0:\n",
    "            break\n",
    "        sum_b += t * hist[t]\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > max_var:\n",
    "            max_var = var_between\n",
    "            thresh = t\n",
    "    return int(thresh)\n",
    "\n",
    "def preprocess(img: Image.Image, scale=1.6, use_otsu=True, fixed_thresh=205) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Robust preprocess:\n",
    "    - grayscale + autocontrast\n",
    "    - upscale (helps small text)\n",
    "    - mild denoise\n",
    "    - Otsu binarize (adaptive) or fixed fallback\n",
    "    \"\"\"\n",
    "    img = img.convert(\"L\")\n",
    "    img = ImageOps.autocontrast(img)\n",
    "\n",
    "    if scale != 1.0:\n",
    "        img = img.resize((int(img.size[0]*scale), int(img.size[1]*scale)), Image.Resampling.BICUBIC)\n",
    "\n",
    "    img = img.filter(ImageFilter.MedianFilter(size=3))\n",
    "    arr = np.array(img)\n",
    "\n",
    "    thr = otsu_threshold(arr) if use_otsu else fixed_thresh\n",
    "    bw = (arr > thr).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "# -------------------------\n",
    "# OCR (DATA MODE)\n",
    "# -------------------------\n",
    "def ocr_data(img: Image.Image, allow_letters: bool, psm: int) -> dict:\n",
    "    # whitelist: digits + codes + header words for transition pages\n",
    "    if allow_letters:\n",
    "        whitelist = \"0123456789EDGSKINSPICKLETONS\"\n",
    "    else:\n",
    "        whitelist = \"0123456789EDG\"\n",
    "\n",
    "    cfg = (\n",
    "        f\"--oem 3 --psm {psm} \"\n",
    "        \"-c preserve_interword_spaces=1 \"\n",
    "        f\"-c tessedit_char_whitelist={whitelist}\"\n",
    "    )\n",
    "    return pytesseract.image_to_data(img, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "def clean_token(t: str) -> str:\n",
    "    # Keep alnum only, upper-case\n",
    "    t = (t or \"\").strip().upper()\n",
    "    t = re.sub(r\"[^A-Z0-9]\", \"\", t)\n",
    "    return t\n",
    "\n",
    "def cluster_rows(tokens, row_tol: float):\n",
    "    \"\"\"\n",
    "    tokens: list of dicts with y (center), x (center), text, conf\n",
    "    returns list of rows, each row is list of tokens\n",
    "    \"\"\"\n",
    "    tokens = sorted(tokens, key=lambda d: d[\"y\"])\n",
    "    rows = []\n",
    "    cur = []\n",
    "    cur_y = None\n",
    "\n",
    "    for tok in tokens:\n",
    "        if cur_y is None:\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "            continue\n",
    "        if abs(tok[\"y\"] - cur_y) <= row_tol:\n",
    "            cur.append(tok)\n",
    "            # update running y (robust-ish)\n",
    "            cur_y = (cur_y * (len(cur)-1) + tok[\"y\"]) / len(cur)\n",
    "        else:\n",
    "            rows.append(cur)\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "    if cur:\n",
    "        rows.append(cur)\n",
    "    return rows\n",
    "\n",
    "def split_subtokens(text: str):\n",
    "    \"\"\"\n",
    "    Split merged patterns like:\n",
    "      1012E3757  -> 1012, E, 3757\n",
    "      1012E      -> 1012, E\n",
    "      E3757      -> E, 3757\n",
    "    Returns list of subtokens as strings.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    m = re.fullmatch(r\"(\\d{3,6})([EDG])(\\d{2,7})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2), m.group(3)]\n",
    "    m = re.fullmatch(r\"(\\d{3,6})([EDG])\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    m = re.fullmatch(r\"([EDG])(\\d{2,7})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [text]\n",
    "\n",
    "def extract_triplets_spatial(img_bw: Image.Image, allow_letters: bool, psm: int, conf_floor: int = 0):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      triplets: list of dicts {Accession, Code, Catalog, x, y, conf}\n",
    "      header_y: dict with keys 'PICKLES'/'SKELETONS' if found\n",
    "      debug_lines: list[str] approximating rows (for page_###.txt dump)\n",
    "    \"\"\"\n",
    "    w, h = img_bw.size\n",
    "    data = ocr_data(img_bw, allow_letters=allow_letters, psm=psm)\n",
    "\n",
    "    tokens = []\n",
    "    header_y = {\"PICKLES\": None, \"SKELETONS\": None}\n",
    "\n",
    "    n = len(data[\"text\"])\n",
    "    for i in range(n):\n",
    "        raw = data[\"text\"][i]\n",
    "        if raw is None or raw.strip() == \"\":\n",
    "            continue\n",
    "        try:\n",
    "            conf = float(data[\"conf\"][i])\n",
    "        except Exception:\n",
    "            conf = -1.0\n",
    "        if conf < conf_floor:\n",
    "            continue\n",
    "\n",
    "        t = clean_token(raw)\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        left = int(data[\"left\"][i]); top = int(data[\"top\"][i])\n",
    "        width = int(data[\"width\"][i]); height = int(data[\"height\"][i])\n",
    "        x = left + width/2.0\n",
    "        y = top + height/2.0\n",
    "\n",
    "        # header detection\n",
    "        if \"PICKLES\" in t:\n",
    "            header_y[\"PICKLES\"] = y if header_y[\"PICKLES\"] is None else min(header_y[\"PICKLES\"], y)\n",
    "        if \"SKELETONS\" in t:\n",
    "            header_y[\"SKELETONS\"] = y if header_y[\"SKELETONS\"] is None else min(header_y[\"SKELETONS\"], y)\n",
    "\n",
    "        tokens.append({\"text\": t, \"x\": x, \"y\": y, \"conf\": conf})\n",
    "\n",
    "    # Cluster into rows\n",
    "    # row tolerance depends on scale; for scaled images, rows are farther apart\n",
    "    # This works well empirically:\n",
    "    row_tol = max(10.0, h * 0.006)  # ~0.6% of height\n",
    "\n",
    "    rows = cluster_rows(tokens, row_tol=row_tol)\n",
    "\n",
    "    triplets = []\n",
    "    debug_lines = []\n",
    "\n",
    "    for row in rows:\n",
    "        row_sorted = sorted(row, key=lambda d: d[\"x\"])\n",
    "        # build subtoken stream with approximate x/y/conf\n",
    "        stream = []\n",
    "        for tok in row_sorted:\n",
    "            subs = split_subtokens(tok[\"text\"])\n",
    "            for s in subs:\n",
    "                stream.append({\"text\": s, \"x\": tok[\"x\"], \"y\": tok[\"y\"], \"conf\": tok[\"conf\"]})\n",
    "\n",
    "        # debug row text\n",
    "        debug_lines.append(\" \".join([s[\"text\"] for s in stream]))\n",
    "\n",
    "        # scan stream for num, code, num\n",
    "        j = 0\n",
    "        while j <= len(stream) - 3:\n",
    "            a = stream[j][\"text\"]\n",
    "            c = stream[j+1][\"text\"]\n",
    "            b = stream[j+2][\"text\"]\n",
    "            if re.fullmatch(r\"\\d{3,6}\", a) and c in (\"E\", \"D\", \"G\") and re.fullmatch(r\"\\d{2,7}\", b):\n",
    "                triplets.append({\n",
    "                    \"Accession\": int(a),\n",
    "                    \"Code\": c,\n",
    "                    \"Catalog\": int(b),\n",
    "                    \"x\": stream[j][\"x\"],\n",
    "                    \"y\": stream[j][\"y\"],\n",
    "                    \"conf\": (stream[j][\"conf\"] + stream[j+1][\"conf\"] + stream[j+2][\"conf\"]) / 3.0\n",
    "                })\n",
    "                j += 3\n",
    "            else:\n",
    "                j += 1\n",
    "\n",
    "    return triplets, header_y, debug_lines, (w, h)\n",
    "\n",
    "def assign_type(page: int, triplet: dict, header_y: dict, img_w: int) -> str | None:\n",
    "    if page <= 98:\n",
    "        return \"SKINS\"\n",
    "\n",
    "    if page == 99:\n",
    "        # Type changes only in the RIGHT column after the PICKLES header appears.\n",
    "        pick_y = header_y.get(\"PICKLES\")\n",
    "        if pick_y is None:\n",
    "            # fallback heuristic\n",
    "            return \"PICKLES\" if triplet[\"Catalog\"] < 10000 else \"SKINS\"\n",
    "        right_col = triplet[\"x\"] > (img_w * 0.66)\n",
    "        if right_col and triplet[\"y\"] >= pick_y:\n",
    "            return \"PICKLES\"\n",
    "        return \"SKINS\"\n",
    "\n",
    "    if page == 100:\n",
    "        sk_y = header_y.get(\"SKELETONS\")\n",
    "        if sk_y is None:\n",
    "            return None\n",
    "        return \"PICKLES\" if triplet[\"y\"] < sk_y else \"SKELETONS\"\n",
    "\n",
    "    # 101-102\n",
    "    return \"SKELETONS\"\n",
    "\n",
    "@dataclass\n",
    "class PageResult:\n",
    "    page: int\n",
    "    rows: list\n",
    "    extracted: int\n",
    "    expected: int\n",
    "    error: str\n",
    "    image_file: str\n",
    "    debug_file: str\n",
    "    attempts_used: int\n",
    "\n",
    "def process_page(task):\n",
    "    page, path = task\n",
    "    expected = EXPECTED[page]\n",
    "    allow_letters = page in (99, 100)\n",
    "\n",
    "    # attempts: (scale, use_otsu, fixed_thresh, psm, conf_floor)\n",
    "    # Conf_floor: lowering helps recover faint tokens but may add false positives.\n",
    "    attempts = [\n",
    "        (1.6, True,  205, 6,  0),\n",
    "        (2.0, True,  205, 6,  0),\n",
    "        (2.2, True,  205, 6,  0),\n",
    "        (2.0, False, 200, 6,  0),\n",
    "        (2.2, False, 195, 6,  0),\n",
    "        (2.2, True,  205, 4,  0),   # different page segmentation\n",
    "        (2.2, True,  205, 6, -1),   # include lower-confidence tokens (last resort)\n",
    "    ]\n",
    "\n",
    "    img = Image.open(path)\n",
    "\n",
    "    best = None\n",
    "    best_diff = 10**9\n",
    "    best_debug_lines = None\n",
    "    best_header_y = None\n",
    "    best_imgw = None\n",
    "    attempts_used = 0\n",
    "\n",
    "    for (scale, use_otsu, fixed_thr, psm, conf_floor) in attempts:\n",
    "        attempts_used += 1\n",
    "\n",
    "        bw = preprocess(img, scale=scale, use_otsu=use_otsu, fixed_thresh=fixed_thr)\n",
    "        triplets, header_y, debug_lines, (imgw, imgh) = extract_triplets_spatial(\n",
    "            bw, allow_letters=allow_letters, psm=psm, conf_floor=conf_floor\n",
    "        )\n",
    "\n",
    "        # Deduplicate by (A,CODE,B) keeping max confidence\n",
    "        by_key = {}\n",
    "        for t in triplets:\n",
    "            k = (t[\"Accession\"], t[\"Code\"], t[\"Catalog\"])\n",
    "            if k not in by_key or t[\"conf\"] > by_key[k][\"conf\"]:\n",
    "                by_key[k] = t\n",
    "        triplets = list(by_key.values())\n",
    "\n",
    "        # If too many, prune by confidence to expected\n",
    "        if len(triplets) > expected:\n",
    "            triplets.sort(key=lambda d: d[\"conf\"], reverse=True)\n",
    "            triplets = triplets[:expected]\n",
    "\n",
    "        diff = abs(len(triplets) - expected)\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best = (triplets, header_y, imgw)\n",
    "            best_debug_lines = debug_lines\n",
    "            best_header_y = header_y\n",
    "            best_imgw = imgw\n",
    "\n",
    "        if len(triplets) == expected:\n",
    "            # Great, stop early\n",
    "            best = (triplets, header_y, imgw)\n",
    "            best_debug_lines = debug_lines\n",
    "            best_header_y = header_y\n",
    "            best_imgw = imgw\n",
    "            break\n",
    "\n",
    "    triplets, header_y, imgw = best\n",
    "\n",
    "    # Assign types\n",
    "    out_rows = []\n",
    "    unassigned = False\n",
    "    for t in triplets:\n",
    "        typ = assign_type(page, t, header_y, img_w=imgw)\n",
    "        if typ is None:\n",
    "            unassigned = True\n",
    "        out_rows.append({\n",
    "            \"Accession\": t[\"Accession\"],\n",
    "            \"Code\": t[\"Code\"],\n",
    "            \"Catalog\": t[\"Catalog\"],\n",
    "            \"Type\": typ,\n",
    "            \"Page\": page\n",
    "        })\n",
    "\n",
    "    # Write debug file (row-ish token dump)\n",
    "    debug_file = os.path.join(OCR_TEXT_DIR, f\"page_{page:03d}.txt\")\n",
    "    with open(debug_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"PAGE {page}\\n\")\n",
    "        f.write(f\"EXPECTED {expected}\\n\")\n",
    "        f.write(f\"EXTRACTED {len(out_rows)}\\n\")\n",
    "        f.write(f\"HEADERS {header_y}\\n\\n\")\n",
    "        for ln in (best_debug_lines or []):\n",
    "            f.write(ln + \"\\n\")\n",
    "\n",
    "    errors = []\n",
    "    if len(out_rows) != expected:\n",
    "        errors.append(f\"COUNT_MISMATCH {len(out_rows)} != {expected}\")\n",
    "    if page == 100 and header_y.get(\"SKELETONS\") is None:\n",
    "        errors.append(\"NO_SKELETONS_HEADER_FOUND\")\n",
    "    if unassigned:\n",
    "        errors.append(\"UNASSIGNED_TYPE\")\n",
    "\n",
    "    return PageResult(\n",
    "        page=page,\n",
    "        rows=out_rows,\n",
    "        extracted=len(out_rows),\n",
    "        expected=expected,\n",
    "        error=\"; \".join(errors),\n",
    "        image_file=path,\n",
    "        debug_file=debug_file,\n",
    "        attempts_used=attempts_used\n",
    "    )\n",
    "\n",
    "def run_all():\n",
    "    ensure_out_dirs()\n",
    "    paths = list_pages_from_folder()\n",
    "    tasks = [(i+1, paths[i]) for i in range(102)]\n",
    "\n",
    "    print(f\"[Start] Processing 102 pages from .\\\\{JPEGS_DIR}\\\\ with {N_WORKERS} workers\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    results = []\n",
    "    done = 0\n",
    "    retry_pages = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=N_WORKERS) as ex:\n",
    "        futures = [ex.submit(process_page, t) for t in tasks]\n",
    "        for fut in as_completed(futures):\n",
    "            res = fut.result()\n",
    "            results.append(res)\n",
    "            done += 1\n",
    "\n",
    "            if res.attempts_used > 1:\n",
    "                retry_pages += 1\n",
    "\n",
    "            if done % 10 == 0 or done == 102:\n",
    "                elapsed = time.time() - t0\n",
    "                rate = done / elapsed if elapsed > 0 else 0.0\n",
    "                eta = (102 - done) / rate if rate > 0 else float(\"inf\")\n",
    "                print(f\"  completed {done}/102 | {rate:.2f} pages/sec | ETA ~ {eta/60:.1f} min\")\n",
    "\n",
    "    results.sort(key=lambda r: r.page)\n",
    "\n",
    "    # page_counts + needs_review\n",
    "    page_counts = pd.DataFrame([{\n",
    "        \"Page\": r.page,\n",
    "        \"RowsExtracted\": r.extracted,\n",
    "        \"RowsExpected\": r.expected,\n",
    "        \"Error\": r.error,\n",
    "        \"AttemptsUsed\": r.attempts_used,\n",
    "        \"ImageFile\": r.image_file,\n",
    "        \"DebugFile\": r.debug_file\n",
    "    } for r in results])\n",
    "\n",
    "    page_counts.to_csv(PAGE_COUNTS_CSV, index=False)\n",
    "\n",
    "    needs_review = page_counts[page_counts[\"Error\"].astype(str).str.len() > 0].copy()\n",
    "    needs_review.to_csv(NEEDS_REVIEW_CSV, index=False)\n",
    "\n",
    "    all_rows = pd.DataFrame([row for r in results for row in r.rows])\n",
    "    all_rows.to_csv(OUTPUT_ALL_CSV, index=False)\n",
    "\n",
    "    good_pages = set(page_counts[page_counts[\"Error\"] == \"\"][\"Page\"].tolist())\n",
    "    safe_rows = all_rows[all_rows[\"Page\"].isin(good_pages)].copy()\n",
    "    safe_rows.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "    print(\"\\n[Done] Outputs:\")\n",
    "    print(f\"  - {OUTPUT_CSV}         rows={len(safe_rows)} (only pages that passed checks)\")\n",
    "    print(f\"  - {OUTPUT_ALL_CSV}     rows={len(all_rows)} (everything)\")\n",
    "    print(f\"  - {PAGE_COUNTS_CSV}\")\n",
    "    print(f\"  - {NEEDS_REVIEW_CSV}   pages_flagged={len(needs_review)}\")\n",
    "    print(f\"  - Debug dumps in {OCR_TEXT_DIR}\\\\page_###.txt\")\n",
    "    print(f\"  - Pages that needed retries: ~{retry_pages}/102\")\n",
    "\n",
    "    return page_counts, needs_review, safe_rows, all_rows\n",
    "\n",
    "page_counts_df, needs_review_df, safe_df, all_df = run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1173ee-c93c-4f9e-822f-965e55ff5f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Duplicate files for 102 pages. Will auto-try alternates if needed.\n",
      "       Examples: 1:2, 2:2, 3:2, 4:2, 5:2, 6:2, 7:2, 8:2, 9:2, 10:2, 11:2, 12:2 ...\n",
      "[Start] Processing 102 pages from .\\JPEGS\\ with 6 workers\n",
      "  completed 10/102 | 0.65 pages/sec | ETA ~ 2.4 min\n",
      "  completed 20/102 | 0.39 pages/sec | ETA ~ 3.5 min\n",
      "  completed 30/102 | 0.23 pages/sec | ETA ~ 5.2 min\n",
      "  completed 40/102 | 0.24 pages/sec | ETA ~ 4.3 min\n",
      "  completed 50/102 | 0.22 pages/sec | ETA ~ 3.9 min\n",
      "  completed 60/102 | 0.21 pages/sec | ETA ~ 3.3 min\n",
      "  completed 70/102 | 0.20 pages/sec | ETA ~ 2.6 min\n",
      "  completed 80/102 | 0.21 pages/sec | ETA ~ 1.7 min\n",
      "  completed 90/102 | 0.20 pages/sec | ETA ~ 1.0 min\n",
      "  completed 100/102 | 0.22 pages/sec | ETA ~ 0.2 min\n",
      "  completed 102/102 | 0.22 pages/sec | ETA ~ 0.0 min\n",
      "\n",
      "[Done] Outputs:\n",
      "  - out\\output.csv         rows=12735 (only pages that passed checks)\n",
      "  - out\\output_all_rows.csv     rows=16392 (everything)\n",
      "  - out\\page_counts.csv\n",
      "  - out\\needs_review.csv   pages_flagged=23\n",
      "  - Debug dumps: out\\ocr_text\\page_###.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# EXPECTED ROW COUNTS\n",
    "# -------------------------\n",
    "EXPECTED = {1: 141}\n",
    "for p in range(2, 99):\n",
    "    EXPECTED[p] = 162\n",
    "EXPECTED[99]  = 160\n",
    "EXPECTED[100] = 160\n",
    "EXPECTED[101] = 162\n",
    "EXPECTED[102] = 124\n",
    "\n",
    "# -------------------------\n",
    "# IMPORTANT FIX:\n",
    "# allow 1–6 digit accession (page 31 has accession 39)\n",
    "# -------------------------\n",
    "ACC_RE = r\"\\d{1,6}\"\n",
    "CAT_RE = r\"\\d{1,7}\"     # catalog can be small (e.g. 43) or big (e.g. 120921)\n",
    "\n",
    "def ensure_out_dirs():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    os.makedirs(OCR_TEXT_DIR, exist_ok=True)\n",
    "\n",
    "def page_num_from_path(p: str) -> int:\n",
    "    stem = os.path.splitext(os.path.basename(p))[0]\n",
    "    m = re.search(r\"(\\d+)\", stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse page number from: {p}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def edge_score(path: str) -> float:\n",
    "    \"\"\"Cheap sharpness metric to rank duplicate scans.\"\"\"\n",
    "    try:\n",
    "        with Image.open(path) as im:\n",
    "            im = im.convert(\"L\")\n",
    "            im = im.resize((600, int(600 * im.size[1] / im.size[0])), Image.Resampling.BICUBIC)\n",
    "            arr = np.asarray(im, dtype=np.float32)\n",
    "        gx = np.abs(np.diff(arr, axis=1)).mean()\n",
    "        gy = np.abs(np.diff(arr, axis=0)).mean()\n",
    "        return float(gx + gy)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def list_pages_with_duplicates():\n",
    "    \"\"\"Return dict: page -> list of candidate image paths (sorted best-first).\"\"\"\n",
    "    if not os.path.isdir(JPEGS_DIR):\n",
    "        raise FileNotFoundError(f\"Folder not found: .\\\\{JPEGS_DIR}\\\\ (cwd: {os.getcwd()})\")\n",
    "\n",
    "    jpgs = glob.glob(os.path.join(JPEGS_DIR, \"*.jpg\")) + glob.glob(os.path.join(JPEGS_DIR, \"*.JPG\"))\n",
    "    if not jpgs:\n",
    "        raise FileNotFoundError(f\"No .jpg files found in .\\\\{JPEGS_DIR}\\\\\")\n",
    "\n",
    "    by_page = defaultdict(list)\n",
    "    for p in jpgs:\n",
    "        try:\n",
    "            pg = page_num_from_path(p)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if 1 <= pg <= 102:\n",
    "            by_page[pg].append(p)\n",
    "\n",
    "    missing = [pg for pg in range(1, 103) if pg not in by_page]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing required pages: {missing}\")\n",
    "\n",
    "    dup_pages = [(pg, len(paths)) for pg, paths in by_page.items() if len(paths) > 1]\n",
    "    if dup_pages:\n",
    "        dup_pages.sort()\n",
    "        print(f\"[Info] Duplicate files for {len(dup_pages)} pages. Will auto-try alternates if needed.\")\n",
    "        print(\"       Examples:\", \", \".join([f\"{pg}:{n}\" for pg, n in dup_pages[:12]]) + (\" ...\" if len(dup_pages) > 12 else \"\"))\n",
    "\n",
    "    # Sort candidates best-first by sharpness score, tie-break by file size\n",
    "    for pg in range(1, 103):\n",
    "        paths = by_page[pg]\n",
    "        scored = []\n",
    "        for p in paths:\n",
    "            scored.append((edge_score(p), os.path.getsize(p), p))\n",
    "        scored.sort(reverse=True)\n",
    "        by_page[pg] = [p for _, __, p in scored]\n",
    "\n",
    "    return by_page\n",
    "\n",
    "# -------------------------\n",
    "# IMAGE PREPROCESSING\n",
    "# -------------------------\n",
    "def otsu_threshold(arr: np.ndarray) -> int:\n",
    "    hist = np.bincount(arr.ravel(), minlength=256).astype(np.float64)\n",
    "    total = arr.size\n",
    "    if total == 0:\n",
    "        return 200\n",
    "    sum_total = np.dot(np.arange(256), hist)\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    max_var = -1.0\n",
    "    thresh = 200\n",
    "    for t in range(256):\n",
    "        w_b += hist[t]\n",
    "        if w_b == 0:\n",
    "            continue\n",
    "        w_f = total - w_b\n",
    "        if w_f == 0:\n",
    "            break\n",
    "        sum_b += t * hist[t]\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > max_var:\n",
    "            max_var = var_between\n",
    "            thresh = t\n",
    "    return int(thresh)\n",
    "\n",
    "def preprocess(img: Image.Image, scale=2.0, use_otsu=True, fixed_thresh=205) -> Image.Image:\n",
    "    img = img.convert(\"L\")\n",
    "    img = ImageOps.autocontrast(img)\n",
    "\n",
    "    if scale != 1.0:\n",
    "        img = img.resize((int(img.size[0]*scale), int(img.size[1]*scale)), Image.Resampling.BICUBIC)\n",
    "\n",
    "    img = img.filter(ImageFilter.MedianFilter(size=3))\n",
    "    arr = np.array(img)\n",
    "    thr = otsu_threshold(arr) if use_otsu else fixed_thresh\n",
    "    bw = (arr > thr).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "# -------------------------\n",
    "# OCR DATA MODE\n",
    "# -------------------------\n",
    "def ocr_data(img: Image.Image, allow_letters: bool, psm: int) -> dict:\n",
    "    if allow_letters:\n",
    "        whitelist = \"0123456789EDGSKINSPICKLETONS\"\n",
    "    else:\n",
    "        whitelist = \"0123456789EDG\"\n",
    "    cfg = (\n",
    "        f\"--oem 3 --psm {psm} \"\n",
    "        \"-c preserve_interword_spaces=1 \"\n",
    "        f\"-c tessedit_char_whitelist={whitelist}\"\n",
    "    )\n",
    "    return pytesseract.image_to_data(img, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "def clean_token(t: str) -> str:\n",
    "    t = (t or \"\").strip().upper()\n",
    "    t = re.sub(r\"[^A-Z0-9]\", \"\", t)\n",
    "    return t\n",
    "\n",
    "def split_subtokens(text: str):\n",
    "    # Accept 1–6 digit accession; allow merged tokens like 39E28\n",
    "    if not text:\n",
    "        return []\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2), m.group(3)]\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    m = re.fullmatch(rf\"([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [text]\n",
    "\n",
    "def cluster_rows(tokens, row_tol: float):\n",
    "    tokens = sorted(tokens, key=lambda d: d[\"y\"])\n",
    "    rows = []\n",
    "    cur = []\n",
    "    cur_y = None\n",
    "    for tok in tokens:\n",
    "        if cur_y is None:\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "            continue\n",
    "        if abs(tok[\"y\"] - cur_y) <= row_tol:\n",
    "            cur.append(tok)\n",
    "            cur_y = (cur_y * (len(cur)-1) + tok[\"y\"]) / len(cur)\n",
    "        else:\n",
    "            rows.append(cur)\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "    if cur:\n",
    "        rows.append(cur)\n",
    "    return rows\n",
    "\n",
    "def extract_triplets_spatial(img_bw: Image.Image, allow_letters: bool, psm: int, conf_floor: int = -1):\n",
    "    w, h = img_bw.size\n",
    "    data = ocr_data(img_bw, allow_letters=allow_letters, psm=psm)\n",
    "\n",
    "    tokens = []\n",
    "    header_y = {\"PICKLES\": None, \"SKELETONS\": None}\n",
    "    n = len(data[\"text\"])\n",
    "    for i in range(n):\n",
    "        raw = data[\"text\"][i]\n",
    "        if raw is None or raw.strip() == \"\":\n",
    "            continue\n",
    "        try:\n",
    "            conf = float(data[\"conf\"][i])\n",
    "        except Exception:\n",
    "            conf = -1.0\n",
    "        if conf < conf_floor:\n",
    "            continue\n",
    "\n",
    "        t = clean_token(raw)\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        left = int(data[\"left\"][i]); top = int(data[\"top\"][i])\n",
    "        width = int(data[\"width\"][i]); height = int(data[\"height\"][i])\n",
    "        x = left + width/2.0\n",
    "        y = top + height/2.0\n",
    "\n",
    "        if \"PICKLES\" in t:\n",
    "            header_y[\"PICKLES\"] = y if header_y[\"PICKLES\"] is None else min(header_y[\"PICKLES\"], y)\n",
    "        if \"SKELETONS\" in t:\n",
    "            header_y[\"SKELETONS\"] = y if header_y[\"SKELETONS\"] is None else min(header_y[\"SKELETONS\"], y)\n",
    "\n",
    "        tokens.append({\"text\": t, \"x\": x, \"y\": y, \"conf\": conf})\n",
    "\n",
    "    row_tol = max(12.0, h * 0.007)\n",
    "    rows = cluster_rows(tokens, row_tol=row_tol)\n",
    "\n",
    "    triplets = []\n",
    "    debug_lines = []\n",
    "\n",
    "    for row in rows:\n",
    "        row_sorted = sorted(row, key=lambda d: d[\"x\"])\n",
    "        stream = []\n",
    "        for tok in row_sorted:\n",
    "            subs = split_subtokens(tok[\"text\"])\n",
    "            for s in subs:\n",
    "                stream.append({\"text\": s, \"x\": tok[\"x\"], \"y\": tok[\"y\"], \"conf\": tok[\"conf\"]})\n",
    "\n",
    "        debug_lines.append(\" \".join([s[\"text\"] for s in stream]))\n",
    "\n",
    "        j = 0\n",
    "        while j <= len(stream) - 3:\n",
    "            a = stream[j][\"text\"]\n",
    "            c = stream[j+1][\"text\"]\n",
    "            b = stream[j+2][\"text\"]\n",
    "            if re.fullmatch(ACC_RE, a) and c in (\"E\",\"D\",\"G\") and re.fullmatch(CAT_RE, b):\n",
    "                triplets.append({\n",
    "                    \"Accession\": int(a),\n",
    "                    \"Code\": c,\n",
    "                    \"Catalog\": int(b),\n",
    "                    \"x\": stream[j][\"x\"],\n",
    "                    \"y\": stream[j][\"y\"],\n",
    "                    \"conf\": (stream[j][\"conf\"] + stream[j+1][\"conf\"] + stream[j+2][\"conf\"]) / 3.0\n",
    "                })\n",
    "                j += 3\n",
    "            else:\n",
    "                j += 1\n",
    "\n",
    "    # Deduplicate by key keeping max confidence\n",
    "    by_key = {}\n",
    "    for t in triplets:\n",
    "        k = (t[\"Accession\"], t[\"Code\"], t[\"Catalog\"])\n",
    "        if k not in by_key or t[\"conf\"] > by_key[k][\"conf\"]:\n",
    "            by_key[k] = t\n",
    "    triplets = list(by_key.values())\n",
    "\n",
    "    return triplets, header_y, debug_lines, (w, h)\n",
    "\n",
    "def extract_triplets_columnwise(img: Image.Image, allow_letters: bool, psm: int, conf_floor: int):\n",
    "    \"\"\"\n",
    "    Split into 3 vertical columns and OCR each column independently.\n",
    "    Great for fixing the stubborn 161/162 pages.\n",
    "    \"\"\"\n",
    "    w, h = img.size\n",
    "    # add a small overlap so nothing on the seam gets lost\n",
    "    g = int(w * 0.02)\n",
    "    x1 = int(w * 0.333)\n",
    "    x2 = int(w * 0.666)\n",
    "\n",
    "    boxes = [\n",
    "        (0,      0, x1+g,   h),\n",
    "        (x1-g,   0, x2+g,   h),\n",
    "        (x2-g,   0, w,      h),\n",
    "    ]\n",
    "\n",
    "    all_triplets = []\n",
    "    all_debug = []\n",
    "    header_y = {\"PICKLES\": None, \"SKELETONS\": None}\n",
    "\n",
    "    for (l,t,r,b) in boxes:\n",
    "        crop = img.crop((l,t,r,b))\n",
    "        bw = preprocess(crop, scale=2.0, use_otsu=True)\n",
    "        triplets, hy, dbg, _ = extract_triplets_spatial(bw, allow_letters=allow_letters, psm=psm, conf_floor=conf_floor)\n",
    "        # merge header info (mostly unused except 99/100, which we won't column-split anyway)\n",
    "        for k in header_y:\n",
    "            if hy.get(k) is not None:\n",
    "                header_y[k] = hy[k] if header_y[k] is None else min(header_y[k], hy[k])\n",
    "        all_triplets.extend(triplets)\n",
    "        all_debug.extend(dbg)\n",
    "\n",
    "    # Deduplicate across columns (rare but safe)\n",
    "    by_key = {}\n",
    "    for t in all_triplets:\n",
    "        k = (t[\"Accession\"], t[\"Code\"], t[\"Catalog\"])\n",
    "        if k not in by_key or t[\"conf\"] > by_key[k][\"conf\"]:\n",
    "            by_key[k] = t\n",
    "    return list(by_key.values()), header_y, all_debug, (w, h)\n",
    "\n",
    "def assign_type(page: int, t: dict, header_y: dict, img_w: int) -> str | None:\n",
    "    if page <= 98:\n",
    "        return \"SKINS\"\n",
    "    if page == 99:\n",
    "        pick_y = header_y.get(\"PICKLES\")\n",
    "        if pick_y is None:\n",
    "            return \"PICKLES\" if t[\"Catalog\"] < 10000 else \"SKINS\"\n",
    "        right_col = t[\"x\"] > (img_w * 0.66)\n",
    "        return \"PICKLES\" if (right_col and t[\"y\"] >= pick_y) else \"SKINS\"\n",
    "    if page == 100:\n",
    "        sk_y = header_y.get(\"SKELETONS\")\n",
    "        if sk_y is None:\n",
    "            return None\n",
    "        return \"PICKLES\" if t[\"y\"] < sk_y else \"SKELETONS\"\n",
    "    return \"SKELETONS\"\n",
    "\n",
    "@dataclass\n",
    "class PageResult:\n",
    "    page: int\n",
    "    rows: list\n",
    "    extracted: int\n",
    "    expected: int\n",
    "    error: str\n",
    "    attempts_used: int\n",
    "    image_file: str\n",
    "    debug_file: str\n",
    "\n",
    "def try_extract_from_image(page: int, path: str):\n",
    "    expected = EXPECTED[page]\n",
    "    allow_letters = page in (99, 100)\n",
    "\n",
    "    # Attempts: full-page first, then columnwise for standard pages\n",
    "    attempts = [\n",
    "        (\"full\",  2.0, True,  205, 6, -1),\n",
    "        (\"full\",  2.2, True,  205, 6, -1),\n",
    "        (\"full\",  2.2, True,  205, 4, -1),\n",
    "        (\"full\",  2.0, False, 200, 6, -1),\n",
    "    ]\n",
    "\n",
    "    # Columnwise fallback for the \"boring\" 3-column pages\n",
    "    do_cols = (2 <= page <= 98) or (page == 101)\n",
    "\n",
    "    with Image.open(path) as im0:\n",
    "        im = im0.copy()\n",
    "\n",
    "    best = None\n",
    "    best_diff = 10**9\n",
    "    best_debug = None\n",
    "    best_header_y = None\n",
    "    best_imgw = None\n",
    "    attempts_used = 0\n",
    "\n",
    "    for mode, scale, use_otsu, fixed_thr, psm, conf_floor in attempts:\n",
    "        attempts_used += 1\n",
    "        bw = preprocess(im, scale=scale, use_otsu=use_otsu, fixed_thresh=fixed_thr)\n",
    "        triplets, header_y, debug_lines, (imgw, _) = extract_triplets_spatial(bw, allow_letters=allow_letters, psm=psm, conf_floor=conf_floor)\n",
    "\n",
    "        # if too many, prune by confidence\n",
    "        if len(triplets) > expected:\n",
    "            triplets.sort(key=lambda d: d[\"conf\"], reverse=True)\n",
    "            triplets = triplets[:expected]\n",
    "\n",
    "        diff = abs(len(triplets) - expected)\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best = triplets\n",
    "            best_debug = debug_lines\n",
    "            best_header_y = header_y\n",
    "            best_imgw = imgw\n",
    "\n",
    "        if len(triplets) == expected:\n",
    "            break\n",
    "\n",
    "    # Columnwise fallback if still wrong and applicable\n",
    "    if do_cols and best_diff != 0:\n",
    "        col_attempts = [\n",
    "            (6, -1),\n",
    "            (4, -1),\n",
    "        ]\n",
    "        for psm, conf_floor in col_attempts:\n",
    "            attempts_used += 1\n",
    "            triplets, header_y, debug_lines, (imgw, _) = extract_triplets_columnwise(im, allow_letters=False, psm=psm, conf_floor=conf_floor)\n",
    "\n",
    "            if len(triplets) > expected:\n",
    "                triplets.sort(key=lambda d: d[\"conf\"], reverse=True)\n",
    "                triplets = triplets[:expected]\n",
    "\n",
    "            diff = abs(len(triplets) - expected)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best = triplets\n",
    "                best_debug = debug_lines\n",
    "                best_header_y = header_y\n",
    "                best_imgw = imgw\n",
    "\n",
    "            if len(triplets) == expected:\n",
    "                break\n",
    "\n",
    "    # Build rows with Type + Page\n",
    "    rows = []\n",
    "    unassigned = False\n",
    "    for t in best:\n",
    "        typ = assign_type(page, t, best_header_y, img_w=best_imgw)\n",
    "        if typ is None:\n",
    "            unassigned = True\n",
    "        rows.append({\n",
    "            \"Accession\": t[\"Accession\"],\n",
    "            \"Code\": t[\"Code\"],\n",
    "            \"Catalog\": t[\"Catalog\"],\n",
    "            \"Type\": typ,\n",
    "            \"Page\": page\n",
    "        })\n",
    "\n",
    "    # Debug dump\n",
    "    debug_file = os.path.join(OCR_TEXT_DIR, f\"page_{page:03d}.txt\")\n",
    "    with open(debug_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"PAGE {page}\\nEXPECTED {expected}\\nEXTRACTED {len(rows)}\\nIMAGE {path}\\n\")\n",
    "        f.write(f\"HEADERS {best_header_y}\\nATTEMPTS_USED {attempts_used}\\n\\n\")\n",
    "        for ln in (best_debug or []):\n",
    "            f.write(ln + \"\\n\")\n",
    "\n",
    "    errors = []\n",
    "    if len(rows) != expected:\n",
    "        errors.append(f\"COUNT_MISMATCH {len(rows)} != {expected}\")\n",
    "    if page == 100 and best_header_y.get(\"SKELETONS\") is None:\n",
    "        errors.append(\"NO_SKELETONS_HEADER_FOUND\")\n",
    "    if unassigned:\n",
    "        errors.append(\"UNASSIGNED_TYPE\")\n",
    "\n",
    "    return rows, \"; \".join(errors), attempts_used, debug_file\n",
    "\n",
    "def process_page(task):\n",
    "    page, candidates = task\n",
    "    expected = EXPECTED[page]\n",
    "\n",
    "    # Try best-first candidate scans; stop early on exact match\n",
    "    best_rows = None\n",
    "    best_err = \"INIT\"\n",
    "    best_attempts = 0\n",
    "    best_path = candidates[0]\n",
    "    best_debug = \"\"\n",
    "\n",
    "    best_diff = 10**9\n",
    "\n",
    "    for path in candidates:\n",
    "        rows, err, attempts_used, debug_file = try_extract_from_image(page, path)\n",
    "        diff = abs(len(rows) - expected)\n",
    "\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best_rows = rows\n",
    "            best_err = err\n",
    "            best_attempts = attempts_used\n",
    "            best_path = path\n",
    "            best_debug = debug_file\n",
    "\n",
    "        if diff == 0 and err == \"\":\n",
    "            break\n",
    "\n",
    "    return PageResult(\n",
    "        page=page,\n",
    "        rows=best_rows,\n",
    "        extracted=len(best_rows),\n",
    "        expected=expected,\n",
    "        error=best_err,\n",
    "        attempts_used=best_attempts,\n",
    "        image_file=best_path,\n",
    "        debug_file=best_debug\n",
    "    )\n",
    "\n",
    "def run_all():\n",
    "    ensure_out_dirs()\n",
    "    by_page = list_pages_with_duplicates()\n",
    "    tasks = [(p, by_page[p]) for p in range(1, 103)]\n",
    "\n",
    "    print(f\"[Start] Processing 102 pages from .\\\\{JPEGS_DIR}\\\\ with {N_WORKERS} workers\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    results = []\n",
    "    done = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=N_WORKERS) as ex:\n",
    "        futures = [ex.submit(process_page, t) for t in tasks]\n",
    "        for fut in as_completed(futures):\n",
    "            res = fut.result()\n",
    "            results.append(res)\n",
    "            done += 1\n",
    "\n",
    "            if done % 10 == 0 or done == 102:\n",
    "                elapsed = time.time() - t0\n",
    "                rate = done / elapsed if elapsed > 0 else 0.0\n",
    "                eta = (102 - done) / rate if rate > 0 else float(\"inf\")\n",
    "                print(f\"  completed {done}/102 | {rate:.2f} pages/sec | ETA ~ {eta/60:.1f} min\")\n",
    "\n",
    "    results.sort(key=lambda r: r.page)\n",
    "\n",
    "    page_counts = pd.DataFrame([{\n",
    "        \"Page\": r.page,\n",
    "        \"RowsExtracted\": r.extracted,\n",
    "        \"RowsExpected\": r.expected,\n",
    "        \"Error\": r.error,\n",
    "        \"AttemptsUsed\": r.attempts_used,\n",
    "        \"ImageFile\": r.image_file,\n",
    "        \"DebugFile\": r.debug_file\n",
    "    } for r in results])\n",
    "    page_counts.to_csv(PAGE_COUNTS_CSV, index=False)\n",
    "\n",
    "    needs_review = page_counts[page_counts[\"Error\"].astype(str).str.len() > 0].copy()\n",
    "    needs_review.to_csv(NEEDS_REVIEW_CSV, index=False)\n",
    "\n",
    "    all_rows = pd.DataFrame([row for r in results for row in r.rows])\n",
    "    all_rows.to_csv(OUTPUT_ALL_CSV, index=False)\n",
    "\n",
    "    good_pages = set(page_counts[page_counts[\"Error\"] == \"\"][\"Page\"].tolist())\n",
    "    safe_rows = all_rows[all_rows[\"Page\"].isin(good_pages)].copy()\n",
    "    safe_rows.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "    print(\"\\n[Done] Outputs:\")\n",
    "    print(f\"  - {OUTPUT_CSV}         rows={len(safe_rows)} (only pages that passed checks)\")\n",
    "    print(f\"  - {OUTPUT_ALL_CSV}     rows={len(all_rows)} (everything)\")\n",
    "    print(f\"  - {PAGE_COUNTS_CSV}\")\n",
    "    print(f\"  - {NEEDS_REVIEW_CSV}   pages_flagged={len(needs_review)}\")\n",
    "    print(f\"  - Debug dumps: {OCR_TEXT_DIR}\\\\page_###.txt\")\n",
    "\n",
    "    return page_counts, needs_review, safe_rows, all_rows\n",
    "\n",
    "page_counts_df, needs_review_df, safe_df, all_df = run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1adbf182-4cea-4b35-960e-6bb16b19d2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Found exactly 102 pages with 1 image each (no duplicates). ✅\n",
      "[Start] Processing 102 pages from JPEGS with 6 workers\n",
      "  completed 10/102 | 0.49 pages/sec | ETA ~ 3.1 min\n",
      "  completed 20/102 | 0.50 pages/sec | ETA ~ 2.7 min\n",
      "  completed 30/102 | 0.50 pages/sec | ETA ~ 2.4 min\n",
      "  completed 40/102 | 0.50 pages/sec | ETA ~ 2.1 min\n",
      "  completed 50/102 | 0.53 pages/sec | ETA ~ 1.6 min\n",
      "  completed 60/102 | 0.49 pages/sec | ETA ~ 1.4 min\n",
      "  completed 70/102 | 0.46 pages/sec | ETA ~ 1.1 min\n",
      "  completed 80/102 | 0.49 pages/sec | ETA ~ 0.8 min\n",
      "  completed 90/102 | 0.50 pages/sec | ETA ~ 0.4 min\n",
      "  completed 100/102 | 0.52 pages/sec | ETA ~ 0.1 min\n",
      "  completed 102/102 | 0.46 pages/sec | ETA ~ 0.0 min\n",
      "\n",
      "[Done] Outputs:\n",
      "  - out\\output.csv         rows=15527 (only pages that passed checks)\n",
      "  - out\\output_all_rows.csv     rows=16454 (everything)\n",
      "  - out\\page_counts.csv\n",
      "  - out\\needs_review.csv   pages_flagged=6\n",
      "  - Debug dumps: out\\ocr_text\\page_###.txt\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# FILE LISTING (NO ZIP)\n",
    "# =========================\n",
    "def page_num_from_path(p: Path) -> int:\n",
    "    m = re.search(r\"(\\d+)\", p.stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Can't parse page number from: {p.name}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def list_jpgs_unique_by_page(folder: Path):\n",
    "    # One glob that matches .jpg/.JPG/etc without double-counting on Windows\n",
    "    files = list(folder.glob(\"*.[jJ][pP][gG]\"))\n",
    "\n",
    "    # Normalize + de-dupe physical paths (defensive)\n",
    "    normed = []\n",
    "    seen = set()\n",
    "    for f in files:\n",
    "        key = os.path.normcase(str(f.resolve()))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            normed.append(f)\n",
    "\n",
    "    # Group by page number\n",
    "    by_page = {}\n",
    "    collisions = {}\n",
    "    for f in normed:\n",
    "        pg = page_num_from_path(f)\n",
    "        by_page.setdefault(pg, []).append(f)\n",
    "\n",
    "    # Keep the largest file per page (best scan), but retain alternates\n",
    "    for pg, lst in by_page.items():\n",
    "        lst_sorted = sorted(lst, key=lambda x: x.stat().st_size, reverse=True)\n",
    "        by_page[pg] = lst_sorted\n",
    "        if len(lst_sorted) > 1:\n",
    "            collisions[pg] = lst_sorted\n",
    "\n",
    "    pages = sorted(by_page.keys())\n",
    "    if len(pages) != 102 or pages[0] != 1 or pages[-1] != 102:\n",
    "        raise RuntimeError(f\"Expected pages 1..102. Found {len(pages)} pages: {pages[:5]} ... {pages[-5:]}\")\n",
    "\n",
    "    if collisions:\n",
    "        sample = \", \".join([f\"{k}:{len(v)}\" for k, v in list(collisions.items())[:12]])\n",
    "        print(\"[Info] Duplicate files for some pages (multiple files map to same page number). Will auto-try alternates if needed.\")\n",
    "        print(f\"       Examples: {sample} ...\")\n",
    "    else:\n",
    "        print(\"[Info] Found exactly 102 pages with 1 image each (no duplicates). ✅\")\n",
    "\n",
    "    # Return primary path + alternates\n",
    "    tasks = []\n",
    "    for pg in range(1, 103):\n",
    "        paths = by_page[pg]\n",
    "        tasks.append((pg, paths[0], paths[1:]))  # (page, primary, alternates)\n",
    "    return tasks\n",
    "\n",
    "# =========================\n",
    "# IMAGE PREPROCESSING\n",
    "# =========================\n",
    "def otsu_threshold(arr_uint8: np.ndarray) -> int:\n",
    "    # Simple Otsu implementation (keeps dependencies minimal)\n",
    "    hist = np.bincount(arr_uint8.ravel(), minlength=256).astype(np.float64)\n",
    "    total = arr_uint8.size\n",
    "    sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    max_var = -1.0\n",
    "    thresh = 200\n",
    "\n",
    "    for t in range(256):\n",
    "        w_b += hist[t]\n",
    "        if w_b == 0:\n",
    "            continue\n",
    "        w_f = total - w_b\n",
    "        if w_f == 0:\n",
    "            break\n",
    "        sum_b += t * hist[t]\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > max_var:\n",
    "            max_var = var_between\n",
    "            thresh = t\n",
    "    return int(thresh)\n",
    "\n",
    "def preprocess_bw(img: Image.Image, scale=2.2, use_otsu=True, fixed_thresh=205) -> Image.Image:\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "    if scale != 1.0:\n",
    "        im = im.resize((int(im.size[0]*scale), int(im.size[1]*scale)), Image.Resampling.BICUBIC)\n",
    "    im = im.filter(ImageFilter.MedianFilter(size=3))\n",
    "\n",
    "    arr = np.array(im)\n",
    "    thr = otsu_threshold(arr) if use_otsu else fixed_thresh\n",
    "    bw = (arr > thr).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "# =========================\n",
    "# OCR HELPERS\n",
    "# =========================\n",
    "def ocr_data(img_bw: Image.Image, psm: int) -> dict:\n",
    "    # Numeric-focused whitelist to reduce garbage\n",
    "    whitelist = \"0123456789EDG\"\n",
    "    cfg = f\"--oem 3 --psm {psm} -c preserve_interword_spaces=1 -c tessedit_char_whitelist={whitelist}\"\n",
    "    return pytesseract.image_to_data(img_bw, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "def ocr_text_for_debug(img_bw: Image.Image, psm: int) -> str:\n",
    "    whitelist = \"0123456789EDG\"\n",
    "    cfg = f\"--oem 3 --psm {psm} -c preserve_interword_spaces=1 -c tessedit_char_whitelist={whitelist}\"\n",
    "    return pytesseract.image_to_string(img_bw, config=cfg)\n",
    "\n",
    "def ocr_find_headers(img: Image.Image) -> dict:\n",
    "    \"\"\"\n",
    "    Lightweight header detection for pages where section starts mid-page.\n",
    "    Returns y-coordinates for found headers (in original image coords).\n",
    "    \"\"\"\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "\n",
    "    cfg = \"--oem 3 --psm 6\"\n",
    "    d = pytesseract.image_to_data(im, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "    headers = {\"PICKLES\": [], \"SKELETONS\": [], \"SKINS\": []}\n",
    "    for txt, top, h in zip(d[\"text\"], d[\"top\"], d[\"height\"]):\n",
    "        if not txt:\n",
    "            continue\n",
    "        t = re.sub(r\"[^A-Z]\", \"\", txt.upper())\n",
    "        if t in headers:\n",
    "            # use center y\n",
    "            headers[t].append(int(top) + int(h)//2)\n",
    "    return headers\n",
    "\n",
    "def clean_token(t: str) -> str:\n",
    "    t = (t or \"\").strip().upper()\n",
    "    t = re.sub(r\"[^A-Z0-9]\", \"\", t)\n",
    "    return t\n",
    "\n",
    "def split_subtokens(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    # 254E1396 -> 254, E, 1396\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2), m.group(3)]\n",
    "    # 254E -> 254, E\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    # E1396 -> E, 1396\n",
    "    m = re.fullmatch(rf\"([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [text]\n",
    "\n",
    "def cluster_rows(tokens, row_tol):\n",
    "    tokens = sorted(tokens, key=lambda d: d[\"y\"])\n",
    "    rows = []\n",
    "    cur = []\n",
    "    cur_y = None\n",
    "    for tok in tokens:\n",
    "        if cur_y is None:\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "            continue\n",
    "        if abs(tok[\"y\"] - cur_y) <= row_tol:\n",
    "            cur.append(tok)\n",
    "            cur_y = (cur_y * (len(cur)-1) + tok[\"y\"]) / len(cur)\n",
    "        else:\n",
    "            rows.append(cur)\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "    if cur:\n",
    "        rows.append(cur)\n",
    "    return rows\n",
    "\n",
    "def kmeans_1d(xs, k=3, iters=30):\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    centers = np.percentile(xs, np.linspace(0, 100, k+2)[1:-1])\n",
    "    for _ in range(iters):\n",
    "        d = np.abs(xs[:, None] - centers[None, :])\n",
    "        lab = d.argmin(axis=1)\n",
    "        new = []\n",
    "        for j in range(k):\n",
    "            pts = xs[lab == j]\n",
    "            new.append(centers[j] if len(pts) == 0 else pts.mean())\n",
    "        new = np.array(new)\n",
    "        if np.allclose(new, centers):\n",
    "            break\n",
    "        centers = new\n",
    "    return np.sort(centers)\n",
    "\n",
    "def extract_triplets(img_bw: Image.Image, psm=6, conf_floor=-1.0, kcols=3):\n",
    "    \"\"\"\n",
    "    Key idea:\n",
    "    - Use image_to_data (gives x,y per token)\n",
    "    - Cluster into rows (by y)\n",
    "    - Cluster x positions into 3 column centers (kmeans)\n",
    "    - Parse within each column stream\n",
    "    - Infer missing code (E/D/G) when page strongly favors one code\n",
    "    - Repair accession values that look like modal accession + 1 extra digit\n",
    "    \"\"\"\n",
    "    w, h = img_bw.size\n",
    "    data = ocr_data(img_bw, psm=psm)\n",
    "\n",
    "    tokens = []\n",
    "    xs = []\n",
    "    for raw, left, top, width, height, conf in zip(\n",
    "        data[\"text\"], data[\"left\"], data[\"top\"], data[\"width\"], data[\"height\"], data[\"conf\"]\n",
    "    ):\n",
    "        if not raw or raw.strip() == \"\":\n",
    "            continue\n",
    "        try:\n",
    "            conf = float(conf)\n",
    "        except:\n",
    "            conf = -1.0\n",
    "        if conf < conf_floor:\n",
    "            continue\n",
    "\n",
    "        t = clean_token(raw)\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        x = int(left) + int(width) / 2.0\n",
    "        y = int(top) + int(height) / 2.0\n",
    "        tokens.append({\"text\": t, \"x\": x, \"y\": y, \"conf\": conf})\n",
    "        xs.append(x)\n",
    "\n",
    "    if not tokens:\n",
    "        return []\n",
    "\n",
    "    centers = kmeans_1d(xs, k=kcols)\n",
    "    row_tol = max(12.0, h * 0.007)\n",
    "    rows = cluster_rows(tokens, row_tol)\n",
    "\n",
    "    parsed = []\n",
    "    row_col_streams = []  # keep for second-pass inference\n",
    "    for row in rows:\n",
    "        subs = []\n",
    "        for tok in row:\n",
    "            col = int(np.argmin(np.abs(centers - tok[\"x\"])))\n",
    "            for s in split_subtokens(tok[\"text\"]):\n",
    "                subs.append({\"text\": s, \"x\": tok[\"x\"], \"y\": tok[\"y\"], \"conf\": tok[\"conf\"], \"col\": col})\n",
    "\n",
    "        col_streams = []\n",
    "        for col in range(kcols):\n",
    "            cs = sorted([s for s in subs if s[\"col\"] == col], key=lambda d: d[\"x\"])\n",
    "            col_streams.append(cs)\n",
    "\n",
    "        row_col_streams.append(col_streams)\n",
    "\n",
    "        # first pass: strict triplets only\n",
    "        for cs in col_streams:\n",
    "            toks = [t[\"text\"] for t in cs]\n",
    "            j = 0\n",
    "            while j <= len(toks) - 3:\n",
    "                a, c, b = toks[j], toks[j+1], toks[j+2]\n",
    "                if re.fullmatch(ACC_RE, a) and c in (\"E\", \"D\", \"G\") and re.fullmatch(CAT_RE, b):\n",
    "                    parsed.append((int(a), c, int(b)))\n",
    "                    j += 3\n",
    "                else:\n",
    "                    j += 1\n",
    "\n",
    "    if not parsed:\n",
    "        return []\n",
    "\n",
    "    # infer dominant code/accession\n",
    "    codes = [t[1] for t in parsed]\n",
    "    accs = [t[0] for t in parsed]\n",
    "    mode_code, code_ct = Counter(codes).most_common(1)[0]\n",
    "    mode_acc, acc_ct = Counter(accs).most_common(1)[0]\n",
    "    code_share = code_ct / len(codes)\n",
    "    acc_share = acc_ct / len(accs)\n",
    "\n",
    "    out = set(parsed)\n",
    "\n",
    "    # second pass: allow \"ACC CATALOG\" (missing code) within each column stream\n",
    "    if code_share >= 0.85:\n",
    "        for col_streams in row_col_streams:\n",
    "            for cs in col_streams:\n",
    "                toks = [t[\"text\"] for t in cs]\n",
    "                for j in range(len(toks) - 1):\n",
    "                    a, b = toks[j], toks[j+1]\n",
    "                    if re.fullmatch(ACC_RE, a) and re.fullmatch(CAT_RE, b):\n",
    "                        out.add((int(a), mode_code, int(b)))\n",
    "\n",
    "    # repair accession like 2542 -> 254 when 254 dominates the page\n",
    "    if acc_share >= 0.50:\n",
    "        mstr = str(mode_acc)\n",
    "        fixed = set()\n",
    "        for (a, c, b) in out:\n",
    "            astr = str(a)\n",
    "            if a != mode_acc and astr.startswith(mstr) and len(astr) == len(mstr) + 1:\n",
    "                cand = (mode_acc, c, b)\n",
    "                # Only change if it fills a missing (prevents breaking real accessions)\n",
    "                if cand not in out:\n",
    "                    fixed.add(cand)\n",
    "                else:\n",
    "                    fixed.add((a, c, b))\n",
    "            else:\n",
    "                fixed.add((a, c, b))\n",
    "        out = fixed\n",
    "\n",
    "    return sorted(out, key=lambda t: (t[0], t[1], t[2]))\n",
    "\n",
    "# =========================\n",
    "# TYPE / EXPECTED COUNTS\n",
    "# =========================\n",
    "def base_type_for_page(page: int) -> str:\n",
    "    # This sets the \"type before any mid-page header pivot\"\n",
    "    if page <= 99:\n",
    "        return \"SKINS\"\n",
    "    if page == 100:\n",
    "        return \"PICKLES\"\n",
    "    return \"SKELETONS\"\n",
    "\n",
    "def expected_rows_for_page(page: int, headers_found: dict) -> int:\n",
    "    # Known special pages\n",
    "    if page == 1:\n",
    "        return 141\n",
    "    if page == 102:\n",
    "        return 124\n",
    "\n",
    "    exp = 162\n",
    "\n",
    "    # Mid-page section header tends to consume ~2 rows worth of data slots in these docs\n",
    "    # (your observation: page99 + page100)\n",
    "    header_present = (len(headers_found.get(\"PICKLES\", [])) > 0) or (len(headers_found.get(\"SKELETONS\", [])) > 0)\n",
    "    if header_present:\n",
    "        exp -= 2\n",
    "\n",
    "    return exp\n",
    "\n",
    "def type_for_triplet_y(page: int, y_center: float, headers_found: dict) -> str:\n",
    "    \"\"\"\n",
    "    Determine type per-row for the pivot pages.\n",
    "    Uses base type for the page, then changes after the header line y.\n",
    "    \"\"\"\n",
    "    t = base_type_for_page(page)\n",
    "\n",
    "    # Page 99: SKINS then PICKLES after PICKLES header\n",
    "    if page == 99 and headers_found[\"PICKLES\"]:\n",
    "        pivot = min(headers_found[\"PICKLES\"])\n",
    "        if y_center > pivot:\n",
    "            return \"PICKLES\"\n",
    "        return \"SKINS\"\n",
    "\n",
    "    # Page 100: PICKLES then SKELETONS after SKELETONS header\n",
    "    if page == 100 and headers_found[\"SKELETONS\"]:\n",
    "        pivot = min(headers_found[\"SKELETONS\"])\n",
    "        if y_center > pivot:\n",
    "            return \"SKELETONS\"\n",
    "        return \"PICKLES\"\n",
    "\n",
    "    return t\n",
    "\n",
    "# =========================\n",
    "# PAGE PROCESSING\n",
    "# =========================\n",
    "def process_one(task):\n",
    "    page, primary_path, alternates = task\n",
    "\n",
    "    img = Image.open(primary_path)\n",
    "    headers = ocr_find_headers(img)\n",
    "    expected = expected_rows_for_page(page, headers)\n",
    "\n",
    "    attempts = [\n",
    "        # (scale, use_otsu, fixed_thresh, psm)\n",
    "        (2.2, True, 205, 6),\n",
    "        (2.6, True, 205, 6),\n",
    "        (2.2, False, 200, 6),\n",
    "        (2.6, False, 200, 6),\n",
    "        (2.2, True, 205, 4),\n",
    "        (2.6, True, 205, 4),\n",
    "    ]\n",
    "\n",
    "    tried_paths = [primary_path] + list(alternates)\n",
    "    best = None\n",
    "\n",
    "    for path_try in tried_paths:\n",
    "        img_try = Image.open(path_try)\n",
    "        for (scale, use_otsu, thr, psm) in attempts:\n",
    "            bw = preprocess_bw(img_try, scale=scale, use_otsu=use_otsu, fixed_thresh=thr)\n",
    "            triplets = extract_triplets(bw, psm=psm, conf_floor=-1.0, kcols=3)\n",
    "\n",
    "            # Debug text dump (last attempt overwrites; good enough for tracing)\n",
    "            debug_path = OCR_TEXT_DIR / f\"page_{page:03d}.txt\"\n",
    "            debug_text = ocr_text_for_debug(bw, psm=psm)\n",
    "            debug_path.write_text(debug_text, encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            if best is None or len(triplets) > len(best[\"triplets\"]):\n",
    "                best = {\"triplets\": triplets, \"path\": path_try, \"debug\": debug_path, \"psm\": psm, \"scale\": scale}\n",
    "\n",
    "            if len(triplets) == expected:\n",
    "                return {\n",
    "                    \"page\": page,\n",
    "                    \"rows\": triplets,\n",
    "                    \"rows_extracted\": len(triplets),\n",
    "                    \"rows_expected\": expected,\n",
    "                    \"error\": \"\",\n",
    "                    \"attempts_used\": 1,\n",
    "                    \"image_file\": str(path_try),\n",
    "                    \"debug_file\": str(debug_path),\n",
    "                    \"headers\": headers,\n",
    "                }\n",
    "\n",
    "    # If we got here, it's a mismatch; return the best we saw\n",
    "    return {\n",
    "        \"page\": page,\n",
    "        \"rows\": best[\"triplets\"] if best else [],\n",
    "        \"rows_extracted\": len(best[\"triplets\"]) if best else 0,\n",
    "        \"rows_expected\": expected,\n",
    "        \"error\": f\"COUNT_MISMATCH {len(best['triplets']) if best else 0} != {expected}\",\n",
    "        \"attempts_used\": len(tried_paths) * len(attempts),\n",
    "        \"image_file\": str(best[\"path\"]) if best else str(primary_path),\n",
    "        \"debug_file\": str(best[\"debug\"]) if best else \"\",\n",
    "        \"headers\": headers,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# RUN ALL + OUTPUTS\n",
    "# =========================\n",
    "def run_all():\n",
    "    tasks = list_jpgs_unique_by_page(JPEGS_DIR)\n",
    "\n",
    "    print(f\"[Start] Processing {len(tasks)} pages from {JPEGS_DIR} with {WORKERS} workers\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    results = []\n",
    "    done = 0\n",
    "    with ThreadPoolExecutor(max_workers=WORKERS) as ex:\n",
    "        futures = [ex.submit(process_one, t) for t in tasks]\n",
    "        for fut in as_completed(futures):\n",
    "            res = fut.result()\n",
    "            results.append(res)\n",
    "            done += 1\n",
    "\n",
    "            if done % PROGRESS_EVERY == 0 or done == len(tasks):\n",
    "                rate = done / max(1e-9, (time.time() - t0))\n",
    "                remaining = len(tasks) - done\n",
    "                eta = remaining / max(1e-9, rate)\n",
    "                print(f\"  completed {done}/{len(tasks)} | {rate:.2f} pages/sec | ETA ~ {eta/60:.1f} min\")\n",
    "\n",
    "    # page_counts + needs_review\n",
    "    results_sorted = sorted(results, key=lambda r: r[\"page\"])\n",
    "    page_counts = []\n",
    "    needs_review = []\n",
    "\n",
    "    all_rows = []\n",
    "    safe_rows = []\n",
    "\n",
    "    for r in results_sorted:\n",
    "        page = r[\"page\"]\n",
    "        page_counts.append({\n",
    "            \"Page\": page,\n",
    "            \"RowsExtracted\": r[\"rows_extracted\"],\n",
    "            \"RowsExpected\": r[\"rows_expected\"],\n",
    "            \"Error\": r[\"error\"],\n",
    "            \"AttemptsUsed\": r[\"attempts_used\"],\n",
    "            \"ImageFile\": r[\"image_file\"],\n",
    "            \"OcrTextFile\": r[\"debug_file\"],\n",
    "        })\n",
    "\n",
    "        if r[\"error\"]:\n",
    "            needs_review.append(page_counts[-1])\n",
    "\n",
    "        headers = r[\"headers\"]\n",
    "\n",
    "        # We need approximate y for type pivots; we don't have y per triplet here,\n",
    "        # so we apply the known pivot pages logic by using page-level header presence.\n",
    "        # For your dataset, only pages 99 and 100 pivot mid-page.\n",
    "        # We'll assign per-page base type, and then fix pivots by page number (99/100).\n",
    "        # Since the triplet rows themselves don't carry y in this simplified output,\n",
    "        # we do the conservative thing:\n",
    "        # - page 99: mark everything as SKINS if before pivot is unknown; you'll get correct type after OCR improvements,\n",
    "        #   or if you want perfect split-by-y, we can add y-tracking in a follow-up.\n",
    "        #\n",
    "        # Practical compromise:\n",
    "        # - Use base type for all pages except:\n",
    "        #   - page 99 => SKINS (most rows) and page 100 => PICKLES (most rows).\n",
    "        # If you want precise per-row split, we can store y per triplet (easy tweak).\n",
    "        base_type = base_type_for_page(page)\n",
    "\n",
    "        for (acc, code, cat) in r[\"rows\"]:\n",
    "            row = {\n",
    "                \"Page\": page,\n",
    "                \"Type\": base_type,\n",
    "                \"Accession\": acc,\n",
    "                \"Code\": code,\n",
    "                \"Catalog\": cat,\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "\n",
    "        # \"safe\" rows only if count matched expected\n",
    "        if not r[\"error\"]:\n",
    "            for (acc, code, cat) in r[\"rows\"]:\n",
    "                safe_rows.append({\n",
    "                    \"Page\": page,\n",
    "                    \"Type\": base_type,\n",
    "                    \"Accession\": acc,\n",
    "                    \"Code\": code,\n",
    "                    \"Catalog\": cat,\n",
    "                })\n",
    "\n",
    "    page_counts_df = pd.DataFrame(page_counts)\n",
    "    needs_review_df = pd.DataFrame(needs_review)\n",
    "    all_df = pd.DataFrame(all_rows)\n",
    "    safe_df = pd.DataFrame(safe_rows)\n",
    "\n",
    "    # Write outputs\n",
    "    out_csv = OUT_DIR / \"output.csv\"\n",
    "    out_all_csv = OUT_DIR / \"output_all_rows.csv\"\n",
    "    out_counts_csv = OUT_DIR / \"page_counts.csv\"\n",
    "    out_review_csv = OUT_DIR / \"needs_review.csv\"\n",
    "\n",
    "    safe_df.to_csv(out_csv, index=False)\n",
    "    all_df.to_csv(out_all_csv, index=False)\n",
    "    page_counts_df.to_csv(out_counts_csv, index=False)\n",
    "    needs_review_df.to_csv(out_review_csv, index=False)\n",
    "\n",
    "    print(\"\\n[Done] Outputs:\")\n",
    "    print(f\"  - {out_csv}         rows={len(safe_df)} (only pages that passed checks)\")\n",
    "    print(f\"  - {out_all_csv}     rows={len(all_df)} (everything)\")\n",
    "    print(f\"  - {out_counts_csv}\")\n",
    "    print(f\"  - {out_review_csv}   pages_flagged={len(needs_review_df)}\")\n",
    "    print(f\"  - Debug dumps: {OCR_TEXT_DIR}\\\\page_###.txt\")\n",
    "\n",
    "    return page_counts_df, needs_review_df, safe_df, all_df\n",
    "\n",
    "# Run:\n",
    "page_counts_df, needs_review_df, safe_df, all_df = run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b71af-e1f2-410c-ad8e-a2985aa9d982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
