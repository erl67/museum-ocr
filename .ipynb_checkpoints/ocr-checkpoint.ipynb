{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b3dcd2-e010-4474-ab88-820a77d36cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Found exactly 102 pages with 1 image each (no duplicates). ✅\n",
      "[Start] Processing 102 pages from JPEGS with 6 workers\n",
      "  completed 10/102 | 0.46 pages/sec | ETA ~ 3.3 min\n",
      "  completed 20/102 | 0.48 pages/sec | ETA ~ 2.8 min\n",
      "  completed 30/102 | 0.51 pages/sec | ETA ~ 2.3 min\n",
      "  completed 40/102 | 0.52 pages/sec | ETA ~ 2.0 min\n",
      "  completed 50/102 | 0.53 pages/sec | ETA ~ 1.6 min\n",
      "  completed 60/102 | 0.54 pages/sec | ETA ~ 1.3 min\n",
      "  completed 70/102 | 0.57 pages/sec | ETA ~ 0.9 min\n",
      "  completed 80/102 | 0.57 pages/sec | ETA ~ 0.6 min\n",
      "  completed 90/102 | 0.56 pages/sec | ETA ~ 0.4 min\n",
      "  completed 100/102 | 0.57 pages/sec | ETA ~ 0.1 min\n",
      "  completed 102/102 | 0.57 pages/sec | ETA ~ 0.0 min\n",
      "\n",
      "[Done] Outputs:\n",
      "  - out\\output.csv                 rows=16473 (includes incomplete pages; see PageComplete)\n",
      "  - out\\output_complete_pages.csv        rows=16473 (only pages that passed checks)\n",
      "  - out\\page_counts.csv\n",
      "  - out\\needs_review.csv          pages_flagged=0\n",
      "  - out\\run_summary.txt\n",
      "\n",
      "[Summary] Target=16473 | Extracted=16473 | Missing=0 | 100.00%\n"
     ]
    }
   ],
   "source": [
    "##RESCAN 20,27,53,61,62\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "JPEGS_DIR = Path(r\".\\JPEGS\")\n",
    "OUT_DIR = Path(r\".\\out\")\n",
    "OCR_TEXT_DIR = OUT_DIR / \"ocr_text\"\n",
    "\n",
    "WORKERS = 6\n",
    "PROGRESS_EVERY = 10\n",
    "\n",
    "# If you installed tesseract but it's not on PATH, set it explicitly:\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OCR_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ACC_RE = r\"\\d{1,6}\"\n",
    "CAT_RE = r\"\\d{1,7}\"\n",
    "\n",
    "# =========================\n",
    "# FILE LISTING (NO ZIP)\n",
    "# =========================\n",
    "def page_num_from_path(p: Path) -> int:\n",
    "    m = re.search(r\"(\\d+)\", p.stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Can't parse page number from: {p.name}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def list_jpgs_unique_by_page(folder: Path):\n",
    "    # One glob that matches .jpg/.JPG/etc without double-counting on Windows\n",
    "    files = list(folder.glob(\"*.[jJ][pP][gG]\"))\n",
    "\n",
    "    # Normalize + de-dupe physical paths (defensive)\n",
    "    normed = []\n",
    "    seen = set()\n",
    "    for f in files:\n",
    "        key = os.path.normcase(str(f.resolve()))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            normed.append(f)\n",
    "\n",
    "    # Group by page number\n",
    "    by_page = {}\n",
    "    collisions = {}\n",
    "    for f in normed:\n",
    "        pg = page_num_from_path(f)\n",
    "        by_page.setdefault(pg, []).append(f)\n",
    "\n",
    "    # Keep the largest file per page (best scan), but retain alternates\n",
    "    for pg, lst in by_page.items():\n",
    "        lst_sorted = sorted(lst, key=lambda x: x.stat().st_size, reverse=True)\n",
    "        by_page[pg] = lst_sorted\n",
    "        if len(lst_sorted) > 1:\n",
    "            collisions[pg] = lst_sorted\n",
    "\n",
    "    pages = sorted(by_page.keys())\n",
    "    if len(pages) != 102 or pages[0] != 1 or pages[-1] != 102:\n",
    "        raise RuntimeError(f\"Expected pages 1..102. Found {len(pages)} pages: {pages[:5]} ... {pages[-5:]}\")\n",
    "\n",
    "    if collisions:\n",
    "        sample = \", \".join([f\"{k}:{len(v)}\" for k, v in list(collisions.items())[:12]])\n",
    "        print(\"[Info] Duplicate files for some pages (multiple files map to same page number). Will auto-try alternates if needed.\")\n",
    "        print(f\"       Examples: {sample} ...\")\n",
    "    else:\n",
    "        print(\"[Info] Found exactly 102 pages with 1 image each (no duplicates). ✅\")\n",
    "\n",
    "    # Return primary path + alternates\n",
    "    tasks = []\n",
    "    for pg in range(1, 103):\n",
    "        paths = by_page[pg]\n",
    "        tasks.append((pg, paths[0], paths[1:]))  # (page, primary, alternates)\n",
    "    return tasks\n",
    "\n",
    "# =========================\n",
    "# IMAGE PREPROCESSING\n",
    "# =========================\n",
    "def otsu_threshold(arr_uint8: np.ndarray) -> int:\n",
    "    # Simple Otsu implementation (keeps dependencies minimal)\n",
    "    hist = np.bincount(arr_uint8.ravel(), minlength=256).astype(np.float64)\n",
    "    total = arr_uint8.size\n",
    "    sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    max_var = -1.0\n",
    "    thresh = 200\n",
    "\n",
    "    for t in range(256):\n",
    "        w_b += hist[t]\n",
    "        if w_b == 0:\n",
    "            continue\n",
    "        w_f = total - w_b\n",
    "        if w_f == 0:\n",
    "            break\n",
    "        sum_b += t * hist[t]\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > max_var:\n",
    "            max_var = var_between\n",
    "            thresh = t\n",
    "    return int(thresh)\n",
    "\n",
    "def preprocess_bw(img: Image.Image, scale=2.2, use_otsu=True, fixed_thresh=205) -> Image.Image:\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "    if scale != 1.0:\n",
    "        im = im.resize((int(im.size[0]*scale), int(im.size[1]*scale)), Image.Resampling.BICUBIC)\n",
    "    im = im.filter(ImageFilter.MedianFilter(size=3))\n",
    "\n",
    "    arr = np.array(im)\n",
    "    thr = otsu_threshold(arr) if use_otsu else fixed_thresh\n",
    "    bw = (arr > thr).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "# =========================\n",
    "# OCR HELPERS\n",
    "# =========================\n",
    "def ocr_data(img_bw: Image.Image, psm: int) -> dict:\n",
    "    # Numeric-focused whitelist to reduce garbage\n",
    "    whitelist = \"0123456789EDG\"\n",
    "    cfg = f\"--oem 3 --psm {psm} -c preserve_interword_spaces=1 -c tessedit_char_whitelist={whitelist}\"\n",
    "    return pytesseract.image_to_data(img_bw, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "def ocr_text_for_debug(img_bw: Image.Image, psm: int) -> str:\n",
    "    whitelist = \"0123456789EDG\"\n",
    "    cfg = f\"--oem 3 --psm {psm} -c preserve_interword_spaces=1 -c tessedit_char_whitelist={whitelist}\"\n",
    "    return pytesseract.image_to_string(img_bw, config=cfg)\n",
    "\n",
    "def ocr_find_headers(img: Image.Image) -> dict:\n",
    "    \"\"\"\n",
    "    Lightweight header detection for pages where section starts mid-page.\n",
    "    Returns y-coordinates for found headers (in original image coords).\n",
    "    \"\"\"\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "\n",
    "    cfg = \"--oem 3 --psm 6\"\n",
    "    d = pytesseract.image_to_data(im, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "    headers = {\"PICKLES\": [], \"SKELETONS\": [], \"SKINS\": []}\n",
    "    for txt, top, h in zip(d[\"text\"], d[\"top\"], d[\"height\"]):\n",
    "        if not txt:\n",
    "            continue\n",
    "        t = re.sub(r\"[^A-Z]\", \"\", txt.upper())\n",
    "        if t in headers:\n",
    "            # use center y\n",
    "            headers[t].append(int(top) + int(h)//2)\n",
    "    return headers\n",
    "\n",
    "def clean_token(t: str) -> str:\n",
    "    t = (t or \"\").strip().upper()\n",
    "    t = re.sub(r\"[^A-Z0-9]\", \"\", t)\n",
    "    return t\n",
    "\n",
    "def split_subtokens(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    # 254E1396 -> 254, E, 1396\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2), m.group(3)]\n",
    "    # 254E -> 254, E\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    # E1396 -> E, 1396\n",
    "    m = re.fullmatch(rf\"([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [text]\n",
    "\n",
    "def cluster_rows(tokens, row_tol):\n",
    "    tokens = sorted(tokens, key=lambda d: d[\"y\"])\n",
    "    rows = []\n",
    "    cur = []\n",
    "    cur_y = None\n",
    "    for tok in tokens:\n",
    "        if cur_y is None:\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "            continue\n",
    "        if abs(tok[\"y\"] - cur_y) <= row_tol:\n",
    "            cur.append(tok)\n",
    "            cur_y = (cur_y * (len(cur)-1) + tok[\"y\"]) / len(cur)\n",
    "        else:\n",
    "            rows.append(cur)\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "    if cur:\n",
    "        rows.append(cur)\n",
    "    return rows\n",
    "\n",
    "def kmeans_1d(xs, k=3, iters=30):\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    centers = np.percentile(xs, np.linspace(0, 100, k+2)[1:-1])\n",
    "    for _ in range(iters):\n",
    "        d = np.abs(xs[:, None] - centers[None, :])\n",
    "        lab = d.argmin(axis=1)\n",
    "        new = []\n",
    "        for j in range(k):\n",
    "            pts = xs[lab == j]\n",
    "            new.append(centers[j] if len(pts) == 0 else pts.mean())\n",
    "        new = np.array(new)\n",
    "        if np.allclose(new, centers):\n",
    "            break\n",
    "        centers = new\n",
    "    return np.sort(centers)\n",
    "\n",
    "def extract_triplets(img_bw: Image.Image, psm=6, conf_floor=-1.0, kcols=3):\n",
    "    \"\"\"\n",
    "    Key idea:\n",
    "    - Use image_to_data (gives x,y per token)\n",
    "    - Cluster into rows (by y)\n",
    "    - Cluster x positions into 3 column centers (kmeans)\n",
    "    - Parse within each column stream\n",
    "    - Infer missing code (E/D/G) when page strongly favors one code\n",
    "    - Repair accession values that look like modal accession + 1 extra digit\n",
    "    \"\"\"\n",
    "    w, h = img_bw.size\n",
    "    data = ocr_data(img_bw, psm=psm)\n",
    "\n",
    "    tokens = []\n",
    "    xs = []\n",
    "    for raw, left, top, width, height, conf in zip(\n",
    "        data[\"text\"], data[\"left\"], data[\"top\"], data[\"width\"], data[\"height\"], data[\"conf\"]\n",
    "    ):\n",
    "        if not raw or raw.strip() == \"\":\n",
    "            continue\n",
    "        try:\n",
    "            conf = float(conf)\n",
    "        except:\n",
    "            conf = -1.0\n",
    "        if conf < conf_floor:\n",
    "            continue\n",
    "\n",
    "        t = clean_token(raw)\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        x = int(left) + int(width) / 2.0\n",
    "        y = int(top) + int(height) / 2.0\n",
    "        tokens.append({\"text\": t, \"x\": x, \"y\": y, \"conf\": conf})\n",
    "        xs.append(x)\n",
    "\n",
    "    if not tokens:\n",
    "        return []\n",
    "\n",
    "    centers = kmeans_1d(xs, k=kcols)\n",
    "    row_tol = max(12.0, h * 0.007)\n",
    "    rows = cluster_rows(tokens, row_tol)\n",
    "\n",
    "    parsed = []\n",
    "    row_col_streams = []  # keep for second-pass inference\n",
    "    for row in rows:\n",
    "        subs = []\n",
    "        for tok in row:\n",
    "            col = int(np.argmin(np.abs(centers - tok[\"x\"])))\n",
    "            for s in split_subtokens(tok[\"text\"]):\n",
    "                subs.append({\"text\": s, \"x\": tok[\"x\"], \"y\": tok[\"y\"], \"conf\": tok[\"conf\"], \"col\": col})\n",
    "\n",
    "        col_streams = []\n",
    "        for col in range(kcols):\n",
    "            cs = sorted([s for s in subs if s[\"col\"] == col], key=lambda d: d[\"x\"])\n",
    "            col_streams.append(cs)\n",
    "\n",
    "        row_col_streams.append(col_streams)\n",
    "\n",
    "        # first pass: strict triplets only\n",
    "        for cs in col_streams:\n",
    "            toks = [t[\"text\"] for t in cs]\n",
    "            j = 0\n",
    "            while j <= len(toks) - 3:\n",
    "                a, c, b = toks[j], toks[j+1], toks[j+2]\n",
    "                if re.fullmatch(ACC_RE, a) and c in (\"E\", \"D\", \"G\") and re.fullmatch(CAT_RE, b):\n",
    "                    parsed.append((int(a), c, int(b)))\n",
    "                    j += 3\n",
    "                else:\n",
    "                    j += 1\n",
    "\n",
    "    if not parsed:\n",
    "        return []\n",
    "\n",
    "    # infer dominant code/accession\n",
    "    codes = [t[1] for t in parsed]\n",
    "    accs = [t[0] for t in parsed]\n",
    "    mode_code, code_ct = Counter(codes).most_common(1)[0]\n",
    "    mode_acc, acc_ct = Counter(accs).most_common(1)[0]\n",
    "    code_share = code_ct / len(codes)\n",
    "    acc_share = acc_ct / len(accs)\n",
    "\n",
    "    out = set(parsed)\n",
    "\n",
    "    # second pass: allow \"ACC CATALOG\" (missing code) within each column stream\n",
    "    if code_share >= 0.85:\n",
    "        for col_streams in row_col_streams:\n",
    "            for cs in col_streams:\n",
    "                toks = [t[\"text\"] for t in cs]\n",
    "                for j in range(len(toks) - 1):\n",
    "                    a, b = toks[j], toks[j+1]\n",
    "                    if re.fullmatch(ACC_RE, a) and re.fullmatch(CAT_RE, b):\n",
    "                        out.add((int(a), mode_code, int(b)))\n",
    "\n",
    "    # repair accession like 2542 -> 254 when 254 dominates the page\n",
    "    if acc_share >= 0.50:\n",
    "        mstr = str(mode_acc)\n",
    "        fixed = set()\n",
    "        for (a, c, b) in out:\n",
    "            astr = str(a)\n",
    "            if a != mode_acc and astr.startswith(mstr) and len(astr) == len(mstr) + 1:\n",
    "                cand = (mode_acc, c, b)\n",
    "                # Only change if it fills a missing (prevents breaking real accessions)\n",
    "                if cand not in out:\n",
    "                    fixed.add(cand)\n",
    "                else:\n",
    "                    fixed.add((a, c, b))\n",
    "            else:\n",
    "                fixed.add((a, c, b))\n",
    "        out = fixed\n",
    "\n",
    "    return sorted(out, key=lambda t: (t[0], t[1], t[2]))\n",
    "\n",
    "# =========================\n",
    "# TYPE / EXPECTED COUNTS\n",
    "# =========================\n",
    "def base_type_for_page(page: int) -> str:\n",
    "    # This sets the \"type before any mid-page header pivot\"\n",
    "    if page <= 99:\n",
    "        return \"SKINS\"\n",
    "    if page == 100:\n",
    "        return \"PICKLES\"\n",
    "    return \"SKELETONS\"\n",
    "\n",
    "def expected_rows_for_page(page: int, headers_found: dict) -> int:\n",
    "    # Known special pages\n",
    "    if page == 1:\n",
    "        return 141\n",
    "    if page == 102:\n",
    "        return 136\n",
    "\n",
    "    exp = 162\n",
    "\n",
    "    # Mid-page section header tends to consume ~2 rows worth of data slots in these docs\n",
    "    # (your observation: page99 + page100)\n",
    "    header_present = (len(headers_found.get(\"PICKLES\", [])) > 0) or (len(headers_found.get(\"SKELETONS\", [])) > 0)\n",
    "    if header_present:\n",
    "        exp -= 2\n",
    "\n",
    "    return exp\n",
    "\n",
    "def type_for_triplet_y(page: int, y_center: float, headers_found: dict) -> str:\n",
    "    \"\"\"\n",
    "    Determine type per-row for the pivot pages.\n",
    "    Uses base type for the page, then changes after the header line y.\n",
    "    \"\"\"\n",
    "    t = base_type_for_page(page)\n",
    "\n",
    "    # Page 99: SKINS then PICKLES after PICKLES header\n",
    "    if page == 99 and headers_found[\"PICKLES\"]:\n",
    "        pivot = min(headers_found[\"PICKLES\"])\n",
    "        if y_center > pivot:\n",
    "            return \"PICKLES\"\n",
    "        return \"SKINS\"\n",
    "\n",
    "    # Page 100: PICKLES then SKELETONS after SKELETONS header\n",
    "    if page == 100 and headers_found[\"SKELETONS\"]:\n",
    "        pivot = min(headers_found[\"SKELETONS\"])\n",
    "        if y_center > pivot:\n",
    "            return \"SKELETONS\"\n",
    "        return \"PICKLES\"\n",
    "\n",
    "    return t\n",
    "\n",
    "# =========================\n",
    "# PAGE PROCESSING\n",
    "# =========================\n",
    "def process_one(task):\n",
    "    page, primary_path, alternates = task\n",
    "\n",
    "    img = Image.open(primary_path)\n",
    "    headers = ocr_find_headers(img)\n",
    "    expected = expected_rows_for_page(page, headers)\n",
    "\n",
    "    attempts = [\n",
    "        # (scale, use_otsu, fixed_thresh, psm)\n",
    "        (2.2, True, 205, 6),\n",
    "        (2.6, True, 205, 6),\n",
    "        (2.2, False, 200, 6),\n",
    "        (2.6, False, 200, 6),\n",
    "        (2.2, True, 205, 4),\n",
    "        (2.6, True, 205, 4),\n",
    "    ]\n",
    "\n",
    "    tried_paths = [primary_path] + list(alternates)\n",
    "    best = None\n",
    "\n",
    "    for path_try in tried_paths:\n",
    "        img_try = Image.open(path_try)\n",
    "        for (scale, use_otsu, thr, psm) in attempts:\n",
    "            bw = preprocess_bw(img_try, scale=scale, use_otsu=use_otsu, fixed_thresh=thr)\n",
    "            triplets = extract_triplets(bw, psm=psm, conf_floor=-1.0, kcols=3)\n",
    "\n",
    "            # Debug text dump (last attempt overwrites; good enough for tracing)\n",
    "            debug_path = OCR_TEXT_DIR / f\"page_{page:03d}.txt\"\n",
    "            debug_text = ocr_text_for_debug(bw, psm=psm)\n",
    "            debug_path.write_text(debug_text, encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            if best is None or len(triplets) > len(best[\"triplets\"]):\n",
    "                best = {\"triplets\": triplets, \"path\": path_try, \"debug\": debug_path, \"psm\": psm, \"scale\": scale}\n",
    "\n",
    "            if len(triplets) == expected:\n",
    "                return {\n",
    "                    \"page\": page,\n",
    "                    \"rows\": triplets,\n",
    "                    \"rows_extracted\": len(triplets),\n",
    "                    \"rows_expected\": expected,\n",
    "                    \"error\": \"\",\n",
    "                    \"attempts_used\": 1,\n",
    "                    \"image_file\": str(path_try),\n",
    "                    \"debug_file\": str(debug_path),\n",
    "                    \"headers\": headers,\n",
    "                }\n",
    "\n",
    "    # If we got here, it's a mismatch; return the best we saw\n",
    "    return {\n",
    "        \"page\": page,\n",
    "        \"rows\": best[\"triplets\"] if best else [],\n",
    "        \"rows_extracted\": len(best[\"triplets\"]) if best else 0,\n",
    "        \"rows_expected\": expected,\n",
    "        \"error\": f\"COUNT_MISMATCH {len(best['triplets']) if best else 0} != {expected}\",\n",
    "        \"attempts_used\": len(tried_paths) * len(attempts),\n",
    "        \"image_file\": str(best[\"path\"]) if best else str(primary_path),\n",
    "        \"debug_file\": str(best[\"debug\"]) if best else \"\",\n",
    "        \"headers\": headers,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# RUN ALL + OUTPUTS  (REVISED OUTPUT/CSVS ONLY)\n",
    "# =========================\n",
    "def run_all():\n",
    "    TARGET_TOTAL_ROWS = 16473  # your stated target end result\n",
    "\n",
    "    tasks = list_jpgs_unique_by_page(JPEGS_DIR)\n",
    "\n",
    "    print(f\"[Start] Processing {len(tasks)} pages from {JPEGS_DIR} with {WORKERS} workers\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    results = []\n",
    "    done = 0\n",
    "    with ThreadPoolExecutor(max_workers=WORKERS) as ex:\n",
    "        futures = [ex.submit(process_one, t) for t in tasks]\n",
    "        for fut in as_completed(futures):\n",
    "            res = fut.result()\n",
    "            results.append(res)\n",
    "            done += 1\n",
    "\n",
    "            if done % PROGRESS_EVERY == 0 or done == len(tasks):\n",
    "                rate = done / max(1e-9, (time.time() - t0))\n",
    "                remaining = len(tasks) - done\n",
    "                eta = remaining / max(1e-9, rate)\n",
    "                print(f\"  completed {done}/{len(tasks)} | {rate:.2f} pages/sec | ETA ~ {eta/60:.1f} min\")\n",
    "\n",
    "    results_sorted = sorted(results, key=lambda r: r[\"page\"])\n",
    "\n",
    "    page_counts = []\n",
    "    needs_review = []\n",
    "\n",
    "    all_rows = []\n",
    "    complete_rows = []\n",
    "\n",
    "    for r in results_sorted:\n",
    "        page = r[\"page\"]\n",
    "        extracted = int(r[\"rows_extracted\"])\n",
    "        expected = int(r[\"rows_expected\"])\n",
    "        err = (r[\"error\"] or \"\").strip()\n",
    "        is_complete = (err == \"\")\n",
    "\n",
    "        pct = (extracted / expected * 100.0) if expected else 0.0\n",
    "\n",
    "        rec = {\n",
    "            \"Page\": page,\n",
    "            \"RowsExtracted\": extracted,\n",
    "            \"RowsExpected\": expected,\n",
    "            \"PctComplete\": round(pct, 2),\n",
    "            \"Complete\": bool(is_complete),\n",
    "            \"Error\": err,\n",
    "            \"AttemptsUsed\": int(r[\"attempts_used\"]),\n",
    "            \"ImageFile\": r[\"image_file\"],\n",
    "            \"OcrTextFile\": r[\"debug_file\"],\n",
    "        }\n",
    "        page_counts.append(rec)\n",
    "\n",
    "        if err:\n",
    "            needs_review.append(rec)\n",
    "\n",
    "        base_type = base_type_for_page(page)\n",
    "\n",
    "        # Include ALL rows, but tag whether page was complete\n",
    "        for (acc, code, cat) in r[\"rows\"]:\n",
    "            row = {\n",
    "                \"Page\": page,\n",
    "                \"PageComplete\": bool(is_complete),\n",
    "                \"Type\": base_type,\n",
    "                \"Accession\": acc,\n",
    "                \"Code\": code,\n",
    "                \"Catalog\": cat,\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "            if is_complete:\n",
    "                complete_rows.append(row)\n",
    "\n",
    "    page_counts_df = pd.DataFrame(page_counts)\n",
    "    needs_review_df = pd.DataFrame(needs_review)\n",
    "    all_df = pd.DataFrame(all_rows)\n",
    "    complete_df = pd.DataFrame(complete_rows)\n",
    "\n",
    "    # ---- Write outputs ----\n",
    "    out_csv = OUT_DIR / \"output.csv\"  # NOW: includes incomplete pages + PageComplete flag\n",
    "    out_complete_csv = OUT_DIR / \"output_complete_pages.csv\"\n",
    "    out_counts_csv = OUT_DIR / \"page_counts.csv\"\n",
    "    out_review_csv = OUT_DIR / \"needs_review.csv\"\n",
    "    out_summary_txt = OUT_DIR / \"run_summary.txt\"\n",
    "\n",
    "    all_df.to_csv(out_csv, index=False)\n",
    "    complete_df.to_csv(out_complete_csv, index=False)\n",
    "    page_counts_df.to_csv(out_counts_csv, index=False)\n",
    "    needs_review_df.to_csv(out_review_csv, index=False)\n",
    "\n",
    "    # ---- Summary (target vs extracted, % with 2 decimals) ----\n",
    "    extracted_total = int(len(all_df))\n",
    "    missing_total = int(TARGET_TOTAL_ROWS - extracted_total)\n",
    "    pct_total = (extracted_total / TARGET_TOTAL_ROWS * 100.0) if TARGET_TOTAL_ROWS else 0.0\n",
    "\n",
    "    # Also show rule-based expected sum, in case it's useful for sanity checking\n",
    "    expected_total_rule = int(page_counts_df[\"RowsExpected\"].sum()) if not page_counts_df.empty else 0\n",
    "    missing_vs_rule = int(expected_total_rule - extracted_total)\n",
    "    pct_vs_rule = (extracted_total / expected_total_rule * 100.0) if expected_total_rule else 0.0\n",
    "\n",
    "    complete_pages = int(page_counts_df[\"Complete\"].sum()) if not page_counts_df.empty else 0\n",
    "    total_pages = int(len(page_counts_df))\n",
    "\n",
    "    summary_lines = [\n",
    "        \"=== OCR RUN SUMMARY ===\",\n",
    "        \"\",\n",
    "        f\"Pages processed:            {total_pages}\",\n",
    "        f\"Pages complete:             {complete_pages}/{total_pages} ({(complete_pages/total_pages*100.0 if total_pages else 0.0):.2f}%)\",\n",
    "        f\"Pages flagged:              {total_pages - complete_pages}\",\n",
    "        \"\",\n",
    "        \"=== ROW COUNTS ===\",\n",
    "        f\"Target rows (your target):  {TARGET_TOTAL_ROWS}\",\n",
    "        f\"Extracted rows (actual):    {extracted_total}\",\n",
    "        f\"Missing vs your target:     {missing_total}\",\n",
    "        f\"Completion vs your target:  {pct_total:.2f}%\",\n",
    "        \"\",\n",
    "        f\"Rule-based expected total:  {expected_total_rule}\",\n",
    "        f\"Missing vs rule-based:      {missing_vs_rule}\",\n",
    "        f\"Completion vs rule-based:   {pct_vs_rule:.2f}%\",\n",
    "        \"\",\n",
    "        \"=== OUTPUT FILES ===\",\n",
    "        f\"All rows (includes failed): {out_csv}\",\n",
    "        f\"Complete pages only:        {out_complete_csv}\",\n",
    "        f\"Page counts:                {out_counts_csv}\",\n",
    "        f\"Needs review:               {out_review_csv}\",\n",
    "        f\"Debug dumps:                {OCR_TEXT_DIR}\\\\page_###.txt\",\n",
    "    ]\n",
    "    out_summary_txt.write_text(\"\\n\".join(summary_lines), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n[Done] Outputs:\")\n",
    "    print(f\"  - {out_csv}                 rows={len(all_df)} (includes incomplete pages; see PageComplete)\")\n",
    "    print(f\"  - {out_complete_csv}        rows={len(complete_df)} (only pages that passed checks)\")\n",
    "    print(f\"  - {out_counts_csv}\")\n",
    "    print(f\"  - {out_review_csv}          pages_flagged={len(needs_review_df)}\")\n",
    "    print(f\"  - {out_summary_txt}\")\n",
    "    print(f\"\\n[Summary] Target={TARGET_TOTAL_ROWS} | Extracted={extracted_total} | Missing={missing_total} | {pct_total:.2f}%\")\n",
    "\n",
    "    return page_counts_df, needs_review_df, complete_df, all_df\n",
    "\n",
    "# Run:\n",
    "page_counts_df, needs_review_df, complete_df, all_df = run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239326e6-0885-4614-b74b-e9fad80afb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
