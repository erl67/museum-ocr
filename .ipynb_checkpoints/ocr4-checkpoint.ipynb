{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66dfa63e-46df-4027-94c7-284073c2a26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Found exactly 102 pages with 1 image each (no duplicates). ✅\n",
      "[Start] Processing 102 pages from JPEGS with 6 workers\n",
      "  completed 10/102 | 0.08 pages/sec | ETA ~ 18.1 min\n",
      "  completed 20/102 | 0.09 pages/sec | ETA ~ 15.5 min\n",
      "  completed 30/102 | 0.10 pages/sec | ETA ~ 11.6 min\n",
      "  completed 40/102 | 0.10 pages/sec | ETA ~ 10.3 min\n",
      "  completed 50/102 | 0.10 pages/sec | ETA ~ 8.9 min\n",
      "  completed 60/102 | 0.11 pages/sec | ETA ~ 6.7 min\n",
      "  completed 70/102 | 0.10 pages/sec | ETA ~ 5.2 min\n",
      "  completed 80/102 | 0.10 pages/sec | ETA ~ 3.6 min\n",
      "  completed 90/102 | 0.10 pages/sec | ETA ~ 1.9 min\n",
      "  completed 100/102 | 0.11 pages/sec | ETA ~ 0.3 min\n",
      "  completed 102/102 | 0.11 pages/sec | ETA ~ 0.0 min\n",
      "\n",
      "[Done] Outputs:\n",
      "  - out\\output_all_rows.csv            rows=11263 (includes incomplete pages)\n",
      "  - out\\output_complete_pages.csv       rows=0 (only pages that passed checks)\n",
      "  - out\\page_counts.csv\n",
      "  - out\\needs_review.csv         pages_flagged=102\n",
      "  - out\\run_summary.txt\n",
      "\n",
      "[Summary] Target=16473 | Extracted=11263 | Missing=5210 | 68.37%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "JPEGS_DIR = Path(r\".\\JPEGS\")\n",
    "OUT_DIR = Path(r\".\\out\")\n",
    "OCR_TEXT_DIR = OUT_DIR / \"ocr_text\"\n",
    "\n",
    "WORKERS = 6\n",
    "PROGRESS_EVERY = 10\n",
    "\n",
    "# If you installed tesseract but it's not on PATH, set it explicitly:\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OCR_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ACC_RE = r\"\\d{1,6}\"\n",
    "CAT_RE = r\"\\d{1,7}\"\n",
    "\n",
    "# =========================\n",
    "# FILE LISTING (NO ZIP)\n",
    "# =========================\n",
    "def page_num_from_path(p: Path) -> int:\n",
    "    m = re.search(r\"(\\d+)\", p.stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Can't parse page number from: {p.name}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def list_jpgs_unique_by_page(folder: Path):\n",
    "    # One glob that matches .jpg/.JPG/etc without double-counting on Windows\n",
    "    files = list(folder.glob(\"*.[jJ][pP][gG]\"))\n",
    "\n",
    "    # Normalize + de-dupe physical paths (defensive)\n",
    "    normed = []\n",
    "    seen = set()\n",
    "    for f in files:\n",
    "        key = os.path.normcase(str(f.resolve()))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            normed.append(f)\n",
    "\n",
    "    # Group by page number\n",
    "    by_page = {}\n",
    "    collisions = {}\n",
    "    for f in normed:\n",
    "        pg = page_num_from_path(f)\n",
    "        by_page.setdefault(pg, []).append(f)\n",
    "\n",
    "    # Keep the largest file per page (best scan), but retain alternates\n",
    "    for pg, lst in by_page.items():\n",
    "        lst_sorted = sorted(lst, key=lambda x: x.stat().st_size, reverse=True)\n",
    "        by_page[pg] = lst_sorted\n",
    "        if len(lst_sorted) > 1:\n",
    "            collisions[pg] = lst_sorted\n",
    "\n",
    "    pages = sorted(by_page.keys())\n",
    "    if len(pages) != 102 or pages[0] != 1 or pages[-1] != 102:\n",
    "        raise RuntimeError(f\"Expected pages 1..102. Found {len(pages)} pages: {pages[:5]} ... {pages[-5:]}\")\n",
    "\n",
    "    if collisions:\n",
    "        sample = \", \".join([f\"{k}:{len(v)}\" for k, v in list(collisions.items())[:12]])\n",
    "        print(\"[Info] Duplicate files for some pages (multiple files map to same page number). Will auto-try alternates if needed.\")\n",
    "        print(f\"       Examples: {sample} ...\")\n",
    "    else:\n",
    "        print(\"[Info] Found exactly 102 pages with 1 image each (no duplicates). ✅\")\n",
    "\n",
    "    # Return primary path + alternates\n",
    "    tasks = []\n",
    "    for pg in range(1, 103):\n",
    "        paths = by_page[pg]\n",
    "        tasks.append((pg, paths[0], paths[1:]))  # (page, primary, alternates)\n",
    "    return tasks\n",
    "\n",
    "# =========================\n",
    "# IMAGE PREPROCESSING\n",
    "# =========================\n",
    "def otsu_threshold(arr_uint8: np.ndarray) -> int:\n",
    "    # Simple Otsu implementation (keeps dependencies minimal)\n",
    "    hist = np.bincount(arr_uint8.ravel(), minlength=256).astype(np.float64)\n",
    "    total = arr_uint8.size\n",
    "    sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    max_var = -1.0\n",
    "    thresh = 200\n",
    "\n",
    "    for t in range(256):\n",
    "        w_b += hist[t]\n",
    "        if w_b == 0:\n",
    "            continue\n",
    "        w_f = total - w_b\n",
    "        if w_f == 0:\n",
    "            break\n",
    "        sum_b += t * hist[t]\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > max_var:\n",
    "            max_var = var_between\n",
    "            thresh = t\n",
    "    return int(thresh)\n",
    "\n",
    "def preprocess_bw(img: Image.Image, scale=2.2, use_otsu=True, fixed_thresh=205) -> Image.Image:\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "    if scale != 1.0:\n",
    "        im = im.resize((int(im.size[0]*scale), int(im.size[1]*scale)), Image.Resampling.BICUBIC)\n",
    "    im = im.filter(ImageFilter.MedianFilter(size=3))\n",
    "\n",
    "    arr = np.array(im)\n",
    "    thr = otsu_threshold(arr) if use_otsu else fixed_thresh\n",
    "    bw = (arr > thr).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "# =========================\n",
    "# OCR HELPERS\n",
    "# =========================\n",
    "def ocr_data(img_bw: Image.Image, psm: int) -> dict:\n",
    "    # Numeric-focused whitelist to reduce garbage\n",
    "    whitelist = \"0123456789EDG\"\n",
    "    cfg = f\"--oem 3 --psm {psm} -c preserve_interword_spaces=1 -c tessedit_char_whitelist={whitelist}\"\n",
    "    return pytesseract.image_to_data(img_bw, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "def ocr_text_for_debug(img_bw: Image.Image, psm: int) -> str:\n",
    "    whitelist = \"0123456789EDG\"\n",
    "    cfg = f\"--oem 3 --psm {psm} -c preserve_interword_spaces=1 -c tessedit_char_whitelist={whitelist}\"\n",
    "    return pytesseract.image_to_string(img_bw, config=cfg)\n",
    "\n",
    "def ocr_find_headers(img: Image.Image) -> dict:\n",
    "    \"\"\"\n",
    "    Lightweight header detection for pages where section starts mid-page.\n",
    "    Returns y-coordinates for found headers (in original image coords).\n",
    "    \"\"\"\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "\n",
    "    cfg = \"--oem 3 --psm 6\"\n",
    "    d = pytesseract.image_to_data(im, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "    headers = {\"PICKLES\": [], \"SKELETONS\": [], \"SKINS\": []}\n",
    "    for txt, top, h in zip(d[\"text\"], d[\"top\"], d[\"height\"]):\n",
    "        if not txt:\n",
    "            continue\n",
    "        t = re.sub(r\"[^A-Z]\", \"\", txt.upper())\n",
    "        if t in headers:\n",
    "            # use center y\n",
    "            headers[t].append(int(top) + int(h)//2)\n",
    "    return headers\n",
    "\n",
    "def clean_token(t: str) -> str:\n",
    "    t = (t or \"\").strip().upper()\n",
    "    t = re.sub(r\"[^A-Z0-9]\", \"\", t)\n",
    "    return t\n",
    "\n",
    "def split_subtokens(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    # 254E1396 -> 254, E, 1396\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2), m.group(3)]\n",
    "    # 254E -> 254, E\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    # E1396 -> E, 1396\n",
    "    m = re.fullmatch(rf\"([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [text]\n",
    "\n",
    "def cluster_rows(tokens, row_tol):\n",
    "    tokens = sorted(tokens, key=lambda d: d[\"y\"])\n",
    "    rows = []\n",
    "    cur = []\n",
    "    cur_y = None\n",
    "    for tok in tokens:\n",
    "        if cur_y is None:\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "            continue\n",
    "        if abs(tok[\"y\"] - cur_y) <= row_tol:\n",
    "            cur.append(tok)\n",
    "            cur_y = (cur_y * (len(cur)-1) + tok[\"y\"]) / len(cur)\n",
    "        else:\n",
    "            rows.append(cur)\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "    if cur:\n",
    "        rows.append(cur)\n",
    "    return rows\n",
    "\n",
    "def kmeans_1d(xs, k=3, iters=30):\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    centers = np.percentile(xs, np.linspace(0, 100, k+2)[1:-1])\n",
    "    for _ in range(iters):\n",
    "        d = np.abs(xs[:, None] - centers[None, :])\n",
    "        lab = d.argmin(axis=1)\n",
    "        new = []\n",
    "        for j in range(k):\n",
    "            pts = xs[lab == j]\n",
    "            new.append(centers[j] if len(pts) == 0 else pts.mean())\n",
    "        new = np.array(new)\n",
    "        if np.allclose(new, centers):\n",
    "            break\n",
    "        centers = new\n",
    "    return np.sort(centers)\n",
    "\n",
    "def extract_triplets(img_bw: Image.Image, psm=6, conf_floor=-1.0, kcols=1):\n",
    "    \"\"\"\n",
    "    Now intended to be run on a SINGLE COLUMN crop (kcols=1).\n",
    "    Improvements:\n",
    "      - dynamic row_tol using median OCR word height (prevents row merges)\n",
    "      - keep list (not set) so counts reflect rows; we separately track duplicates\n",
    "      - infer missing CODE (as before) AND infer missing ACCESSION when column dominated\n",
    "    \"\"\"\n",
    "    w, h = img_bw.size\n",
    "    data = ocr_data(img_bw, psm=psm)\n",
    "\n",
    "    tokens = []\n",
    "    xs = []\n",
    "    hs = []\n",
    "\n",
    "    for raw, left, top, width, height, conf in zip(\n",
    "        data[\"text\"], data[\"left\"], data[\"top\"], data[\"width\"], data[\"height\"], data[\"conf\"]\n",
    "    ):\n",
    "        if not raw or raw.strip() == \"\":\n",
    "            continue\n",
    "        try:\n",
    "            conf = float(conf)\n",
    "        except:\n",
    "            conf = -1.0\n",
    "        if conf < conf_floor:\n",
    "            continue\n",
    "\n",
    "        t = clean_token(raw)\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        x = int(left) + int(width) / 2.0\n",
    "        y = int(top) + int(height) / 2.0\n",
    "        hh = max(1, int(height))\n",
    "        tokens.append({\"text\": t, \"x\": x, \"y\": y, \"conf\": conf, \"h\": hh})\n",
    "        xs.append(x)\n",
    "        hs.append(hh)\n",
    "\n",
    "    if not tokens:\n",
    "        return []\n",
    "\n",
    "    # dynamic row tolerance: ~0.8 * median token height, clamped\n",
    "    med_h = float(np.median(hs)) if hs else 18.0\n",
    "    row_tol = max(8.0, min(0.8 * med_h, 30.0))\n",
    "\n",
    "    centers = kmeans_1d(xs, k=kcols)\n",
    "    rows = cluster_rows(tokens, row_tol)\n",
    "\n",
    "    strict = []\n",
    "    unresolved_rows = []  # store token sequences for 2nd-pass inference\n",
    "\n",
    "    for row in rows:\n",
    "        subs = []\n",
    "        for tok in row:\n",
    "            col = int(np.argmin(np.abs(centers - tok[\"x\"]))) if kcols > 1 else 0\n",
    "            for s in split_subtokens(tok[\"text\"]):\n",
    "                subs.append({\"text\": s, \"x\": tok[\"x\"], \"y\": tok[\"y\"], \"conf\": tok[\"conf\"], \"col\": col})\n",
    "\n",
    "        # single column stream (kcols=1 expected)\n",
    "        cs = sorted([s for s in subs if s[\"col\"] == 0], key=lambda d: d[\"x\"])\n",
    "        toks = [t[\"text\"] for t in cs]\n",
    "\n",
    "        # strict triplet search (take first match in this row)\n",
    "        found = False\n",
    "        for j in range(0, len(toks) - 2):\n",
    "            a, c, b = toks[j], toks[j + 1], toks[j + 2]\n",
    "            if re.fullmatch(ACC_RE, a) and c in (\"E\", \"D\", \"G\") and re.fullmatch(CAT_RE, b):\n",
    "                strict.append((int(a), c, int(b)))\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found and toks:\n",
    "            unresolved_rows.append(toks)\n",
    "\n",
    "    if not strict:\n",
    "        return []\n",
    "\n",
    "    # dominant code/accession in THIS COLUMN\n",
    "    codes = [t[1] for t in strict]\n",
    "    accs = [t[0] for t in strict]\n",
    "    mode_code, code_ct = Counter(codes).most_common(1)[0]\n",
    "    mode_acc, acc_ct = Counter(accs).most_common(1)[0]\n",
    "    code_share = code_ct / len(codes)\n",
    "    acc_share = acc_ct / len(accs)\n",
    "\n",
    "    out = list(strict)\n",
    "\n",
    "    # second-pass inference on unresolved rows:\n",
    "    # 1) ACC + CAT => fill CODE if strongly dominated\n",
    "    if code_share >= 0.85:\n",
    "        for toks in unresolved_rows:\n",
    "            for j in range(len(toks) - 1):\n",
    "                a, b = toks[j], toks[j + 1]\n",
    "                if re.fullmatch(ACC_RE, a) and re.fullmatch(CAT_RE, b):\n",
    "                    out.append((int(a), mode_code, int(b)))\n",
    "\n",
    "    # 2) CODE + CAT => fill ACCESSION if strongly dominated\n",
    "    if acc_share >= 0.70:\n",
    "        for toks in unresolved_rows:\n",
    "            for j in range(len(toks) - 1):\n",
    "                c, b = toks[j], toks[j + 1]\n",
    "                if c in (\"E\", \"D\", \"G\") and re.fullmatch(CAT_RE, b):\n",
    "                    out.append((int(mode_acc), c, int(b)))\n",
    "\n",
    "    # repair accession like 2542 -> 254 when 254 dominates\n",
    "    if acc_share >= 0.50:\n",
    "        mstr = str(mode_acc)\n",
    "        repaired = []\n",
    "        for (a, c, b) in out:\n",
    "            astr = str(a)\n",
    "            if a != mode_acc and astr.startswith(mstr) and len(astr) == len(mstr) + 1:\n",
    "                repaired.append((mode_acc, c, b))\n",
    "            else:\n",
    "                repaired.append((a, c, b))\n",
    "        out = repaired\n",
    "\n",
    "    # DO NOT dedupe blindly; but compute duplicates so caller can penalize/flag\n",
    "    return out\n",
    "    \n",
    "# =========================\n",
    "# DESKEW + COLUMN SPLIT (NEW)\n",
    "# =========================\n",
    "def _binarize_for_proj(img: Image.Image, thr=200) -> np.ndarray:\n",
    "    g = ImageOps.grayscale(img)\n",
    "    g = ImageOps.autocontrast(g)\n",
    "    arr = np.array(g)\n",
    "    # text as 1s\n",
    "    return (arr < thr).astype(np.uint8)\n",
    "\n",
    "def estimate_skew_angle(img: Image.Image, search_deg=3.0, step=0.2, downsample=0.40) -> float:\n",
    "    \"\"\"\n",
    "    Returns angle (degrees) to rotate (PIL rotate) to best deskew.\n",
    "    Uses variance of horizontal projection (Radon-lite).\n",
    "    \"\"\"\n",
    "    w, h = img.size\n",
    "    small = img.resize((max(1, int(w * downsample)), max(1, int(h * downsample))))\n",
    "    bw = _binarize_for_proj(small, thr=200)\n",
    "    im = Image.fromarray((bw * 255).astype(np.uint8))\n",
    "\n",
    "    angles = np.arange(-search_deg, search_deg + 1e-9, step)\n",
    "    best_angle = 0.0\n",
    "    best_score = -1.0\n",
    "\n",
    "    for a in angles:\n",
    "        rot = im.rotate(float(a), resample=Image.Resampling.NEAREST, expand=False, fillcolor=0)\n",
    "        arr = (np.array(rot) > 0).astype(np.uint8)\n",
    "        proj = arr.sum(axis=1)\n",
    "        score = float(proj.var())\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_angle = float(a)\n",
    "\n",
    "    return best_angle\n",
    "\n",
    "def deskew_image(img: Image.Image, angle_deg: float) -> Image.Image:\n",
    "    if abs(angle_deg) < 0.05:\n",
    "        return img\n",
    "\n",
    "    # Ensure fillcolor matches mode\n",
    "    mode = img.mode\n",
    "    if mode in (\"L\", \"1\"):\n",
    "        fill = 255\n",
    "    elif mode == \"RGBA\":\n",
    "        fill = (255, 255, 255, 255)\n",
    "    else:\n",
    "        # Convert odd modes like \"P\", \"CMYK\" safely to RGB\n",
    "        if mode not in (\"RGB\", \"RGBA\"):\n",
    "            img = img.convert(\"RGB\")\n",
    "        fill = (255, 255, 255)\n",
    "\n",
    "    return img.rotate(\n",
    "        angle_deg,\n",
    "        resample=Image.Resampling.BICUBIC,\n",
    "        expand=False,\n",
    "        fillcolor=fill\n",
    "    )\n",
    "\n",
    "def find_column_bounds(img: Image.Image, downsample=0.40) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Finds 2 x-splits between 3 columns using vertical projection valleys near 1/3 and 2/3.\n",
    "    Returns (x_split1, x_split2) in ORIGINAL image coordinates.\n",
    "    \"\"\"\n",
    "    w, h = img.size\n",
    "    small = img.resize((max(1, int(w * downsample)), max(1, int(h * downsample))))\n",
    "    bw = _binarize_for_proj(small, thr=200)\n",
    "\n",
    "    proj = bw.sum(axis=0).astype(np.float64)\n",
    "    # smooth\n",
    "    win = 21\n",
    "    kernel = np.ones(win) / win\n",
    "    smooth = np.convolve(proj, kernel, mode=\"same\")\n",
    "\n",
    "    width = len(smooth)\n",
    "    targets = [width / 3, 2 * width / 3]\n",
    "    splits = []\n",
    "\n",
    "    for t in targets:\n",
    "        lo = int(max(0, t - 0.12 * width))\n",
    "        hi = int(min(width, t + 0.12 * width))\n",
    "        idx = lo + int(np.argmin(smooth[lo:hi]))\n",
    "        splits.append(idx)\n",
    "\n",
    "    splits = sorted(splits)\n",
    "    scale = 1.0 / downsample\n",
    "    return int(splits[0] * scale), int(splits[1] * scale)\n",
    "\n",
    "def column_boxes(img: Image.Image, pad_px=25) -> list[tuple[int, int, int, int]]:\n",
    "    w, h = img.size\n",
    "    x1, x2 = find_column_bounds(img)\n",
    "    # pad to avoid cutting characters at boundaries\n",
    "    left  = (0, 0, min(w, x1 + pad_px), h)\n",
    "    mid   = (max(0, x1 - pad_px), 0, min(w, x2 + pad_px), h)\n",
    "    right = (max(0, x2 - pad_px), 0, w, h)\n",
    "    return [left, mid, right]\n",
    "\n",
    "# =========================\n",
    "# TYPE / EXPECTED COUNTS\n",
    "# =========================\n",
    "def base_type_for_page(page: int) -> str:\n",
    "    # This sets the \"type before any mid-page header pivot\"\n",
    "    if page <= 99:\n",
    "        return \"SKINS\"\n",
    "    if page == 100:\n",
    "        return \"PICKLES\"\n",
    "    return \"SKELETONS\"\n",
    "\n",
    "def expected_rows_for_page(page: int, headers_found: dict) -> int:\n",
    "    # Known special pages\n",
    "    if page == 1:\n",
    "        return 141\n",
    "    if page == 102:\n",
    "        return 136\n",
    "\n",
    "    exp = 162\n",
    "\n",
    "    # Mid-page section header tends to consume ~2 rows worth of data slots in these docs\n",
    "    # (your observation: page99 + page100)\n",
    "    header_present = (len(headers_found.get(\"PICKLES\", [])) > 0) or (len(headers_found.get(\"SKELETONS\", [])) > 0)\n",
    "    if header_present:\n",
    "        exp -= 2\n",
    "\n",
    "    return exp\n",
    "\n",
    "def type_for_triplet_y(page: int, y_center: float, headers_found: dict) -> str:\n",
    "    \"\"\"\n",
    "    Determine type per-row for the pivot pages.\n",
    "    Uses base type for the page, then changes after the header line y.\n",
    "    \"\"\"\n",
    "    t = base_type_for_page(page)\n",
    "\n",
    "    # Page 99: SKINS then PICKLES after PICKLES header\n",
    "    if page == 99 and headers_found[\"PICKLES\"]:\n",
    "        pivot = min(headers_found[\"PICKLES\"])\n",
    "        if y_center > pivot:\n",
    "            return \"PICKLES\"\n",
    "        return \"SKINS\"\n",
    "\n",
    "    # Page 100: PICKLES then SKELETONS after SKELETONS header\n",
    "    if page == 100 and headers_found[\"SKELETONS\"]:\n",
    "        pivot = min(headers_found[\"SKELETONS\"])\n",
    "        if y_center > pivot:\n",
    "            return \"SKELETONS\"\n",
    "        return \"PICKLES\"\n",
    "\n",
    "    return t\n",
    "\n",
    "# =========================\n",
    "# PAGE PROCESSING\n",
    "# =========================\n",
    "def process_one(task):\n",
    "    page, primary_path, alternates = task\n",
    "\n",
    "    img0 = Image.open(primary_path)\n",
    "    headers = ocr_find_headers(img0)\n",
    "    expected = expected_rows_for_page(page, headers)\n",
    "\n",
    "    attempts = [\n",
    "        # (scale, use_otsu, fixed_thresh, psm)\n",
    "        (2.2, True, 205, 6),\n",
    "        (2.6, True, 205, 6),\n",
    "        (2.2, False, 200, 6),\n",
    "        (2.6, False, 200, 6),\n",
    "        (2.2, True, 205, 4),\n",
    "        (2.6, True, 205, 4),\n",
    "    ]\n",
    "\n",
    "    tried_paths = [primary_path] + list(alternates)\n",
    "\n",
    "    best = None\n",
    "    best_score = (10**9, 10**9, -10**9)  # (abs_count_error, duplicates, extracted_count)\n",
    "    attempts_used = 0\n",
    "\n",
    "    for path_try in tried_paths:\n",
    "        img_try = Image.open(path_try)\n",
    "\n",
    "        # deskew once per source image\n",
    "        ang = estimate_skew_angle(img_try, search_deg=3.0, step=0.2, downsample=0.40)\n",
    "        img_ds = deskew_image(img_try, ang)\n",
    "\n",
    "        # compute 3 column boxes once\n",
    "        boxes = column_boxes(img_ds, pad_px=25)\n",
    "\n",
    "        for (scale, use_otsu, thr, psm) in attempts:\n",
    "            attempts_used += 1\n",
    "\n",
    "            all_triplets = []\n",
    "            debug_parts = []\n",
    "\n",
    "            for ci, box in enumerate(boxes, start=1):\n",
    "                col = img_ds.crop(box)\n",
    "                bw = preprocess_bw(col, scale=scale, use_otsu=use_otsu, fixed_thresh=thr)\n",
    "\n",
    "                trip = extract_triplets(bw, psm=psm, conf_floor=-1.0, kcols=1)\n",
    "                all_triplets.extend(trip)\n",
    "\n",
    "                # debug OCR text per column\n",
    "                debug_parts.append(f\"\\n===== PAGE {page:03d} COL {ci} | angle={ang:.2f} | scale={scale} | psm={psm} =====\\n\")\n",
    "                debug_parts.append(ocr_text_for_debug(bw, psm=psm))\n",
    "\n",
    "            # write debug text for this attempt (overwrites each attempt; last attempt stored)\n",
    "            debug_path = OCR_TEXT_DIR / f\"page_{page:03d}.txt\"\n",
    "            debug_path.write_text(\"\".join(debug_parts), encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            extracted = len(all_triplets)\n",
    "            dupes = extracted - len(set(all_triplets))\n",
    "            abs_err = abs(extracted - expected)\n",
    "\n",
    "            # scoring: closest count, then fewer duplicates, then higher extracted\n",
    "            score = (abs_err, dupes, extracted)\n",
    "\n",
    "            if best is None or score < best_score:\n",
    "                best_score = score\n",
    "                best = {\n",
    "                    \"triplets\": all_triplets,\n",
    "                    \"path\": path_try,\n",
    "                    \"debug\": debug_path,\n",
    "                    \"psm\": psm,\n",
    "                    \"scale\": scale,\n",
    "                    \"angle\": ang,\n",
    "                    \"dupes\": dupes,\n",
    "                }\n",
    "\n",
    "            # success condition: exact expected AND no duplicates\n",
    "            if extracted == expected and dupes == 0:\n",
    "                return {\n",
    "                    \"page\": page,\n",
    "                    \"rows\": all_triplets,\n",
    "                    \"rows_extracted\": extracted,\n",
    "                    \"rows_expected\": expected,\n",
    "                    \"error\": \"\",\n",
    "                    \"attempts_used\": attempts_used,\n",
    "                    \"image_file\": str(path_try),\n",
    "                    \"debug_file\": str(debug_path),\n",
    "                    \"headers\": headers,\n",
    "                    \"dupes\": dupes,\n",
    "                    \"deskew_angle\": float(ang),\n",
    "                }\n",
    "\n",
    "    # mismatch: return best attempt\n",
    "    got = len(best[\"triplets\"]) if best else 0\n",
    "    dupes = best[\"dupes\"] if best else 0\n",
    "    err_bits = []\n",
    "    if got != expected:\n",
    "        err_bits.append(f\"COUNT_MISMATCH {got} != {expected}\")\n",
    "    if dupes > 0:\n",
    "        err_bits.append(f\"DUPLICATES {dupes}\")\n",
    "    err = \" | \".join(err_bits) if err_bits else \"UNKNOWN_ERROR\"\n",
    "\n",
    "    return {\n",
    "        \"page\": page,\n",
    "        \"rows\": best[\"triplets\"] if best else [],\n",
    "        \"rows_extracted\": got,\n",
    "        \"rows_expected\": expected,\n",
    "        \"error\": err,\n",
    "        \"attempts_used\": attempts_used,\n",
    "        \"image_file\": str(best[\"path\"]) if best else str(primary_path),\n",
    "        \"debug_file\": str(best[\"debug\"]) if best else \"\",\n",
    "        \"headers\": headers,\n",
    "        \"dupes\": dupes,\n",
    "        \"deskew_angle\": float(best[\"angle\"]) if best else 0.0,\n",
    "    }\n",
    "    \n",
    "# =========================\n",
    "# RUN ALL + OUTPUTS\n",
    "# =========================\n",
    "def run_all():\n",
    "    TARGET_TOTAL_ROWS = 16473  # your known final target\n",
    "\n",
    "    tasks = list_jpgs_unique_by_page(JPEGS_DIR)\n",
    "\n",
    "    print(f\"[Start] Processing {len(tasks)} pages from {JPEGS_DIR} with {WORKERS} workers\", flush=True)\n",
    "    t0 = time.time()\n",
    "\n",
    "    results = []\n",
    "    done = 0\n",
    "\n",
    "    # Map futures -> page so we can log failures and keep going\n",
    "    future_to_page = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=WORKERS) as ex:\n",
    "        futures = []\n",
    "        for t in tasks:\n",
    "            fut = ex.submit(process_one, t)\n",
    "            futures.append(fut)\n",
    "            future_to_page[fut] = t[0]  # page number\n",
    "\n",
    "        for fut in as_completed(futures):\n",
    "            page = future_to_page[fut]\n",
    "            try:\n",
    "                res = fut.result()\n",
    "            except Exception as e:\n",
    "                # Don't crash the whole run; record this page as failed.\n",
    "                # Expected rows: use your rule-based function where possible.\n",
    "                # We don't have headers here, so default to 162 except special cases.\n",
    "                expected = 141 if page == 1 else 136 if page == 102 else 162\n",
    "                res = {\n",
    "                    \"page\": page,\n",
    "                    \"rows\": [],\n",
    "                    \"rows_extracted\": 0,\n",
    "                    \"rows_expected\": expected,\n",
    "                    \"error\": f\"EXCEPTION {type(e).__name__}: {e}\",\n",
    "                    \"attempts_used\": 0,\n",
    "                    \"image_file\": \"\",\n",
    "                    \"debug_file\": \"\",\n",
    "                    \"headers\": {\"PICKLES\": [], \"SKELETONS\": [], \"SKINS\": []},\n",
    "                    \"dupes\": 0,\n",
    "                    \"deskew_angle\": 0.0,\n",
    "                }\n",
    "\n",
    "            results.append(res)\n",
    "            done += 1\n",
    "\n",
    "            # Progress output exactly like your original\n",
    "            if done % PROGRESS_EVERY == 0 or done == len(tasks):\n",
    "                rate = done / max(1e-9, (time.time() - t0))\n",
    "                remaining = len(tasks) - done\n",
    "                eta = remaining / max(1e-9, rate)\n",
    "                print(f\"  completed {done}/{len(tasks)} | {rate:.2f} pages/sec | ETA ~ {eta/60:.1f} min\", flush=True)\n",
    "\n",
    "    # Sort results by page\n",
    "    results_sorted = sorted(results, key=lambda r: r[\"page\"])\n",
    "\n",
    "    page_counts = []\n",
    "    needs_review = []\n",
    "\n",
    "    all_rows = []\n",
    "    complete_rows = []\n",
    "\n",
    "    for r in results_sorted:\n",
    "        page = r[\"page\"]\n",
    "        extracted = int(r.get(\"rows_extracted\", 0))\n",
    "        expected = int(r.get(\"rows_expected\", 0))\n",
    "        err = (r.get(\"error\", \"\") or \"\").strip()\n",
    "        is_complete = (err == \"\")\n",
    "        dupes = int(r.get(\"dupes\", 0))\n",
    "        ang = float(r.get(\"deskew_angle\", 0.0))\n",
    "\n",
    "        pct = (extracted / expected * 100.0) if expected else 0.0\n",
    "\n",
    "        rec = {\n",
    "            \"Page\": page,\n",
    "            \"RowsExtracted\": extracted,\n",
    "            \"RowsExpected\": expected,\n",
    "            \"PctComplete\": round(pct, 2),\n",
    "            \"Complete\": bool(is_complete),\n",
    "            \"Error\": err,\n",
    "            \"Duplicates\": dupes,\n",
    "            \"DeskewAngleDeg\": round(ang, 2),\n",
    "            \"AttemptsUsed\": int(r.get(\"attempts_used\", 0)),\n",
    "            \"ImageFile\": r.get(\"image_file\", \"\"),\n",
    "            \"OcrTextFile\": r.get(\"debug_file\", \"\"),\n",
    "        }\n",
    "        page_counts.append(rec)\n",
    "        if not is_complete:\n",
    "            needs_review.append(rec)\n",
    "\n",
    "        base_type = base_type_for_page(page)\n",
    "\n",
    "        for (acc, code, cat) in r.get(\"rows\", []):\n",
    "            row = {\n",
    "                \"Page\": page,\n",
    "                \"PageComplete\": bool(is_complete),\n",
    "                \"Type\": base_type,\n",
    "                \"Accession\": int(acc),\n",
    "                \"Code\": str(code),\n",
    "                \"Catalog\": int(cat),\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "            if is_complete:\n",
    "                complete_rows.append(row)\n",
    "\n",
    "    page_counts_df = pd.DataFrame(page_counts)\n",
    "    needs_review_df = pd.DataFrame(needs_review)\n",
    "    all_df = pd.DataFrame(all_rows)\n",
    "    complete_df = pd.DataFrame(complete_rows)\n",
    "\n",
    "    out_all_csv = OUT_DIR / \"output_all_rows.csv\"              # includes incomplete pages\n",
    "    out_complete_csv = OUT_DIR / \"output_complete_pages.csv\"   # only pages that passed checks\n",
    "    out_counts_csv = OUT_DIR / \"page_counts.csv\"\n",
    "    out_review_csv = OUT_DIR / \"needs_review.csv\"\n",
    "    out_summary_txt = OUT_DIR / \"run_summary.txt\"\n",
    "\n",
    "    all_df.to_csv(out_all_csv, index=False)\n",
    "    complete_df.to_csv(out_complete_csv, index=False)\n",
    "    page_counts_df.to_csv(out_counts_csv, index=False)\n",
    "    needs_review_df.to_csv(out_review_csv, index=False)\n",
    "\n",
    "    expected_total = int(page_counts_df[\"RowsExpected\"].sum()) if not page_counts_df.empty else 0\n",
    "    extracted_total = int(page_counts_df[\"RowsExtracted\"].sum()) if not page_counts_df.empty else 0\n",
    "\n",
    "    missing_vs_expected = expected_total - extracted_total\n",
    "    pct_vs_expected = (extracted_total / expected_total * 100.0) if expected_total else 0.0\n",
    "\n",
    "    missing_vs_target = TARGET_TOTAL_ROWS - extracted_total\n",
    "    pct_vs_target = (extracted_total / TARGET_TOTAL_ROWS * 100.0) if TARGET_TOTAL_ROWS else 0.0\n",
    "\n",
    "    complete_pages = int(page_counts_df[\"Complete\"].sum()) if not page_counts_df.empty else 0\n",
    "    total_pages = int(len(page_counts_df))\n",
    "\n",
    "    summary_lines = [\n",
    "        \"=== OCR RUN SUMMARY ===\",\n",
    "        \"\",\n",
    "        f\"Pages processed:          {total_pages}\",\n",
    "        f\"Pages complete:           {complete_pages}/{total_pages} ({(complete_pages/total_pages*100.0 if total_pages else 0.0):.2f}%)\",\n",
    "        f\"Pages flagged:            {total_pages - complete_pages}\",\n",
    "        \"\",\n",
    "        \"=== ROW COUNTS ===\",\n",
    "        f\"Target rows (you expect): {TARGET_TOTAL_ROWS}\",\n",
    "        f\"Target rows (rule-based): {expected_total}\",\n",
    "        \"\",\n",
    "        f\"Extracted rows:           {extracted_total}\",\n",
    "        \"\",\n",
    "        f\"Missing vs rule-based:    {missing_vs_expected}\",\n",
    "        f\"Completion vs rule-based: {pct_vs_expected:.2f}%\",\n",
    "        \"\",\n",
    "        f\"Missing vs your target:   {missing_vs_target}\",\n",
    "        f\"Completion vs your target:{pct_vs_target:.2f}%\",\n",
    "        \"\",\n",
    "        f\"Complete-page output:     {out_complete_csv}\",\n",
    "        f\"All-rows output:          {out_all_csv}\",\n",
    "        f\"Counts:                   {out_counts_csv}\",\n",
    "        f\"Needs review:             {out_review_csv}\",\n",
    "        f\"Debug dumps:              {OCR_TEXT_DIR}\\\\page_###.txt\",\n",
    "    ]\n",
    "    out_summary_txt.write_text(\"\\n\".join(summary_lines), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n[Done] Outputs:\", flush=True)\n",
    "    print(f\"  - {out_all_csv}            rows={len(all_df)} (includes incomplete pages)\", flush=True)\n",
    "    print(f\"  - {out_complete_csv}       rows={len(complete_df)} (only pages that passed checks)\", flush=True)\n",
    "    print(f\"  - {out_counts_csv}\", flush=True)\n",
    "    print(f\"  - {out_review_csv}         pages_flagged={len(needs_review_df)}\", flush=True)\n",
    "    print(f\"  - {out_summary_txt}\", flush=True)\n",
    "    print(f\"\\n[Summary] Target={TARGET_TOTAL_ROWS} | Extracted={extracted_total} | Missing={missing_vs_target} | {pct_vs_target:.2f}%\", flush=True)\n",
    "\n",
    "    return page_counts_df, needs_review_df, complete_df, all_df\n",
    "\n",
    "page_counts_df, needs_review_df, complete_df, all_df = run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3dcd2-e010-4474-ab88-820a77d36cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
