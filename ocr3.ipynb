{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594c9f5a-fdac-404b-870b-06b4e6edea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Found exactly 102 pages with 1 image each (no duplicates). ✅\n",
      "[Start] Processing 102 pages from JPEGS with 6 workers\n",
      "  completed 10/102 | 0.47 pages/sec | ETA ~ 3.3 min\n",
      "  completed 20/102 | 0.49 pages/sec | ETA ~ 2.8 min\n",
      "  completed 30/102 | 0.45 pages/sec | ETA ~ 2.7 min\n",
      "  completed 40/102 | 0.45 pages/sec | ETA ~ 2.3 min\n",
      "  completed 50/102 | 0.46 pages/sec | ETA ~ 1.9 min\n",
      "  completed 60/102 | 0.44 pages/sec | ETA ~ 1.6 min\n",
      "  completed 70/102 | 0.43 pages/sec | ETA ~ 1.3 min\n",
      "  completed 80/102 | 0.44 pages/sec | ETA ~ 0.8 min\n",
      "  completed 90/102 | 0.46 pages/sec | ETA ~ 0.4 min\n",
      "  completed 100/102 | 0.47 pages/sec | ETA ~ 0.1 min\n",
      "  completed 102/102 | 0.46 pages/sec | ETA ~ 0.0 min\n",
      "\n",
      "[Done] Outputs:\n",
      "  - out\\output.csv         rows=15663 (only pages that passed checks)\n",
      "  - out\\output_all_rows.csv     rows=16454 (everything)\n",
      "  - out\\page_counts.csv\n",
      "  - out\\needs_review.csv   pages_flagged=5\n",
      "  - Debug dumps: out\\ocr_text\\page_###.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "JPEGS_DIR = Path(r\".\\JPEGS\")\n",
    "OUT_DIR = Path(r\".\\out\")\n",
    "OCR_TEXT_DIR = OUT_DIR / \"ocr_text\"\n",
    "\n",
    "WORKERS = 6\n",
    "PROGRESS_EVERY = 10\n",
    "\n",
    "# If you installed tesseract but it's not on PATH, set it explicitly:\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OCR_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ACC_RE = r\"\\d{1,6}\"\n",
    "CAT_RE = r\"\\d{1,7}\"\n",
    "\n",
    "# =========================\n",
    "# FILE LISTING (NO ZIP)\n",
    "# =========================\n",
    "def page_num_from_path(p: Path) -> int:\n",
    "    m = re.search(r\"(\\d+)\", p.stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Can't parse page number from: {p.name}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def list_jpgs_unique_by_page(folder: Path):\n",
    "    # One glob that matches .jpg/.JPG/etc without double-counting on Windows\n",
    "    files = list(folder.glob(\"*.[jJ][pP][gG]\"))\n",
    "\n",
    "    # Normalize + de-dupe physical paths (defensive)\n",
    "    normed = []\n",
    "    seen = set()\n",
    "    for f in files:\n",
    "        key = os.path.normcase(str(f.resolve()))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            normed.append(f)\n",
    "\n",
    "    # Group by page number\n",
    "    by_page = {}\n",
    "    collisions = {}\n",
    "    for f in normed:\n",
    "        pg = page_num_from_path(f)\n",
    "        by_page.setdefault(pg, []).append(f)\n",
    "\n",
    "    # Keep the largest file per page (best scan), but retain alternates\n",
    "    for pg, lst in by_page.items():\n",
    "        lst_sorted = sorted(lst, key=lambda x: x.stat().st_size, reverse=True)\n",
    "        by_page[pg] = lst_sorted\n",
    "        if len(lst_sorted) > 1:\n",
    "            collisions[pg] = lst_sorted\n",
    "\n",
    "    pages = sorted(by_page.keys())\n",
    "    if len(pages) != 102 or pages[0] != 1 or pages[-1] != 102:\n",
    "        raise RuntimeError(f\"Expected pages 1..102. Found {len(pages)} pages: {pages[:5]} ... {pages[-5:]}\")\n",
    "\n",
    "    if collisions:\n",
    "        sample = \", \".join([f\"{k}:{len(v)}\" for k, v in list(collisions.items())[:12]])\n",
    "        print(\"[Info] Duplicate files for some pages (multiple files map to same page number). Will auto-try alternates if needed.\")\n",
    "        print(f\"       Examples: {sample} ...\")\n",
    "    else:\n",
    "        print(\"[Info] Found exactly 102 pages with 1 image each (no duplicates). ✅\")\n",
    "\n",
    "    # Return primary path + alternates\n",
    "    tasks = []\n",
    "    for pg in range(1, 103):\n",
    "        paths = by_page[pg]\n",
    "        tasks.append((pg, paths[0], paths[1:]))  # (page, primary, alternates)\n",
    "    return tasks\n",
    "\n",
    "# =========================\n",
    "# IMAGE PREPROCESSING\n",
    "# =========================\n",
    "def otsu_threshold(arr_uint8: np.ndarray) -> int:\n",
    "    # Simple Otsu implementation (keeps dependencies minimal)\n",
    "    hist = np.bincount(arr_uint8.ravel(), minlength=256).astype(np.float64)\n",
    "    total = arr_uint8.size\n",
    "    sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    max_var = -1.0\n",
    "    thresh = 200\n",
    "\n",
    "    for t in range(256):\n",
    "        w_b += hist[t]\n",
    "        if w_b == 0:\n",
    "            continue\n",
    "        w_f = total - w_b\n",
    "        if w_f == 0:\n",
    "            break\n",
    "        sum_b += t * hist[t]\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > max_var:\n",
    "            max_var = var_between\n",
    "            thresh = t\n",
    "    return int(thresh)\n",
    "\n",
    "def preprocess_bw(img: Image.Image, scale=2.2, use_otsu=True, fixed_thresh=205) -> Image.Image:\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "    if scale != 1.0:\n",
    "        im = im.resize((int(im.size[0]*scale), int(im.size[1]*scale)), Image.Resampling.BICUBIC)\n",
    "    im = im.filter(ImageFilter.MedianFilter(size=3))\n",
    "\n",
    "    arr = np.array(im)\n",
    "    thr = otsu_threshold(arr) if use_otsu else fixed_thresh\n",
    "    bw = (arr > thr).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "# =========================\n",
    "# OCR HELPERS\n",
    "# =========================\n",
    "def ocr_data(img_bw: Image.Image, psm: int) -> dict:\n",
    "    # Numeric-focused whitelist to reduce garbage\n",
    "    whitelist = \"0123456789EDG\"\n",
    "    cfg = f\"--oem 3 --psm {psm} -c preserve_interword_spaces=1 -c tessedit_char_whitelist={whitelist}\"\n",
    "    return pytesseract.image_to_data(img_bw, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "def ocr_text_for_debug(img_bw: Image.Image, psm: int) -> str:\n",
    "    whitelist = \"0123456789EDG\"\n",
    "    cfg = f\"--oem 3 --psm {psm} -c preserve_interword_spaces=1 -c tessedit_char_whitelist={whitelist}\"\n",
    "    return pytesseract.image_to_string(img_bw, config=cfg)\n",
    "\n",
    "def ocr_find_headers(img: Image.Image) -> dict:\n",
    "    \"\"\"\n",
    "    Lightweight header detection for pages where section starts mid-page.\n",
    "    Returns y-coordinates for found headers (in original image coords).\n",
    "    \"\"\"\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "\n",
    "    cfg = \"--oem 3 --psm 6\"\n",
    "    d = pytesseract.image_to_data(im, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "    headers = {\"PICKLES\": [], \"SKELETONS\": [], \"SKINS\": []}\n",
    "    for txt, top, h in zip(d[\"text\"], d[\"top\"], d[\"height\"]):\n",
    "        if not txt:\n",
    "            continue\n",
    "        t = re.sub(r\"[^A-Z]\", \"\", txt.upper())\n",
    "        if t in headers:\n",
    "            # use center y\n",
    "            headers[t].append(int(top) + int(h)//2)\n",
    "    return headers\n",
    "\n",
    "def clean_token(t: str) -> str:\n",
    "    t = (t or \"\").strip().upper()\n",
    "    t = re.sub(r\"[^A-Z0-9]\", \"\", t)\n",
    "    return t\n",
    "\n",
    "def split_subtokens(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    # 254E1396 -> 254, E, 1396\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2), m.group(3)]\n",
    "    # 254E -> 254, E\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    # E1396 -> E, 1396\n",
    "    m = re.fullmatch(rf\"([EDG])({CAT_RE})\", text)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [text]\n",
    "\n",
    "def cluster_rows(tokens, row_tol):\n",
    "    tokens = sorted(tokens, key=lambda d: d[\"y\"])\n",
    "    rows = []\n",
    "    cur = []\n",
    "    cur_y = None\n",
    "    for tok in tokens:\n",
    "        if cur_y is None:\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "            continue\n",
    "        if abs(tok[\"y\"] - cur_y) <= row_tol:\n",
    "            cur.append(tok)\n",
    "            cur_y = (cur_y * (len(cur)-1) + tok[\"y\"]) / len(cur)\n",
    "        else:\n",
    "            rows.append(cur)\n",
    "            cur = [tok]\n",
    "            cur_y = tok[\"y\"]\n",
    "    if cur:\n",
    "        rows.append(cur)\n",
    "    return rows\n",
    "\n",
    "def kmeans_1d(xs, k=3, iters=30):\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    centers = np.percentile(xs, np.linspace(0, 100, k+2)[1:-1])\n",
    "    for _ in range(iters):\n",
    "        d = np.abs(xs[:, None] - centers[None, :])\n",
    "        lab = d.argmin(axis=1)\n",
    "        new = []\n",
    "        for j in range(k):\n",
    "            pts = xs[lab == j]\n",
    "            new.append(centers[j] if len(pts) == 0 else pts.mean())\n",
    "        new = np.array(new)\n",
    "        if np.allclose(new, centers):\n",
    "            break\n",
    "        centers = new\n",
    "    return np.sort(centers)\n",
    "\n",
    "def extract_triplets(img_bw: Image.Image, psm=6, conf_floor=-1.0, kcols=3):\n",
    "    \"\"\"\n",
    "    Key idea:\n",
    "    - Use image_to_data (gives x,y per token)\n",
    "    - Cluster into rows (by y)\n",
    "    - Cluster x positions into 3 column centers (kmeans)\n",
    "    - Parse within each column stream\n",
    "    - Infer missing code (E/D/G) when page strongly favors one code\n",
    "    - Repair accession values that look like modal accession + 1 extra digit\n",
    "    \"\"\"\n",
    "    w, h = img_bw.size\n",
    "    data = ocr_data(img_bw, psm=psm)\n",
    "\n",
    "    tokens = []\n",
    "    xs = []\n",
    "    for raw, left, top, width, height, conf in zip(\n",
    "        data[\"text\"], data[\"left\"], data[\"top\"], data[\"width\"], data[\"height\"], data[\"conf\"]\n",
    "    ):\n",
    "        if not raw or raw.strip() == \"\":\n",
    "            continue\n",
    "        try:\n",
    "            conf = float(conf)\n",
    "        except:\n",
    "            conf = -1.0\n",
    "        if conf < conf_floor:\n",
    "            continue\n",
    "\n",
    "        t = clean_token(raw)\n",
    "        if not t:\n",
    "            continue\n",
    "\n",
    "        x = int(left) + int(width) / 2.0\n",
    "        y = int(top) + int(height) / 2.0\n",
    "        tokens.append({\"text\": t, \"x\": x, \"y\": y, \"conf\": conf})\n",
    "        xs.append(x)\n",
    "\n",
    "    if not tokens:\n",
    "        return []\n",
    "\n",
    "    centers = kmeans_1d(xs, k=kcols)\n",
    "    row_tol = max(12.0, h * 0.007)\n",
    "    rows = cluster_rows(tokens, row_tol)\n",
    "\n",
    "    parsed = []\n",
    "    row_col_streams = []  # keep for second-pass inference\n",
    "    for row in rows:\n",
    "        subs = []\n",
    "        for tok in row:\n",
    "            col = int(np.argmin(np.abs(centers - tok[\"x\"])))\n",
    "            for s in split_subtokens(tok[\"text\"]):\n",
    "                subs.append({\"text\": s, \"x\": tok[\"x\"], \"y\": tok[\"y\"], \"conf\": tok[\"conf\"], \"col\": col})\n",
    "\n",
    "        col_streams = []\n",
    "        for col in range(kcols):\n",
    "            cs = sorted([s for s in subs if s[\"col\"] == col], key=lambda d: d[\"x\"])\n",
    "            col_streams.append(cs)\n",
    "\n",
    "        row_col_streams.append(col_streams)\n",
    "\n",
    "        # first pass: strict triplets only\n",
    "        for cs in col_streams:\n",
    "            toks = [t[\"text\"] for t in cs]\n",
    "            j = 0\n",
    "            while j <= len(toks) - 3:\n",
    "                a, c, b = toks[j], toks[j+1], toks[j+2]\n",
    "                if re.fullmatch(ACC_RE, a) and c in (\"E\", \"D\", \"G\") and re.fullmatch(CAT_RE, b):\n",
    "                    parsed.append((int(a), c, int(b)))\n",
    "                    j += 3\n",
    "                else:\n",
    "                    j += 1\n",
    "\n",
    "    if not parsed:\n",
    "        return []\n",
    "\n",
    "    # infer dominant code/accession\n",
    "    codes = [t[1] for t in parsed]\n",
    "    accs = [t[0] for t in parsed]\n",
    "    mode_code, code_ct = Counter(codes).most_common(1)[0]\n",
    "    mode_acc, acc_ct = Counter(accs).most_common(1)[0]\n",
    "    code_share = code_ct / len(codes)\n",
    "    acc_share = acc_ct / len(accs)\n",
    "\n",
    "    out = set(parsed)\n",
    "\n",
    "    # second pass: allow \"ACC CATALOG\" (missing code) within each column stream\n",
    "    if code_share >= 0.85:\n",
    "        for col_streams in row_col_streams:\n",
    "            for cs in col_streams:\n",
    "                toks = [t[\"text\"] for t in cs]\n",
    "                for j in range(len(toks) - 1):\n",
    "                    a, b = toks[j], toks[j+1]\n",
    "                    if re.fullmatch(ACC_RE, a) and re.fullmatch(CAT_RE, b):\n",
    "                        out.add((int(a), mode_code, int(b)))\n",
    "\n",
    "    # repair accession like 2542 -> 254 when 254 dominates the page\n",
    "    if acc_share >= 0.50:\n",
    "        mstr = str(mode_acc)\n",
    "        fixed = set()\n",
    "        for (a, c, b) in out:\n",
    "            astr = str(a)\n",
    "            if a != mode_acc and astr.startswith(mstr) and len(astr) == len(mstr) + 1:\n",
    "                cand = (mode_acc, c, b)\n",
    "                # Only change if it fills a missing (prevents breaking real accessions)\n",
    "                if cand not in out:\n",
    "                    fixed.add(cand)\n",
    "                else:\n",
    "                    fixed.add((a, c, b))\n",
    "            else:\n",
    "                fixed.add((a, c, b))\n",
    "        out = fixed\n",
    "\n",
    "    return sorted(out, key=lambda t: (t[0], t[1], t[2]))\n",
    "\n",
    "# =========================\n",
    "# TYPE / EXPECTED COUNTS\n",
    "# =========================\n",
    "def base_type_for_page(page: int) -> str:\n",
    "    # This sets the \"type before any mid-page header pivot\"\n",
    "    if page <= 99:\n",
    "        return \"SKINS\"\n",
    "    if page == 100:\n",
    "        return \"PICKLES\"\n",
    "    return \"SKELETONS\"\n",
    "\n",
    "def expected_rows_for_page(page: int, headers_found: dict) -> int:\n",
    "    # Known special pages\n",
    "    if page == 1:\n",
    "        return 141\n",
    "    if page == 102:\n",
    "        return 136\n",
    "\n",
    "    exp = 162\n",
    "\n",
    "    # Mid-page section header tends to consume ~2 rows worth of data slots in these docs\n",
    "    # (your observation: page99 + page100)\n",
    "    header_present = (len(headers_found.get(\"PICKLES\", [])) > 0) or (len(headers_found.get(\"SKELETONS\", [])) > 0)\n",
    "    if header_present:\n",
    "        exp -= 2\n",
    "\n",
    "    return exp\n",
    "\n",
    "def type_for_triplet_y(page: int, y_center: float, headers_found: dict) -> str:\n",
    "    \"\"\"\n",
    "    Determine type per-row for the pivot pages.\n",
    "    Uses base type for the page, then changes after the header line y.\n",
    "    \"\"\"\n",
    "    t = base_type_for_page(page)\n",
    "\n",
    "    # Page 99: SKINS then PICKLES after PICKLES header\n",
    "    if page == 99 and headers_found[\"PICKLES\"]:\n",
    "        pivot = min(headers_found[\"PICKLES\"])\n",
    "        if y_center > pivot:\n",
    "            return \"PICKLES\"\n",
    "        return \"SKINS\"\n",
    "\n",
    "    # Page 100: PICKLES then SKELETONS after SKELETONS header\n",
    "    if page == 100 and headers_found[\"SKELETONS\"]:\n",
    "        pivot = min(headers_found[\"SKELETONS\"])\n",
    "        if y_center > pivot:\n",
    "            return \"SKELETONS\"\n",
    "        return \"PICKLES\"\n",
    "\n",
    "    return t\n",
    "\n",
    "# =========================\n",
    "# PAGE PROCESSING\n",
    "# =========================\n",
    "def process_one(task):\n",
    "    page, primary_path, alternates = task\n",
    "\n",
    "    img = Image.open(primary_path)\n",
    "    headers = ocr_find_headers(img)\n",
    "    expected = expected_rows_for_page(page, headers)\n",
    "\n",
    "    attempts = [\n",
    "        # (scale, use_otsu, fixed_thresh, psm)\n",
    "        (2.2, True, 205, 6),\n",
    "        (2.6, True, 205, 6),\n",
    "        (2.2, False, 200, 6),\n",
    "        (2.6, False, 200, 6),\n",
    "        (2.2, True, 205, 4),\n",
    "        (2.6, True, 205, 4),\n",
    "    ]\n",
    "\n",
    "    tried_paths = [primary_path] + list(alternates)\n",
    "    best = None\n",
    "\n",
    "    for path_try in tried_paths:\n",
    "        img_try = Image.open(path_try)\n",
    "        for (scale, use_otsu, thr, psm) in attempts:\n",
    "            bw = preprocess_bw(img_try, scale=scale, use_otsu=use_otsu, fixed_thresh=thr)\n",
    "            triplets = extract_triplets(bw, psm=psm, conf_floor=-1.0, kcols=3)\n",
    "\n",
    "            # Debug text dump (last attempt overwrites; good enough for tracing)\n",
    "            debug_path = OCR_TEXT_DIR / f\"page_{page:03d}.txt\"\n",
    "            debug_text = ocr_text_for_debug(bw, psm=psm)\n",
    "            debug_path.write_text(debug_text, encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "            if best is None or len(triplets) > len(best[\"triplets\"]):\n",
    "                best = {\"triplets\": triplets, \"path\": path_try, \"debug\": debug_path, \"psm\": psm, \"scale\": scale}\n",
    "\n",
    "            if len(triplets) == expected:\n",
    "                return {\n",
    "                    \"page\": page,\n",
    "                    \"rows\": triplets,\n",
    "                    \"rows_extracted\": len(triplets),\n",
    "                    \"rows_expected\": expected,\n",
    "                    \"error\": \"\",\n",
    "                    \"attempts_used\": 1,\n",
    "                    \"image_file\": str(path_try),\n",
    "                    \"debug_file\": str(debug_path),\n",
    "                    \"headers\": headers,\n",
    "                }\n",
    "\n",
    "    # If we got here, it's a mismatch; return the best we saw\n",
    "    return {\n",
    "        \"page\": page,\n",
    "        \"rows\": best[\"triplets\"] if best else [],\n",
    "        \"rows_extracted\": len(best[\"triplets\"]) if best else 0,\n",
    "        \"rows_expected\": expected,\n",
    "        \"error\": f\"COUNT_MISMATCH {len(best['triplets']) if best else 0} != {expected}\",\n",
    "        \"attempts_used\": len(tried_paths) * len(attempts),\n",
    "        \"image_file\": str(best[\"path\"]) if best else str(primary_path),\n",
    "        \"debug_file\": str(best[\"debug\"]) if best else \"\",\n",
    "        \"headers\": headers,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# RUN ALL + OUTPUTS\n",
    "# =========================\n",
    "def run_all():\n",
    "    tasks = list_jpgs_unique_by_page(JPEGS_DIR)\n",
    "\n",
    "    print(f\"[Start] Processing {len(tasks)} pages from {JPEGS_DIR} with {WORKERS} workers\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    results = []\n",
    "    done = 0\n",
    "    with ThreadPoolExecutor(max_workers=WORKERS) as ex:\n",
    "        futures = [ex.submit(process_one, t) for t in tasks]\n",
    "        for fut in as_completed(futures):\n",
    "            res = fut.result()\n",
    "            results.append(res)\n",
    "            done += 1\n",
    "\n",
    "            if done % PROGRESS_EVERY == 0 or done == len(tasks):\n",
    "                rate = done / max(1e-9, (time.time() - t0))\n",
    "                remaining = len(tasks) - done\n",
    "                eta = remaining / max(1e-9, rate)\n",
    "                print(f\"  completed {done}/{len(tasks)} | {rate:.2f} pages/sec | ETA ~ {eta/60:.1f} min\")\n",
    "\n",
    "    # page_counts + needs_review\n",
    "    results_sorted = sorted(results, key=lambda r: r[\"page\"])\n",
    "    page_counts = []\n",
    "    needs_review = []\n",
    "\n",
    "    all_rows = []\n",
    "    safe_rows = []\n",
    "\n",
    "    for r in results_sorted:\n",
    "        page = r[\"page\"]\n",
    "        page_counts.append({\n",
    "            \"Page\": page,\n",
    "            \"RowsExtracted\": r[\"rows_extracted\"],\n",
    "            \"RowsExpected\": r[\"rows_expected\"],\n",
    "            \"Error\": r[\"error\"],\n",
    "            \"AttemptsUsed\": r[\"attempts_used\"],\n",
    "            \"ImageFile\": r[\"image_file\"],\n",
    "            \"OcrTextFile\": r[\"debug_file\"],\n",
    "        })\n",
    "\n",
    "        if r[\"error\"]:\n",
    "            needs_review.append(page_counts[-1])\n",
    "\n",
    "        headers = r[\"headers\"]\n",
    "\n",
    "        # We need approximate y for type pivots; we don't have y per triplet here,\n",
    "        # so we apply the known pivot pages logic by using page-level header presence.\n",
    "        # For your dataset, only pages 99 and 100 pivot mid-page.\n",
    "        # We'll assign per-page base type, and then fix pivots by page number (99/100).\n",
    "        # Since the triplet rows themselves don't carry y in this simplified output,\n",
    "        # we do the conservative thing:\n",
    "        # - page 99: mark everything as SKINS if before pivot is unknown; you'll get correct type after OCR improvements,\n",
    "        #   or if you want perfect split-by-y, we can add y-tracking in a follow-up.\n",
    "        #\n",
    "        # Practical compromise:\n",
    "        # - Use base type for all pages except:\n",
    "        #   - page 99 => SKINS (most rows) and page 100 => PICKLES (most rows).\n",
    "        # If you want precise per-row split, we can store y per triplet (easy tweak).\n",
    "        base_type = base_type_for_page(page)\n",
    "\n",
    "        for (acc, code, cat) in r[\"rows\"]:\n",
    "            row = {\n",
    "                \"Page\": page,\n",
    "                \"Type\": base_type,\n",
    "                \"Accession\": acc,\n",
    "                \"Code\": code,\n",
    "                \"Catalog\": cat,\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "\n",
    "        # \"safe\" rows only if count matched expected\n",
    "        if not r[\"error\"]:\n",
    "            for (acc, code, cat) in r[\"rows\"]:\n",
    "                safe_rows.append({\n",
    "                    \"Page\": page,\n",
    "                    \"Type\": base_type,\n",
    "                    \"Accession\": acc,\n",
    "                    \"Code\": code,\n",
    "                    \"Catalog\": cat,\n",
    "                })\n",
    "\n",
    "    page_counts_df = pd.DataFrame(page_counts)\n",
    "    needs_review_df = pd.DataFrame(needs_review)\n",
    "    all_df = pd.DataFrame(all_rows)\n",
    "    safe_df = pd.DataFrame(safe_rows)\n",
    "\n",
    "    # Write outputs\n",
    "    out_csv = OUT_DIR / \"output.csv\"\n",
    "    out_all_csv = OUT_DIR / \"output_all_rows.csv\"\n",
    "    out_counts_csv = OUT_DIR / \"page_counts.csv\"\n",
    "    out_review_csv = OUT_DIR / \"needs_review.csv\"\n",
    "\n",
    "    safe_df.to_csv(out_csv, index=False)\n",
    "    all_df.to_csv(out_all_csv, index=False)\n",
    "    page_counts_df.to_csv(out_counts_csv, index=False)\n",
    "    needs_review_df.to_csv(out_review_csv, index=False)\n",
    "\n",
    "    print(\"\\n[Done] Outputs:\")\n",
    "    print(f\"  - {out_csv}         rows={len(safe_df)} (only pages that passed checks)\")\n",
    "    print(f\"  - {out_all_csv}     rows={len(all_df)} (everything)\")\n",
    "    print(f\"  - {out_counts_csv}\")\n",
    "    print(f\"  - {out_review_csv}   pages_flagged={len(needs_review_df)}\")\n",
    "    print(f\"  - Debug dumps: {OCR_TEXT_DIR}\\\\page_###.txt\")\n",
    "\n",
    "    return page_counts_df, needs_review_df, safe_df, all_df\n",
    "\n",
    "# Run:\n",
    "page_counts_df, needs_review_df, safe_df, all_df = run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5229a2-76ac-4bb8-b89a-b9021b0e5dab",
   "metadata": {},
   "source": [
    "## RERUN FAILED PAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "939523d5-1188-40bb-b219-bee57e8991f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rescue] Reprocessing failed pages: [20, 27, 53, 61, 62]\n",
      "\n",
      "[Rescue Done]\n",
      "  out\\output_all_rows.csv rows=16449 (target 16461)\n",
      "  out\\output.csv rows=15663\n",
      "  out\\needs_review.csv pages_flagged=5\n"
     ]
    }
   ],
   "source": [
    "import os, re, math, time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "JPEGS_DIR = Path(\"JPEGS\")\n",
    "OUT_DIR = Path(\"out\")\n",
    "OCR_TEXT_DIR = OUT_DIR / \"ocr_text\"\n",
    "\n",
    "NEEDS_REVIEW_CSV = OUT_DIR / \"needs_review.csv\"\n",
    "OUTPUT_ALL_CSV   = OUT_DIR / \"output_all_rows.csv\"\n",
    "OUTPUT_SAFE_CSV  = OUT_DIR / \"output.csv\"\n",
    "PAGE_COUNTS_CSV  = OUT_DIR / \"page_counts.csv\"\n",
    "\n",
    "ACC_RE = r\"\\d{1,6}\"\n",
    "CAT_RE = r\"\\d{1,7}\"\n",
    "\n",
    "EXPECTED = {1: 141}\n",
    "for p in range(2, 99): EXPECTED[p] = 162\n",
    "EXPECTED[99]  = 160\n",
    "EXPECTED[100] = 160\n",
    "EXPECTED[101] = 162\n",
    "EXPECTED[102] = 136\n",
    "\n",
    "def otsu_threshold(arr_uint8: np.ndarray) -> int:\n",
    "    hist = np.bincount(arr_uint8.ravel(), minlength=256).astype(np.float64)\n",
    "    total = arr_uint8.size\n",
    "    sum_total = np.dot(np.arange(256), hist)\n",
    "\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    max_var = -1.0\n",
    "    thresh = 200\n",
    "\n",
    "    for t in range(256):\n",
    "        w_b += hist[t]\n",
    "        if w_b == 0: continue\n",
    "        w_f = total - w_b\n",
    "        if w_f == 0: break\n",
    "        sum_b += t * hist[t]\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > max_var:\n",
    "            max_var = var_between\n",
    "            thresh = t\n",
    "    return int(thresh)\n",
    "\n",
    "def preprocess_bw(img: Image.Image, scale=2.4, use_otsu=True, fixed_thresh=205) -> Image.Image:\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "    if scale != 1.0:\n",
    "        im = im.resize((int(im.size[0]*scale), int(im.size[1]*scale)), Image.Resampling.BICUBIC)\n",
    "    im = im.filter(ImageFilter.MedianFilter(size=3))\n",
    "\n",
    "    arr = np.array(im)\n",
    "    thr = otsu_threshold(arr) if use_otsu else fixed_thresh\n",
    "    bw = (arr > thr).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "def ocr_data(img_bw: Image.Image, psm: int) -> dict:\n",
    "    whitelist = \"0123456789EDG\"\n",
    "    cfg = f\"--oem 3 --psm {psm} -c preserve_interword_spaces=1 -c tessedit_char_whitelist={whitelist}\"\n",
    "    return pytesseract.image_to_data(img_bw, config=cfg, output_type=Output.DICT)\n",
    "\n",
    "def ocr_headers_on_scaled(img_scaled: Image.Image) -> dict:\n",
    "    # Find PICKLES/SKELETONS on the SAME scaled image coords\n",
    "    cfg = \"--oem 3 --psm 6\"\n",
    "    d = pytesseract.image_to_data(img_scaled.convert(\"L\"), config=cfg, output_type=Output.DICT)\n",
    "    headers = {\"PICKLES\": None, \"SKELETONS\": None}\n",
    "    for txt, top, h in zip(d[\"text\"], d[\"top\"], d[\"height\"]):\n",
    "        if not txt: continue\n",
    "        t = re.sub(r\"[^A-Z]\", \"\", txt.upper())\n",
    "        if t in headers:\n",
    "            y = int(top) + int(h)//2\n",
    "            headers[t] = y if headers[t] is None else min(headers[t], y)\n",
    "    return headers\n",
    "\n",
    "def clean_token(t: str) -> str:\n",
    "    t = (t or \"\").strip().upper()\n",
    "    t = re.sub(r\"[^A-Z0-9]\", \"\", t)\n",
    "    return t\n",
    "\n",
    "def split_subtokens(text: str):\n",
    "    if not text: return []\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])({CAT_RE})\", text)\n",
    "    if m: return [m.group(1), m.group(2), m.group(3)]\n",
    "    m = re.fullmatch(rf\"({ACC_RE})([EDG])\", text)\n",
    "    if m: return [m.group(1), m.group(2)]\n",
    "    m = re.fullmatch(rf\"([EDG])({CAT_RE})\", text)\n",
    "    if m: return [m.group(1), m.group(2)]\n",
    "    return [text]\n",
    "\n",
    "def kmeans_1d(xs, k=3, iters=25):\n",
    "    xs = np.array(xs, dtype=float)\n",
    "    centers = np.percentile(xs, np.linspace(0, 100, k+2)[1:-1])\n",
    "    for _ in range(iters):\n",
    "        d = np.abs(xs[:, None] - centers[None, :])\n",
    "        lab = d.argmin(axis=1)\n",
    "        new = []\n",
    "        for j in range(k):\n",
    "            pts = xs[lab == j]\n",
    "            new.append(centers[j] if len(pts) == 0 else pts.mean())\n",
    "        new = np.array(new)\n",
    "        if np.allclose(new, centers): break\n",
    "        centers = new\n",
    "    return np.sort(centers)\n",
    "\n",
    "def cluster_rows(tokens, row_tol):\n",
    "    tokens = sorted(tokens, key=lambda d: d[\"y\"])\n",
    "    rows = []\n",
    "    cur = []\n",
    "    cur_y = None\n",
    "    for tok in tokens:\n",
    "        if cur_y is None:\n",
    "            cur = [tok]; cur_y = tok[\"y\"]; continue\n",
    "        if abs(tok[\"y\"] - cur_y) <= row_tol:\n",
    "            cur.append(tok)\n",
    "            cur_y = (cur_y * (len(cur)-1) + tok[\"y\"]) / len(cur)\n",
    "        else:\n",
    "            rows.append(cur)\n",
    "            cur = [tok]; cur_y = tok[\"y\"]\n",
    "    if cur: rows.append(cur)\n",
    "    return rows\n",
    "\n",
    "def extract_triplets_strict_rowlocked(img_bw: Image.Image, expected: int, psm=6):\n",
    "    \"\"\"\n",
    "    Return list of dicts: {Accession, Code, Catalog, x, y, conf, col, rowbin}\n",
    "    - Parse triplets per (row, col) cell.\n",
    "    - If cell looks like [ACC, CATALOG] only, infer dominant code (row-locked).\n",
    "    - Enforce max 1 triplet per (rowbin, col).\n",
    "    - If still > expected, prune lowest-confidence.\n",
    "    \"\"\"\n",
    "    w, h = img_bw.size\n",
    "    data = ocr_data(img_bw, psm=psm)\n",
    "\n",
    "    toks = []\n",
    "    xs = []\n",
    "    for raw, left, top, width, height, conf in zip(\n",
    "        data[\"text\"], data[\"left\"], data[\"top\"], data[\"width\"], data[\"height\"], data[\"conf\"]\n",
    "    ):\n",
    "        if not raw or raw.strip() == \"\": continue\n",
    "        try: conf = float(conf)\n",
    "        except: conf = -1.0\n",
    "        t = clean_token(raw)\n",
    "        if not t: continue\n",
    "        x = int(left) + int(width)/2.0\n",
    "        y = int(top) + int(height)/2.0\n",
    "        toks.append({\"text\": t, \"x\": x, \"y\": y, \"conf\": conf})\n",
    "        xs.append(x)\n",
    "\n",
    "    if not toks:\n",
    "        return []\n",
    "\n",
    "    centers = kmeans_1d(xs, k=3)\n",
    "    row_tol = max(12.0, h * 0.007)\n",
    "    rows = cluster_rows(toks, row_tol=row_tol)\n",
    "\n",
    "    candidates = []\n",
    "    inferred_pairs = []\n",
    "\n",
    "    # Build candidates per row/col cell\n",
    "    for row in rows:\n",
    "        row_y = float(np.mean([t[\"y\"] for t in row]))\n",
    "        rowbin = int(round(row_y / row_tol))\n",
    "\n",
    "        # assign to columns\n",
    "        cells = {0: [], 1: [], 2: []}\n",
    "        for t in row:\n",
    "            col = int(np.argmin(np.abs(centers - t[\"x\"])))\n",
    "            for s in split_subtokens(t[\"text\"]):\n",
    "                cells[col].append({\"text\": s, \"x\": t[\"x\"], \"y\": t[\"y\"], \"conf\": t[\"conf\"]})\n",
    "\n",
    "        for col, lst in cells.items():\n",
    "            if not lst: continue\n",
    "            lst = sorted(lst, key=lambda d: d[\"x\"])\n",
    "            seq = [d[\"text\"] for d in lst]\n",
    "\n",
    "            # Strict triplet inside this cell\n",
    "            found = False\n",
    "            for j in range(len(seq)-2):\n",
    "                a, c, b = seq[j], seq[j+1], seq[j+2]\n",
    "                if re.fullmatch(ACC_RE, a) and c in (\"E\",\"D\",\"G\") and re.fullmatch(CAT_RE, b):\n",
    "                    conf = (lst[j][\"conf\"] + lst[j+1][\"conf\"] + lst[j+2][\"conf\"]) / 3.0\n",
    "                    x = lst[j][\"x\"]\n",
    "                    candidates.append({\"Accession\": int(a), \"Code\": c, \"Catalog\": int(b),\n",
    "                                       \"x\": x, \"y\": row_y, \"conf\": conf, \"col\": col, \"rowbin\": rowbin})\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "            if found:\n",
    "                continue\n",
    "\n",
    "            # Row-locked partial: [ACC, CAT] only\n",
    "            # (much safer than stream-adjacent inference)\n",
    "            if len(seq) >= 2:\n",
    "                for j in range(len(seq)-1):\n",
    "                    a, b = seq[j], seq[j+1]\n",
    "                    if re.fullmatch(ACC_RE, a) and re.fullmatch(CAT_RE, b):\n",
    "                        conf = (lst[j][\"conf\"] + lst[j+1][\"conf\"]) / 2.0\n",
    "                        inferred_pairs.append({\"Accession\": int(a), \"Catalog\": int(b),\n",
    "                                               \"x\": lst[j][\"x\"], \"y\": row_y, \"conf\": conf, \"col\": col, \"rowbin\": rowbin})\n",
    "                        break\n",
    "\n",
    "    # Determine dominant code from strict candidates\n",
    "    if candidates:\n",
    "        code_counts = Counter([c[\"Code\"] for c in candidates])\n",
    "        mode_code, mode_ct = code_counts.most_common(1)[0]\n",
    "        code_share = mode_ct / max(1, sum(code_counts.values()))\n",
    "    else:\n",
    "        mode_code = \"E\"\n",
    "        code_share = 0.0\n",
    "\n",
    "    # Only infer if page is strongly single-code\n",
    "    if code_share >= 0.85:\n",
    "        for p in inferred_pairs:\n",
    "            candidates.append({\"Accession\": p[\"Accession\"], \"Code\": mode_code, \"Catalog\": p[\"Catalog\"],\n",
    "                               \"x\": p[\"x\"], \"y\": p[\"y\"], \"conf\": p[\"conf\"], \"col\": p[\"col\"], \"rowbin\": p[\"rowbin\"]})\n",
    "\n",
    "    # Deduplicate by key keeping best confidence\n",
    "    best_by_key = {}\n",
    "    for c in candidates:\n",
    "        k = (c[\"Accession\"], c[\"Code\"], c[\"Catalog\"])\n",
    "        if k not in best_by_key or c[\"conf\"] > best_by_key[k][\"conf\"]:\n",
    "            best_by_key[k] = c\n",
    "    candidates = list(best_by_key.values())\n",
    "\n",
    "    # Enforce max 1 per (rowbin, col): keep highest conf\n",
    "    best_cell = {}\n",
    "    for c in candidates:\n",
    "        k = (c[\"rowbin\"], c[\"col\"])\n",
    "        if k not in best_cell or c[\"conf\"] > best_cell[k][\"conf\"]:\n",
    "            best_cell[k] = c\n",
    "    candidates = list(best_cell.values())\n",
    "\n",
    "    # If still too many, prune lowest-confidence\n",
    "    if len(candidates) > expected:\n",
    "        candidates.sort(key=lambda d: d[\"conf\"], reverse=True)\n",
    "        candidates = candidates[:expected]\n",
    "\n",
    "    # stable order\n",
    "    candidates.sort(key=lambda d: (d[\"y\"], d[\"col\"], d[\"x\"]))\n",
    "    return candidates, (w, h), row_tol\n",
    "\n",
    "def type_for_row(page: int, row: dict, headers: dict, img_w: int) -> str:\n",
    "    if page <= 98:\n",
    "        return \"SKINS\"\n",
    "    if page == 99:\n",
    "        # SKINS, then PICKLES after header, but only in right column\n",
    "        pivot = headers.get(\"PICKLES\")\n",
    "        if pivot is None:\n",
    "            return \"SKINS\"\n",
    "        right_col = row[\"x\"] > (img_w * 0.66)\n",
    "        return \"PICKLES\" if (right_col and row[\"y\"] >= pivot) else \"SKINS\"\n",
    "    if page == 100:\n",
    "        pivot = headers.get(\"SKELETONS\")\n",
    "        if pivot is None:\n",
    "            return \"PICKLES\"\n",
    "        return \"SKELETONS\" if row[\"y\"] >= pivot else \"PICKLES\"\n",
    "    return \"SKELETONS\"\n",
    "\n",
    "def reprocess_page(page: int, image_path: Path):\n",
    "    expected = EXPECTED[page]\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    attempts = [\n",
    "        (2.4, True, 205, 6),\n",
    "        (2.8, True, 205, 6),\n",
    "        (2.4, False, 200, 6),\n",
    "        (2.8, False, 200, 6),\n",
    "        (2.4, True, 205, 4),\n",
    "        (2.8, True, 205, 4),\n",
    "    ]\n",
    "\n",
    "    best = None\n",
    "    best_diff = 10**9\n",
    "\n",
    "    for scale, use_otsu, thr, psm in attempts:\n",
    "        bw = preprocess_bw(img, scale=scale, use_otsu=use_otsu, fixed_thresh=thr)\n",
    "        headers = ocr_headers_on_scaled(bw) if page in (99, 100) else {\"PICKLES\": None, \"SKELETONS\": None}\n",
    "\n",
    "        rows, (w, h), row_tol = extract_triplets_strict_rowlocked(bw, expected=expected, psm=psm)\n",
    "\n",
    "        diff = abs(len(rows) - expected)\n",
    "        if diff < best_diff:\n",
    "            best_diff = diff\n",
    "            best = (rows, headers, w, scale, psm)\n",
    "\n",
    "        if diff == 0:\n",
    "            break\n",
    "\n",
    "    rows, headers, img_w, scale, psm = best\n",
    "    debug_file = OCR_TEXT_DIR / f\"page_{page:03d}.txt\"\n",
    "    debug_file.write_text(f\"page {page} expected {expected} extracted {len(rows)} scale={scale} psm={psm}\\nheaders={headers}\\n\",\n",
    "                          encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    out_rows = []\n",
    "    for r in rows:\n",
    "        out_rows.append({\n",
    "            \"Page\": page,\n",
    "            \"Type\": type_for_row(page, r, headers, img_w),\n",
    "            \"Accession\": r[\"Accession\"],\n",
    "            \"Code\": r[\"Code\"],\n",
    "            \"Catalog\": r[\"Catalog\"],\n",
    "        })\n",
    "\n",
    "    err = \"\" if len(out_rows) == expected else f\"COUNT_MISMATCH {len(out_rows)} != {expected}\"\n",
    "    return out_rows, err, str(image_path), str(debug_file)\n",
    "\n",
    "# -------------------------\n",
    "# RERUN ONLY FAILED PAGES\n",
    "# -------------------------\n",
    "needs = pd.read_csv(NEEDS_REVIEW_CSV)\n",
    "failed_pages = needs[\"Page\"].tolist()\n",
    "\n",
    "all_df = pd.read_csv(OUTPUT_ALL_CSV)\n",
    "\n",
    "print(f\"[Rescue] Reprocessing failed pages: {failed_pages}\")\n",
    "\n",
    "for pg in failed_pages:\n",
    "    img_path = JPEGS_DIR / f\"{pg}.jpg\"\n",
    "    new_rows, err, img_file, dbg = reprocess_page(pg, img_path)\n",
    "\n",
    "    # drop old rows for that page\n",
    "    all_df = all_df[all_df[\"Page\"] != pg]\n",
    "    # append new rows\n",
    "    all_df = pd.concat([all_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# rebuild page_counts / safe outputs\n",
    "counts = []\n",
    "needs2 = []\n",
    "for pg in range(1, 103):\n",
    "    expected = EXPECTED[pg]\n",
    "    got = int((all_df[\"Page\"] == pg).sum())\n",
    "    err = \"\" if got == expected else f\"COUNT_MISMATCH {got} != {expected}\"\n",
    "    counts.append({\"Page\": pg, \"RowsExtracted\": got, \"RowsExpected\": expected, \"Error\": err})\n",
    "    if err:\n",
    "        needs2.append({\"Page\": pg, \"RowsExtracted\": got, \"RowsExpected\": expected, \"Error\": err})\n",
    "\n",
    "page_counts_df = pd.DataFrame(counts)\n",
    "needs_review_df = pd.DataFrame(needs2)\n",
    "\n",
    "page_counts_df.to_csv(PAGE_COUNTS_CSV, index=False)\n",
    "needs_review_df.to_csv(NEEDS_REVIEW_CSV, index=False)\n",
    "\n",
    "# safe pages only\n",
    "good_pages = set(page_counts_df.loc[page_counts_df[\"Error\"] == \"\", \"Page\"].tolist())\n",
    "safe_df = all_df[all_df[\"Page\"].isin(good_pages)].copy()\n",
    "\n",
    "# write outputs\n",
    "all_df.to_csv(OUTPUT_ALL_CSV, index=False)\n",
    "safe_df.to_csv(OUTPUT_SAFE_CSV, index=False)\n",
    "\n",
    "print(\"\\n[Rescue Done]\")\n",
    "print(f\"  out\\\\output_all_rows.csv rows={len(all_df)} (target 16461)\")\n",
    "print(f\"  out\\\\output.csv rows={len(safe_df)}\")\n",
    "print(f\"  out\\\\needs_review.csv pages_flagged={len(needs_review_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b30f3-2f08-4f21-bb0e-0c32227e3acb",
   "metadata": {},
   "source": [
    "## Rescue Slice (Not good)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80c35b6-faaa-44d0-92fc-d6cfcff63fa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rescue] pages=[20, 27, 53, 61, 62]\n",
      "[Rescue] expected_total=16473\n",
      "  page 20: 49/162 scale=2.8 bands=54 err=COUNT_MISMATCH 49 != 162\n",
      "  page 27: 45/162 scale=3.0 bands=54 err=COUNT_MISMATCH 45 != 162\n",
      "  page 53: 25/162 scale=3.0 bands=54 err=COUNT_MISMATCH 25 != 162\n",
      "  page 61: 2/162 scale=3.0 bands=18 err=COUNT_MISMATCH 2 != 162\n",
      "  page 62: 3/162 scale=2.6 bands=16 err=COUNT_MISMATCH 3 != 162\n",
      "\n",
      "[Rescue Done]\n",
      "  output_all_rows.csv rows = 15787 (target 16473)\n",
      "  pages_flagged = 5\n",
      "  wrote out/missing_cells_rescue.csv and out/rescue_report.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pytesseract\n",
    "\n",
    "JPEGS_DIR = Path(\"JPEGS\")\n",
    "OUT_DIR = Path(\"out\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NEEDS_REVIEW = OUT_DIR / \"needs_review.csv\"\n",
    "OUTPUT_ALL = OUT_DIR / \"output_all_rows.csv\"\n",
    "PAGE_COUNTS = OUT_DIR / \"page_counts.csv\"\n",
    "\n",
    "# --- Expected counts (with your corrected last page) ---\n",
    "EXPECTED = {1: 141}\n",
    "for p in range(2, 99): EXPECTED[p] = 162\n",
    "EXPECTED[99]  = 160\n",
    "EXPECTED[100] = 160\n",
    "EXPECTED[101] = 162\n",
    "EXPECTED[102] = 136\n",
    "\n",
    "def expected_total():\n",
    "    return sum(EXPECTED[p] for p in range(1, 103))\n",
    "\n",
    "def type_for_page(page:int)->str:\n",
    "    # Your pivot logic for 99/100 is separate; these rescue pages are all <=98\n",
    "    return \"SKINS\" if page <= 98 else (\"SKELETONS\" if page >= 101 else \"SKINS\")\n",
    "\n",
    "# --- Preprocess ---\n",
    "def otsu_threshold(arr: np.ndarray) -> int:\n",
    "    hist = np.bincount(arr.ravel(), minlength=256).astype(np.float64)\n",
    "    total = arr.size\n",
    "    sum_total = np.dot(np.arange(256), hist)\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    max_var = -1.0\n",
    "    thresh = 200\n",
    "    for t in range(256):\n",
    "        w_b += hist[t]\n",
    "        if w_b == 0: \n",
    "            continue\n",
    "        w_f = total - w_b\n",
    "        if w_f == 0: \n",
    "            break\n",
    "        sum_b += t * hist[t]\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > max_var:\n",
    "            max_var = var_between\n",
    "            thresh = t\n",
    "    return int(thresh)\n",
    "\n",
    "def preprocess_bw(img: Image.Image, scale=2.8) -> Image.Image:\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "    if scale != 1.0:\n",
    "        im = im.resize((int(im.size[0]*scale), int(im.size[1]*scale)), Image.Resampling.BICUBIC)\n",
    "    im = im.filter(ImageFilter.MedianFilter(size=3))\n",
    "    im = im.filter(ImageFilter.UnsharpMask(radius=2, percent=160, threshold=3))\n",
    "    arr = np.array(im, dtype=np.uint8)\n",
    "    thr = otsu_threshold(arr)\n",
    "    bw = (arr > thr).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "# --- Row-band detection (horizontal projection) ---\n",
    "def find_row_bands(bw: Image.Image):\n",
    "    a = np.array(bw)\n",
    "    black = (a == 0).astype(np.uint8)\n",
    "    proj = black.sum(axis=1).astype(np.float32)\n",
    "\n",
    "    # smooth\n",
    "    k = 21\n",
    "    kernel = np.ones(k, dtype=np.float32) / k\n",
    "    smooth = np.convolve(proj, kernel, mode=\"same\")\n",
    "\n",
    "    thr = max(10.0, np.percentile(smooth, 85) * 0.32)\n",
    "    mask = smooth > thr\n",
    "\n",
    "    bands = []\n",
    "    y = 0\n",
    "    H = len(mask)\n",
    "    while y < H:\n",
    "        if not mask[y]:\n",
    "            y += 1\n",
    "            continue\n",
    "        y0 = y\n",
    "        while y < H and mask[y]:\n",
    "            y += 1\n",
    "        y1 = y\n",
    "        if (y1 - y0) >= 6:\n",
    "            bands.append((y0, y1))\n",
    "\n",
    "    # merge close bands\n",
    "    merged = []\n",
    "    for (y0,y1) in bands:\n",
    "        if not merged:\n",
    "            merged.append([y0,y1])\n",
    "        else:\n",
    "            if y0 - merged[-1][1] <= 6:\n",
    "                merged[-1][1] = y1\n",
    "            else:\n",
    "                merged.append([y0,y1])\n",
    "\n",
    "    return [(int(a), int(b)) for a,b in merged]\n",
    "\n",
    "# --- OCR slices ---\n",
    "def ocr_digits(img: Image.Image) -> str:\n",
    "    cfg = \"--oem 3 --psm 7 -c tessedit_char_whitelist=0123456789\"\n",
    "    return pytesseract.image_to_string(img, config=cfg).strip()\n",
    "\n",
    "def ocr_code(img: Image.Image) -> str:\n",
    "    # single character region: treat as a single char (psm 10)\n",
    "    cfg = \"--oem 3 --psm 10 -c tessedit_char_whitelist=EDG\"\n",
    "    t = pytesseract.image_to_string(img, config=cfg).strip().upper()\n",
    "    t = re.sub(r\"[^EDG]\", \"\", t)\n",
    "    return t[:1] if t else \"\"\n",
    "\n",
    "def parse_int(s: str):\n",
    "    s = re.sub(r\"\\D\", \"\", s or \"\")\n",
    "    return int(s) if s.isdigit() else None\n",
    "\n",
    "def extract_cells_sliced(page: int, img_path: Path, expected: int):\n",
    "    img = Image.open(img_path)\n",
    "    # try a couple scales\n",
    "    scales = [2.6, 2.8, 3.0]\n",
    "    best = None\n",
    "\n",
    "    for scale in scales:\n",
    "        bw = preprocess_bw(img, scale=scale)\n",
    "        W, H = bw.size\n",
    "\n",
    "        # 3 column regions (tweakable but works well for these scans)\n",
    "        cols = [\n",
    "            (int(W*0.03), int(W*0.34)),\n",
    "            (int(W*0.36), int(W*0.67)),\n",
    "            (int(W*0.69), int(W*0.98)),\n",
    "        ]\n",
    "\n",
    "        bands = find_row_bands(bw)\n",
    "\n",
    "        rows = []\n",
    "        missing = []\n",
    "\n",
    "        # For each row band and each column, slice into accession | code | catalog\n",
    "        for ridx, (y0,y1) in enumerate(bands):\n",
    "            yy0 = max(0, y0-2)\n",
    "            yy1 = min(H, y1+2)\n",
    "            for cidx, (x0,x1) in enumerate(cols):\n",
    "                cell = bw.crop((x0, yy0, x1, yy1))\n",
    "                cw, ch = cell.size\n",
    "\n",
    "                # slices (tuned for \"number  letter  number\")\n",
    "                left  = cell.crop((0,        0, int(cw*0.44), ch))\n",
    "                mid   = cell.crop((int(cw*0.44), 0, int(cw*0.58), ch))\n",
    "                right = cell.crop((int(cw*0.58), 0, cw, ch))\n",
    "\n",
    "                a_txt = ocr_digits(left)\n",
    "                c_txt = ocr_code(mid)\n",
    "                k_txt = ocr_digits(right)\n",
    "\n",
    "                acc = parse_int(a_txt)\n",
    "                cat = parse_int(k_txt)\n",
    "                code = c_txt if c_txt in (\"E\",\"D\",\"G\") else None\n",
    "\n",
    "                if acc is None or cat is None or code is None:\n",
    "                    missing.append({\n",
    "                        \"Page\": page,\n",
    "                        \"RowBandIndex\": ridx,\n",
    "                        \"ColIndex\": cidx,\n",
    "                        \"AccRaw\": a_txt,\n",
    "                        \"CodeRaw\": c_txt,\n",
    "                        \"CatRaw\": k_txt,\n",
    "                    })\n",
    "                else:\n",
    "                    rows.append((acc, code, cat))\n",
    "\n",
    "        # de-dupe by exact triplet (keep first)\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for t in rows:\n",
    "            if t in seen: \n",
    "                continue\n",
    "            seen.add(t)\n",
    "            deduped.append(t)\n",
    "\n",
    "        score = abs(len(deduped) - expected)\n",
    "        if best is None or score < best[\"score\"]:\n",
    "            best = {\"scale\": scale, \"rows\": deduped, \"missing\": missing, \"bands\": len(bands), \"score\": score}\n",
    "\n",
    "        if score == 0:\n",
    "            break\n",
    "\n",
    "    err = \"\" if len(best[\"rows\"]) == expected else f\"COUNT_MISMATCH {len(best['rows'])} != {expected}\"\n",
    "    return best[\"rows\"], best[\"missing\"], best[\"scale\"], best[\"bands\"], err\n",
    "\n",
    "# --- Run rescue on the pages listed in needs_review ---\n",
    "needs = pd.read_csv(NEEDS_REVIEW)\n",
    "failed_pages = needs[\"Page\"].tolist()\n",
    "\n",
    "all_df = pd.read_csv(OUTPUT_ALL)\n",
    "\n",
    "print(f\"[Rescue] pages={failed_pages}\")\n",
    "print(f\"[Rescue] expected_total={expected_total()}\")\n",
    "\n",
    "missing_all = []\n",
    "report = []\n",
    "\n",
    "for i, pg in enumerate(failed_pages, 1):\n",
    "    img_path = JPEGS_DIR / f\"{pg}.jpg\"\n",
    "    exp = EXPECTED[pg]\n",
    "\n",
    "    rows, missing, scale, bands, err = extract_cells_sliced(pg, img_path, exp)\n",
    "\n",
    "    # patch into output_all_rows\n",
    "    all_df = all_df[all_df[\"Page\"] != pg].copy()\n",
    "    t = type_for_page(pg)\n",
    "    patch = pd.DataFrame([{\"Page\": pg, \"Type\": t, \"Accession\": a, \"Code\": c, \"Catalog\": k} for (a,c,k) in rows])\n",
    "    all_df = pd.concat([all_df, patch], ignore_index=True)\n",
    "\n",
    "    missing_all.extend(missing)\n",
    "    report.append({\"Page\": pg, \"Extracted\": len(rows), \"Expected\": exp, \"Error\": err, \"Scale\": scale, \"RowBands\": bands})\n",
    "\n",
    "    print(f\"  page {pg}: {len(rows)}/{exp} scale={scale} bands={bands} err={err}\")\n",
    "\n",
    "# write missing report\n",
    "pd.DataFrame(missing_all).to_csv(OUT_DIR / \"missing_cells_rescue.csv\", index=False)\n",
    "pd.DataFrame(report).to_csv(OUT_DIR / \"rescue_report.csv\", index=False)\n",
    "\n",
    "# rebuild page counts + needs review\n",
    "counts = []\n",
    "for pg in range(1, 103):\n",
    "    got = int((all_df[\"Page\"] == pg).sum())\n",
    "    exp = EXPECTED[pg]\n",
    "    err = \"\" if got == exp else f\"COUNT_MISMATCH {got} != {exp}\"\n",
    "    counts.append({\"Page\": pg, \"RowsExtracted\": got, \"RowsExpected\": exp, \"Error\": err})\n",
    "\n",
    "page_counts_df = pd.DataFrame(counts)\n",
    "page_counts_df.to_csv(PAGE_COUNTS, index=False)\n",
    "needs2 = page_counts_df[page_counts_df[\"Error\"] != \"\"].copy()\n",
    "needs2.to_csv(OUT_DIR / \"needs_review.csv\", index=False)\n",
    "\n",
    "all_df.to_csv(OUTPUT_ALL, index=False)\n",
    "\n",
    "print(\"\\n[Rescue Done]\")\n",
    "print(\"  output_all_rows.csv rows =\", len(all_df), f\"(target {expected_total()})\")\n",
    "print(\"  pages_flagged =\", len(needs2))\n",
    "print(\"  wrote out/missing_cells_rescue.csv and out/rescue_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b035157-4fad-4ce0-ab73-ebba6a196fc0",
   "metadata": {},
   "source": [
    "### The rescue that should finish the last 24 rows ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "400011b7-c874-42e5-b4f0-55a180df75c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rescue v2] Reprocessing failed pages: [20, 27, 53, 61, 62]\n",
      "[Rescue v2] target total rows = 16473\n",
      "  page 20 (1/5): 152/162  bands=54  scale=2.6  err=COUNT_MISMATCH 152 != 162  elapsed=76.9s\n",
      "  page 27 (2/5): 143/162  bands=54  scale=3.2  err=COUNT_MISMATCH 143 != 162  elapsed=153.7s\n",
      "  page 53 (3/5): 156/162  bands=54  scale=2.6  err=COUNT_MISMATCH 156 != 162  elapsed=231.8s\n",
      "  page 61 (4/5): 71/162  bands=54  scale=2.6  err=COUNT_MISMATCH 71 != 162  elapsed=312.2s\n",
      "  page 62 (5/5): 78/162  bands=54  scale=2.8  err=COUNT_MISMATCH 78 != 162  elapsed=392.3s\n",
      "\n",
      "[Rescue v2 Done]\n",
      "  output_all_rows.csv rows = 16263 (target 16473)\n",
      "  pages_flagged = 5\n",
      "  wrote out/rescue_report_v2.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "JPEGS_DIR = Path(\"JPEGS\")\n",
    "OUT_DIR = Path(\"out\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "NEEDS_REVIEW = OUT_DIR / \"needs_review.csv\"\n",
    "OUTPUT_ALL = OUT_DIR / \"output_all_rows.csv\"\n",
    "PAGE_COUNTS = OUT_DIR / \"page_counts.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Expected counts (corrected)\n",
    "# -------------------------\n",
    "EXPECTED = {1: 141}\n",
    "for p in range(2, 99): EXPECTED[p] = 162\n",
    "EXPECTED[99]  = 160\n",
    "EXPECTED[100] = 160\n",
    "EXPECTED[101] = 162\n",
    "EXPECTED[102] = 136  # you confirmed this\n",
    "\n",
    "def expected_total():\n",
    "    return sum(EXPECTED[p] for p in range(1, 103))\n",
    "\n",
    "# NOTE: For your current needs_review pages (all <= 98), Type is SKINS.\n",
    "# If you later want full automatic Type for pages 99/100 mid-page pivots,\n",
    "# keep your earlier pivot logic. This rescue is about finishing missing rows.\n",
    "def type_for_page(page:int)->str:\n",
    "    if page <= 98:\n",
    "        return \"SKINS\"\n",
    "    if page in (99, 100):\n",
    "        return \"MIXED\"  # placeholders; your earlier pipeline should assign per-row based on pivot line\n",
    "    return \"SKELETONS\"\n",
    "\n",
    "# -------------------------\n",
    "# Binarization helpers\n",
    "# -------------------------\n",
    "def otsu_threshold(arr: np.ndarray) -> int:\n",
    "    hist = np.bincount(arr.ravel(), minlength=256).astype(np.float64)\n",
    "    total = arr.size\n",
    "    sum_total = np.dot(np.arange(256), hist)\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    max_var = -1.0\n",
    "    thresh = 200\n",
    "    for t in range(256):\n",
    "        w_b += hist[t]\n",
    "        if w_b == 0: \n",
    "            continue\n",
    "        w_f = total - w_b\n",
    "        if w_f == 0: \n",
    "            break\n",
    "        sum_b += t * hist[t]\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > max_var:\n",
    "            max_var = var_between\n",
    "            thresh = t\n",
    "    return int(thresh)\n",
    "\n",
    "def preprocess_bw(img: Image.Image, scale=3.0) -> Image.Image:\n",
    "    im = img.convert(\"L\")\n",
    "    im = ImageOps.autocontrast(im)\n",
    "    if scale != 1.0:\n",
    "        im = im.resize((int(im.size[0]*scale), int(im.size[1]*scale)), Image.Resampling.BICUBIC)\n",
    "    # mild cleanup + sharpen\n",
    "    im = im.filter(ImageFilter.MedianFilter(size=3))\n",
    "    im = im.filter(ImageFilter.UnsharpMask(radius=2, percent=170, threshold=3))\n",
    "\n",
    "    arr = np.array(im, dtype=np.uint8)\n",
    "    thr = otsu_threshold(arr)\n",
    "    bw = (arr > thr).astype(np.uint8) * 255\n",
    "    return Image.fromarray(bw)\n",
    "\n",
    "# -------------------------\n",
    "# Adaptive row band detection\n",
    "# -------------------------\n",
    "def find_row_bands_adaptive(bw: Image.Image, target_bands: int):\n",
    "    a = np.array(bw)\n",
    "    black = (a == 0).astype(np.uint8)\n",
    "    proj = black.sum(axis=1).astype(np.float32)\n",
    "\n",
    "    k = 21\n",
    "    smooth = np.convolve(proj, np.ones(k, dtype=np.float32)/k, mode=\"same\")\n",
    "    p85 = np.percentile(smooth, 85)\n",
    "\n",
    "    def bands_for_factor(factor: float):\n",
    "        thr = max(10.0, p85 * factor)\n",
    "        mask = smooth > thr\n",
    "        bands = []\n",
    "        y = 0\n",
    "        H = len(mask)\n",
    "        while y < H:\n",
    "            if not mask[y]:\n",
    "                y += 1\n",
    "                continue\n",
    "            y0 = y\n",
    "            while y < H and mask[y]:\n",
    "                y += 1\n",
    "            y1 = y\n",
    "            if (y1 - y0) >= 6:\n",
    "                bands.append((y0, y1))\n",
    "\n",
    "        # merge close\n",
    "        merged = []\n",
    "        for (y0, y1) in bands:\n",
    "            if not merged:\n",
    "                merged.append([y0, y1])\n",
    "            else:\n",
    "                if y0 - merged[-1][1] <= 6:\n",
    "                    merged[-1][1] = y1\n",
    "                else:\n",
    "                    merged.append([y0, y1])\n",
    "        return [(int(a), int(b)) for a, b in merged]\n",
    "\n",
    "    # try multiple thresholds; pick closest to target\n",
    "    factors = [0.36, 0.32, 0.28, 0.24, 0.20, 0.17, 0.14]\n",
    "    best = None\n",
    "    for f in factors:\n",
    "        b = bands_for_factor(f)\n",
    "        score = abs(len(b) - target_bands)\n",
    "        if best is None or score < best[0]:\n",
    "            best = (score, f, b)\n",
    "        if score == 0:\n",
    "            break\n",
    "\n",
    "    score, f, bands = best\n",
    "\n",
    "    # hard fallback: if still wildly off, force a uniform grid inside text-y span\n",
    "    if len(bands) < max(10, int(target_bands * 0.6)):\n",
    "        ys, xs = np.where(black)\n",
    "        if len(ys) == 0:\n",
    "            return []\n",
    "        y_min, y_max = int(ys.min()), int(ys.max())\n",
    "        span = y_max - y_min\n",
    "        step = span / target_bands\n",
    "        bands = []\n",
    "        for i in range(target_bands):\n",
    "            y0 = int(y_min + i * step)\n",
    "            y1 = int(y_min + (i + 1) * step)\n",
    "            bands.append((y0, y1))\n",
    "\n",
    "    return bands\n",
    "\n",
    "# -------------------------\n",
    "# 1D k-means for 3 column centers (no sklearn)\n",
    "# -------------------------\n",
    "def kmeans_1d_three(xs: np.ndarray, iters=20):\n",
    "    xs = xs.astype(np.float32)\n",
    "    # init at percentiles\n",
    "    c = np.array([np.percentile(xs, 20), np.percentile(xs, 50), np.percentile(xs, 80)], dtype=np.float32)\n",
    "    for _ in range(iters):\n",
    "        # assign\n",
    "        d0 = np.abs(xs - c[0])\n",
    "        d1 = np.abs(xs - c[1])\n",
    "        d2 = np.abs(xs - c[2])\n",
    "        labels = np.argmin(np.stack([d0, d1, d2], axis=1), axis=1)\n",
    "        new_c = c.copy()\n",
    "        for k in range(3):\n",
    "            m = xs[labels == k]\n",
    "            if len(m) > 50:\n",
    "                new_c[k] = m.mean()\n",
    "        # sort centers to keep order\n",
    "        new_c.sort()\n",
    "        if np.max(np.abs(new_c - c)) < 0.5:\n",
    "            c = new_c\n",
    "            break\n",
    "        c = new_c\n",
    "    return c\n",
    "\n",
    "def compute_column_bounds(bw: Image.Image):\n",
    "    a = np.array(bw)\n",
    "    black = (a == 0)\n",
    "    ys, xs = np.where(black)\n",
    "    H, W = black.shape\n",
    "\n",
    "    if len(xs) < 1000:\n",
    "        # fallback to rough thirds\n",
    "        b1 = int(W * 0.33)\n",
    "        b2 = int(W * 0.66)\n",
    "        return [(0, b1), (b1, b2), (b2, W)]\n",
    "\n",
    "    # sample xs for speed\n",
    "    sample = xs[:: max(1, len(xs)//20000)]\n",
    "    centers = kmeans_1d_three(sample)\n",
    "\n",
    "    b1 = int((centers[0] + centers[1]) / 2)\n",
    "    b2 = int((centers[1] + centers[2]) / 2)\n",
    "\n",
    "    pad = int(W * 0.02)\n",
    "    return [\n",
    "        (max(0, 0), min(W, b1 + pad)),\n",
    "        (max(0, b1 - pad), min(W, b2 + pad)),\n",
    "        (max(0, b2 - pad), min(W, W)),\n",
    "    ]\n",
    "\n",
    "# -------------------------\n",
    "# OCR + parse per cell\n",
    "# -------------------------\n",
    "CELL_CFG = (\n",
    "    \"--oem 3 --psm 7 \"\n",
    "    \"-c preserve_interword_spaces=1 \"\n",
    "    \"-c tessedit_char_whitelist=0123456789EDG\"\n",
    ")\n",
    "\n",
    "TRIPLET_RE = re.compile(r\"(\\d{1,6})\\s*([EDG])\\s*(\\d{1,7})\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    # allow parsing of 2513E22802 or 2513E 22802 etc\n",
    "    s = s.strip().upper()\n",
    "    s = re.sub(r\"[^0-9EDG\\s]\", \" \", s)\n",
    "    s = re.sub(r\"(\\d)([EDG])\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"([EDG])(\\d)\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def ocr_cell_triplet(cell: Image.Image):\n",
    "    d = pytesseract.image_to_data(cell, config=CELL_CFG, output_type=Output.DICT)\n",
    "\n",
    "    tokens = []\n",
    "    for txt in d[\"text\"]:\n",
    "        t = (txt or \"\").strip()\n",
    "        if t:\n",
    "            tokens.append(t)\n",
    "\n",
    "    if not tokens:\n",
    "        return None\n",
    "\n",
    "    raw = \" \".join(tokens)\n",
    "    norm = normalize_text(raw)\n",
    "\n",
    "    m = TRIPLET_RE.search(norm)\n",
    "    if m:\n",
    "        acc = int(m.group(1))\n",
    "        code = m.group(2)\n",
    "        cat = int(m.group(3))\n",
    "        return (acc, code, cat)\n",
    "\n",
    "    # fallback: sometimes we get [\"2513E\", \"22802\"] and regex fails before normalize; try normalize again\n",
    "    m = TRIPLET_RE.search(normalize_text(norm))\n",
    "    if m:\n",
    "        return (int(m.group(1)), m.group(2), int(m.group(3)))\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_page(page: int, path: Path):\n",
    "    img = Image.open(path)\n",
    "\n",
    "    exp = EXPECTED[page]\n",
    "    target_bands = int(round(exp / 3))\n",
    "\n",
    "    # try a couple scales; pick the one that matches expected count\n",
    "    scales = [2.6, 2.8, 3.0, 3.2]\n",
    "    best = None\n",
    "\n",
    "    for sc in scales:\n",
    "        bw = preprocess_bw(img, scale=sc)\n",
    "        bands = find_row_bands_adaptive(bw, target_bands=target_bands)\n",
    "        if not bands:\n",
    "            continue\n",
    "\n",
    "        col_bounds = compute_column_bounds(bw)\n",
    "        W, H = bw.size\n",
    "\n",
    "        rows = []\n",
    "        for (y0, y1) in bands:\n",
    "            yy0 = max(0, y0 - 2)\n",
    "            yy1 = min(H, y1 + 2)\n",
    "            for (x0, x1) in col_bounds:\n",
    "                cell = bw.crop((x0, yy0, x1, yy1))\n",
    "                t = ocr_cell_triplet(cell)\n",
    "                if t is not None:\n",
    "                    rows.append(t)\n",
    "\n",
    "        # de-dupe exact triplets\n",
    "        seen = set()\n",
    "        dedup = []\n",
    "        for t in rows:\n",
    "            if t in seen:\n",
    "                continue\n",
    "            seen.add(t)\n",
    "            dedup.append(t)\n",
    "\n",
    "        score = abs(len(dedup) - exp)\n",
    "        cand = {\n",
    "            \"scale\": sc,\n",
    "            \"bands\": len(bands),\n",
    "            \"rows\": dedup,\n",
    "            \"score\": score,\n",
    "            \"col_bounds\": col_bounds\n",
    "        }\n",
    "        if best is None or score < best[\"score\"]:\n",
    "            best = cand\n",
    "        if score == 0:\n",
    "            break\n",
    "\n",
    "    if best is None:\n",
    "        return [], {\"Page\": page, \"Extracted\": 0, \"Expected\": exp, \"Error\": \"FAILED_NO_BANDS\", \"Scale\": None, \"RowBands\": 0}\n",
    "\n",
    "    err = \"\" if len(best[\"rows\"]) == exp else f\"COUNT_MISMATCH {len(best['rows'])} != {exp}\"\n",
    "    rep = {\"Page\": page, \"Extracted\": len(best[\"rows\"]), \"Expected\": exp, \"Error\": err, \"Scale\": best[\"scale\"], \"RowBands\": best[\"bands\"]}\n",
    "    return best[\"rows\"], rep\n",
    "\n",
    "# -------------------------\n",
    "# Run rescue on pages in needs_review.csv\n",
    "# -------------------------\n",
    "needs = pd.read_csv(NEEDS_REVIEW)\n",
    "failed_pages = [int(x) for x in needs[\"Page\"].tolist()]\n",
    "\n",
    "all_df = pd.read_csv(OUTPUT_ALL)\n",
    "\n",
    "print(f\"[Rescue v2] Reprocessing failed pages: {failed_pages}\")\n",
    "print(f\"[Rescue v2] target total rows = {expected_total()}\")\n",
    "\n",
    "t0 = time.time()\n",
    "reports = []\n",
    "\n",
    "for i, pg in enumerate(failed_pages, 1):\n",
    "    path = JPEGS_DIR / f\"{pg}.jpg\"\n",
    "    rows, rep = extract_page(pg, path)\n",
    "\n",
    "    # patch page rows into output_all_rows\n",
    "    all_df = all_df[all_df[\"Page\"] != pg].copy()\n",
    "    tname = type_for_page(pg)\n",
    "\n",
    "    patch = pd.DataFrame([{\n",
    "        \"Page\": pg,\n",
    "        \"Type\": tname,\n",
    "        \"Accession\": a,\n",
    "        \"Code\": c,\n",
    "        \"Catalog\": k,\n",
    "    } for (a, c, k) in rows])\n",
    "\n",
    "    all_df = pd.concat([all_df, patch], ignore_index=True)\n",
    "\n",
    "    reports.append(rep)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"  page {pg} ({i}/{len(failed_pages)}): {rep['Extracted']}/{rep['Expected']}  bands={rep['RowBands']}  scale={rep['Scale']}  err={rep['Error']}  elapsed={dt:.1f}s\")\n",
    "\n",
    "# write reports\n",
    "rescue_report = pd.DataFrame(reports)\n",
    "rescue_report.to_csv(OUT_DIR / \"rescue_report_v2.csv\", index=False)\n",
    "\n",
    "# rebuild page counts + needs review\n",
    "counts = []\n",
    "for pg in range(1, 103):\n",
    "    got = int((all_df[\"Page\"] == pg).sum())\n",
    "    exp = EXPECTED[pg]\n",
    "    err = \"\" if got == exp else f\"COUNT_MISMATCH {got} != {exp}\"\n",
    "    counts.append({\"Page\": pg, \"RowsExtracted\": got, \"RowsExpected\": exp, \"Error\": err})\n",
    "\n",
    "page_counts_df = pd.DataFrame(counts)\n",
    "page_counts_df.to_csv(PAGE_COUNTS, index=False)\n",
    "needs2 = page_counts_df[page_counts_df[\"Error\"] != \"\"].copy()\n",
    "needs2.to_csv(OUT_DIR / \"needs_review.csv\", index=False)\n",
    "\n",
    "# write patched output_all\n",
    "all_df.to_csv(OUTPUT_ALL, index=False)\n",
    "\n",
    "print(\"\\n[Rescue v2 Done]\")\n",
    "print(\"  output_all_rows.csv rows =\", len(all_df), f\"(target {expected_total()})\")\n",
    "print(\"  pages_flagged =\", len(needs2))\n",
    "print(\"  wrote out/rescue_report_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1830787-13ac-4e32-9f03-fc47f43ece2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
